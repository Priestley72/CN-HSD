[2025-06-12 21:23:55,680] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|2025-06-12 21:23:59] llamafactory.cli:143 >> Initializing 1 distributed tasks at: 127.0.0.1:43959
[2025-06-12 21:24:07,366] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|2025-06-12 21:24:10] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-06-12 21:24:10] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:10,625 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:10,626 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:10,626 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:10,626 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:10,626 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:10,626 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:10,626 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-12 21:24:11,094 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:696] 2025-06-12 21:24:11,097 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-12 21:24:11,100 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:11,101 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:11,101 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:11,102 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:11,102 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:11,102 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:11,102 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2021] 2025-06-12 21:24:11,102 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2299] 2025-06-12 21:24:11,568 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-06-12 21:24:11] llamafactory.data.loader:143 >> Loading dataset format_enhanced_train.json...
Converting format of dataset (num_proc=8):   0%|          | 0/4000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=8):  11%|█         | 448/4000 [00:00<00:00, 4298.76 examples/s]Converting format of dataset (num_proc=8):  91%|█████████ | 3646/4000 [00:00<00:00, 20265.00 examples/s]Converting format of dataset (num_proc=8): 100%|██████████| 4000/4000 [00:00<00:00, 12593.83 examples/s]
Running tokenizer on dataset (num_proc=8):   0%|          | 0/4000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=8):  12%|█▎        | 500/4000 [00:01<00:08, 419.79 examples/s]Running tokenizer on dataset (num_proc=8):  25%|██▌       | 1000/4000 [00:01<00:04, 692.04 examples/s]Running tokenizer on dataset (num_proc=8):  38%|███▊      | 1500/4000 [00:01<00:02, 1079.46 examples/s]Running tokenizer on dataset (num_proc=8):  50%|█████     | 2000/4000 [00:01<00:01, 1414.16 examples/s]Running tokenizer on dataset (num_proc=8):  62%|██████▎   | 2500/4000 [00:02<00:00, 1791.00 examples/s]Running tokenizer on dataset (num_proc=8):  75%|███████▌  | 3000/4000 [00:02<00:00, 2220.86 examples/s]Running tokenizer on dataset (num_proc=8):  88%|████████▊ | 3500/4000 [00:02<00:00, 2499.54 examples/s]Running tokenizer on dataset (num_proc=8): 100%|██████████| 4000/4000 [00:02<00:00, 2758.79 examples/s]Running tokenizer on dataset (num_proc=8): 100%|██████████| 4000/4000 [00:03<00:00, 1278.42 examples/s]
training example:
input_ids:
[56568, 110124, 43815, 104998, 101057, 3837, 100006, 101042, 104811, 72881, 99700, 62926, 108015, 107439, 46944, 57191, 101213, 63703, 23305, 40027, 11, 6567, 107, 237, 18947, 63703, 23305, 40027, 59879, 87752, 107684, 101286, 2073, 85641, 64429, 760, 69162, 46423, 101313, 760, 220, 117828, 104553, 760, 71951, 117828, 88774, 14880, 112293, 87752, 101882, 28311, 12, 58230, 121, 32757, 37029, 104811, 57191, 99700, 106594, 18830, 115076, 3837, 16530, 37029, 92894, 102064, 8997, 12, 1036, 85641, 64429, 854, 73670, 20412, 100398, 17340, 13072, 5373, 112451, 5373, 104553, 57191, 111372, 101290, 3837, 107427, 100692, 64429, 95053, 107354, 2073, 4576, 33590, 92894, 59151, 58362, 104465, 52129, 99689, 100414, 8997, 12, 1036, 64429, 101313, 854, 101092, 2073, 85641, 64429, 854, 31838, 102098, 104867, 101313, 5373, 103964, 5373, 104405, 57191, 101070, 112926, 3837, 50511, 17714, 99936, 115076, 3837, 104638, 100662, 52129, 99700, 112926, 3837, 98237, 30858, 117225, 30534, 8997, 12, 1036, 117828, 104553, 854, 91680, 100630, 4957, 21235, 48, 5373, 14091, 5373, 19439, 2142, 5373, 49, 580, 2142, 5373, 58861, 5373, 6280, 2832, 349, 69515, 90919, 14091, 51463, 106002, 111427, 3837, 49, 580, 2142, 51463, 112187, 111427, 3837, 19439, 2142, 51463, 109391, 111427, 3837, 43, 21235, 48, 51463, 111427, 33071, 102800, 104553, 3837, 58861, 51463, 111427, 92894, 105149, 104553, 3837, 6280, 2832, 349, 51463, 65676, 117828, 57191, 15946, 33071, 104553, 3837, 102700, 63703, 23305, 40027, 30440, 102031, 101213, 117828, 104553, 198, 12, 1036, 117828, 105151, 854, 111066, 75882, 85641, 64429, 12, 67831, 27442, 12, 104553, 101220, 64471, 104384, 32664, 104553, 9370, 117828, 102124, 3837, 71, 349, 51463, 100629, 106050, 117828, 57191, 102109, 33071, 3837, 6280, 2832, 349, 51463, 17714, 15946, 33071, 57191, 16530, 104384, 104553, 117828, 198, 12, 34369, 225, 71138, 101920, 11622, 1, 760, 330, 17177, 99859, 3837, 101213, 63703, 23305, 40027, 101920, 91680, 102496, 11622, 1, 508, 81376, 60, 330, 17177, 99859, 3837, 100161, 71268, 23031, 1, 508, 4689, 19177, 114903, 3837, 60533, 34794, 33983, 198, 12, 10236, 99, 223, 81433, 102704, 71134, 22243, 38304, 57191, 43316, 20742, 38304, 49567, 107173, 108872, 3837, 104484, 42855, 104136, 33071, 87335, 49567, 107173, 43815, 3837, 104484, 105444, 102700, 63703, 23305, 40027, 198, 90885, 26355, 14777, 28311, 31196, 2073, 107042, 56007, 17992, 11319, 104198, 102053, 17340, 109890, 80158, 104169, 83751, 34187, 99487, 102104, 10236, 255, 242, 35568, 102658, 100148, 100397, 36587, 110115, 145549, 145549, 9336, 70568, 2073, 99590, 35568, 760, 86009, 100272, 100148, 100397, 36587, 110115, 145549, 145549, 760, 2477, 2832, 349, 760, 2477, 2832, 349, 508, 4689, 60, 88774, 90885, 26355, 40820, 28311, 31196, 2073, 104083, 16530, 112459, 3837, 100632, 104361, 100380, 3837, 101058, 105926, 100732, 105198, 20412, 104185, 9370, 3837, 21894, 13072, 99313, 99937, 99462, 3837, 100483, 99313, 71416, 9370, 107640, 146929, 9336, 70568, 2073, 101058, 105926, 100732, 105198, 760, 6567, 110, 111, 58463, 9370, 3837, 21894, 13072, 99313, 99937, 99462, 3837, 100483, 99313, 71416, 9370, 107640, 146929, 760, 2477, 2832, 349, 760, 2477, 2832, 349, 508, 4689, 60, 88774, 99601, 14880, 101042, 87752, 109949, 62926, 107439, 63703, 23305, 40027, 3837, 103148, 104787, 63703, 23305, 40027, 59151, 3837, 104484, 104787, 104107, 43815, 3837, 104484, 104787, 107173, 43815, 3837, 104484, 105444, 102700, 63703, 23305, 40027, 510, 99520, 83751, 9370, 3837, 101219, 109890, 17447, 101041, 100118, 9370, 7646, 104152, 107108, 100118, 34187, 3837, 100672, 100681, 32664, 104185, 17340, 99601, 101174, 116174, 34187, 608, 2152, 5854, 766, 198, 14374, 66017, 25, 104185, 17340, 760, 6567, 234, 118, 116174, 34187, 760, 2477, 2832, 349, 760, 2477, 2832, 349, 508, 4689, 60]
inputs:
你是一名内容审查专家，能够分析中文语句并从中提取一个或多个四元组, 每个四元组按以下顺序组成“评论对象 | 对象观点 | 仇恨群体 | 是否仇恨”
请严格执行以下要求：
- 尽量使用中文或句中原有片段，不使用其他语言。
- “评论对象”可以是具体人名、称呼、群体或抽象概念，如果没有明确对象，则填写“NULL”，其他结果需保留原词形式。
- “对象观点”针对“评论对象”所发表的核心观点、评价、情绪或行为表述，应为关键片段，尽量保持原句表述，简明扼要。
- “仇恨群体”只包括(LGBTQ、Region、Sexism、Racism、others、non-hate)，其中Region表示地域歧视，Racism表示种族歧视，Sexism表示性别歧视，LGBTQ表示歧视性少数群体，others表示歧视其他特定群体，non-hate表示非仇恨或中性群体，同一四元组可涉及多个仇恨群体
- “仇恨标签”标注该评论对象-论点-群体组合是否构成对群体的仇恨表达，hate表示具有明显的仇恨或攻击性，non-hate表示为中性或不构成群体仇恨
- 元素之间用" | "分隔，多个四元组之间只允许用" [SEP] "分隔，最后都以" [END]"结尾，注意空格
- 禁止含有换行符或制表符等无关符号，禁止添加解释性文字等无关内容，禁止重复同一四元组
样例一：
输入“一脸问号？我是广东人知乎就给我推了这个回答 答主不懂不要乱说好吧🐶🐶”, 输出“答主 | 不懂不要乱说好吧🐶🐶 | non-hate | non-hate [END]”
样例二：
输入“很难不赞同，不过补充一点，杨靖宇将军是河南的，本名马尚德，驻马店的骄傲🍉”, 输出“杨靖宇将军 | 河南的，本名马尚德，驻马店的骄傲🍉 | non-hate | non-hate [END]”
现在请分析以下句子并提取四元组，单纯回复四元组结果，禁止回复思考内容，禁止回复无关内容，禁止重复同一四元组:
不是推的，是在知乎上直接搜的??所有的我都搜了，真的感觉对河南人现在挺友善了 /no_think
###输出:河南人 | 挺友善了 | non-hate | non-hate [END]
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 104185, 17340, 760, 6567, 234, 118, 116174, 34187, 760, 2477, 2832, 349, 760, 2477, 2832, 349, 508, 4689, 60]
labels:
河南人 | 挺友善了 | non-hate | non-hate [END]
[INFO|configuration_utils.py:696] 2025-06-12 21:24:16,656 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-12 21:24:16,658 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|2025-06-12 21:24:16] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|modeling_utils.py:1148] 2025-06-12 21:24:17,009 >> loading weights file ./models/Qwen3-8B/model.safetensors.index.json
[INFO|modeling_utils.py:2241] 2025-06-12 21:24:17,010 >> Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-06-12 21:24:17,013 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.14s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.02s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.03it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.46it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.22it/s]
[INFO|modeling_utils.py:5131] 2025-06-12 21:24:21,153 >> All model checkpoint weights were used when initializing Qwen3ForCausalLM.

[INFO|modeling_utils.py:5139] 2025-06-12 21:24:21,154 >> All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at ./models/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-06-12 21:24:21,158 >> loading configuration file ./models/Qwen3-8B/generation_config.json
[INFO|configuration_utils.py:1135] 2025-06-12 21:24:21,159 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

[INFO|2025-06-12 21:24:21] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-06-12 21:24:21] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-06-12 21:24:21] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-06-12 21:24:21] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-06-12 21:24:22] llamafactory.model.loader:143 >> trainable params: 21,823,488 || all params: 8,212,558,848 || trainable%: 0.2657
[INFO|trainer.py:756] 2025-06-12 21:24:22,175 >> Using auto half precision backend
[WARNING|2025-06-12 21:24:22] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[INFO|trainer.py:2409] 2025-06-12 21:24:22,735 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-06-12 21:24:22,735 >>   Num examples = 4,000
[INFO|trainer.py:2411] 2025-06-12 21:24:22,735 >>   Num Epochs = 8
[INFO|trainer.py:2412] 2025-06-12 21:24:22,735 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2415] 2025-06-12 21:24:22,735 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2416] 2025-06-12 21:24:22,735 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2417] 2025-06-12 21:24:22,735 >>   Total optimization steps = 2,000
[INFO|trainer.py:2418] 2025-06-12 21:24:22,739 >>   Number of trainable parameters = 21,823,488
  0%|          | 0/2000 [00:00<?, ?it/s]  0%|          | 1/2000 [00:05<3:11:20,  5.74s/it]  0%|          | 2/2000 [00:10<2:54:32,  5.24s/it]  0%|          | 3/2000 [00:15<2:54:10,  5.23s/it]  0%|          | 4/2000 [00:21<2:55:43,  5.28s/it]  0%|          | 5/2000 [00:26<2:55:26,  5.28s/it]  0%|          | 6/2000 [00:31<2:56:23,  5.31s/it]  0%|          | 7/2000 [00:36<2:52:22,  5.19s/it]  0%|          | 8/2000 [00:42<2:55:31,  5.29s/it]  0%|          | 9/2000 [00:47<2:56:32,  5.32s/it]  0%|          | 10/2000 [00:52<2:55:45,  5.30s/it]                                                   {'loss': 1.1425, 'grad_norm': 1.8340026140213013, 'learning_rate': 4.5e-06, 'epoch': 0.04}
  0%|          | 10/2000 [00:52<2:55:45,  5.30s/it]  1%|          | 11/2000 [00:57<2:50:13,  5.14s/it]  1%|          | 12/2000 [01:02<2:50:41,  5.15s/it]  1%|          | 13/2000 [01:08<2:53:54,  5.25s/it]  1%|          | 14/2000 [01:13<2:52:36,  5.21s/it]  1%|          | 15/2000 [01:19<2:57:07,  5.35s/it]  1%|          | 16/2000 [01:24<2:55:45,  5.32s/it]  1%|          | 17/2000 [01:29<2:54:42,  5.29s/it]  1%|          | 18/2000 [01:34<2:51:56,  5.21s/it]  1%|          | 19/2000 [01:39<2:52:41,  5.23s/it]  1%|          | 20/2000 [01:44<2:49:59,  5.15s/it]                                                   {'loss': 1.1292, 'grad_norm': 2.0897421836853027, 'learning_rate': 9.5e-06, 'epoch': 0.08}
  1%|          | 20/2000 [01:44<2:49:59,  5.15s/it]  1%|          | 21/2000 [01:50<2:51:58,  5.21s/it]  1%|          | 22/2000 [01:55<2:52:58,  5.25s/it]  1%|          | 23/2000 [02:00<2:49:43,  5.15s/it]  1%|          | 24/2000 [02:06<2:55:42,  5.34s/it]  1%|▏         | 25/2000 [02:11<2:53:45,  5.28s/it]  1%|▏         | 26/2000 [02:16<2:52:19,  5.24s/it]  1%|▏         | 27/2000 [02:21<2:50:26,  5.18s/it]  1%|▏         | 28/2000 [02:26<2:50:05,  5.18s/it]  1%|▏         | 29/2000 [02:31<2:49:53,  5.17s/it]  2%|▏         | 30/2000 [02:37<2:48:59,  5.15s/it]                                                   {'loss': 1.1657, 'grad_norm': 2.0050039291381836, 'learning_rate': 1.45e-05, 'epoch': 0.12}
  2%|▏         | 30/2000 [02:37<2:48:59,  5.15s/it]  2%|▏         | 31/2000 [02:42<2:48:51,  5.15s/it]  2%|▏         | 32/2000 [02:47<2:46:40,  5.08s/it]  2%|▏         | 33/2000 [02:52<2:48:03,  5.13s/it]  2%|▏         | 34/2000 [02:57<2:47:14,  5.10s/it]  2%|▏         | 35/2000 [03:02<2:47:38,  5.12s/it]  2%|▏         | 36/2000 [03:07<2:48:01,  5.13s/it]  2%|▏         | 37/2000 [03:12<2:48:41,  5.16s/it]  2%|▏         | 38/2000 [03:18<2:48:45,  5.16s/it]  2%|▏         | 39/2000 [03:22<2:45:18,  5.06s/it]  2%|▏         | 40/2000 [03:28<2:46:20,  5.09s/it]                                                   {'loss': 1.0164, 'grad_norm': 1.9910166263580322, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.16}
  2%|▏         | 40/2000 [03:28<2:46:20,  5.09s/it]  2%|▏         | 41/2000 [03:33<2:49:36,  5.19s/it]  2%|▏         | 42/2000 [03:38<2:50:06,  5.21s/it]  2%|▏         | 43/2000 [03:43<2:47:57,  5.15s/it]  2%|▏         | 44/2000 [03:49<2:49:06,  5.19s/it]  2%|▏         | 45/2000 [03:54<2:48:10,  5.16s/it]  2%|▏         | 46/2000 [03:59<2:48:40,  5.18s/it]  2%|▏         | 47/2000 [04:04<2:48:44,  5.18s/it]  2%|▏         | 48/2000 [04:09<2:48:18,  5.17s/it]  2%|▏         | 49/2000 [04:14<2:46:26,  5.12s/it]  2%|▎         | 50/2000 [04:20<2:52:45,  5.32s/it]                                                   {'loss': 0.6624, 'grad_norm': 1.09648597240448, 'learning_rate': 2.45e-05, 'epoch': 0.2}
  2%|▎         | 50/2000 [04:20<2:52:45,  5.32s/it]  3%|▎         | 51/2000 [04:25<2:51:59,  5.29s/it]  3%|▎         | 52/2000 [04:31<2:52:59,  5.33s/it]  3%|▎         | 53/2000 [04:36<2:50:12,  5.25s/it]  3%|▎         | 54/2000 [04:41<2:49:41,  5.23s/it]  3%|▎         | 55/2000 [04:46<2:48:46,  5.21s/it]  3%|▎         | 56/2000 [04:51<2:48:32,  5.20s/it]  3%|▎         | 57/2000 [04:57<2:49:38,  5.24s/it]  3%|▎         | 58/2000 [05:02<2:50:06,  5.26s/it]  3%|▎         | 59/2000 [05:07<2:45:42,  5.12s/it]  3%|▎         | 60/2000 [05:12<2:46:48,  5.16s/it]                                                   {'loss': 0.4216, 'grad_norm': 0.7935116291046143, 'learning_rate': 2.95e-05, 'epoch': 0.24}
  3%|▎         | 60/2000 [05:12<2:46:48,  5.16s/it]  3%|▎         | 61/2000 [05:17<2:45:00,  5.11s/it]  3%|▎         | 62/2000 [05:22<2:47:29,  5.19s/it]  3%|▎         | 63/2000 [05:28<2:49:55,  5.26s/it]  3%|▎         | 64/2000 [05:33<2:49:00,  5.24s/it]  3%|▎         | 65/2000 [05:38<2:46:48,  5.17s/it]  3%|▎         | 66/2000 [05:44<2:52:50,  5.36s/it]  3%|▎         | 67/2000 [05:49<2:52:24,  5.35s/it]  3%|▎         | 68/2000 [05:54<2:47:44,  5.21s/it]  3%|▎         | 69/2000 [05:59<2:45:51,  5.15s/it]  4%|▎         | 70/2000 [06:04<2:48:51,  5.25s/it]                                                   {'loss': 0.3545, 'grad_norm': 0.6737170219421387, 'learning_rate': 3.45e-05, 'epoch': 0.28}
  4%|▎         | 70/2000 [06:04<2:48:51,  5.25s/it]  4%|▎         | 71/2000 [06:09<2:45:52,  5.16s/it]  4%|▎         | 72/2000 [06:15<2:47:19,  5.21s/it]  4%|▎         | 73/2000 [06:20<2:49:57,  5.29s/it]  4%|▎         | 74/2000 [06:25<2:46:29,  5.19s/it]  4%|▍         | 75/2000 [06:31<2:48:51,  5.26s/it]  4%|▍         | 76/2000 [06:36<2:51:00,  5.33s/it]  4%|▍         | 77/2000 [06:41<2:47:57,  5.24s/it]  4%|▍         | 78/2000 [06:46<2:46:35,  5.20s/it]  4%|▍         | 79/2000 [06:51<2:45:52,  5.18s/it]  4%|▍         | 80/2000 [06:56<2:42:47,  5.09s/it]                                                   {'loss': 0.3408, 'grad_norm': 0.6587926149368286, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.32}
  4%|▍         | 80/2000 [06:56<2:42:47,  5.09s/it]  4%|▍         | 81/2000 [07:01<2:43:18,  5.11s/it]  4%|▍         | 82/2000 [07:06<2:42:22,  5.08s/it]  4%|▍         | 83/2000 [07:12<2:44:21,  5.14s/it]  4%|▍         | 84/2000 [07:17<2:49:55,  5.32s/it]  4%|▍         | 85/2000 [07:22<2:44:43,  5.16s/it]  4%|▍         | 86/2000 [07:27<2:44:38,  5.16s/it]  4%|▍         | 87/2000 [07:33<2:44:52,  5.17s/it]  4%|▍         | 88/2000 [07:38<2:46:41,  5.23s/it]  4%|▍         | 89/2000 [07:43<2:42:28,  5.10s/it]  4%|▍         | 90/2000 [07:48<2:46:51,  5.24s/it]                                                   {'loss': 0.3196, 'grad_norm': 0.6959449648857117, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.36}
  4%|▍         | 90/2000 [07:50<2:46:51,  5.24s/it]  5%|▍         | 91/2000 [07:56<3:07:15,  5.89s/it]  5%|▍         | 92/2000 [08:01<3:04:06,  5.79s/it]  5%|▍         | 93/2000 [08:07<3:00:20,  5.67s/it]  5%|▍         | 94/2000 [08:11<2:52:27,  5.43s/it]  5%|▍         | 95/2000 [08:17<2:50:03,  5.36s/it]  5%|▍         | 96/2000 [08:21<2:43:33,  5.15s/it]  5%|▍         | 97/2000 [08:27<2:47:12,  5.27s/it]  5%|▍         | 98/2000 [08:32<2:45:24,  5.22s/it]  5%|▍         | 99/2000 [08:37<2:44:31,  5.19s/it]  5%|▌         | 100/2000 [08:42<2:41:28,  5.10s/it]                                                    {'loss': 0.296, 'grad_norm': 0.6310460567474365, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.4}
  5%|▌         | 100/2000 [08:42<2:41:28,  5.10s/it]  5%|▌         | 101/2000 [08:47<2:41:31,  5.10s/it]  5%|▌         | 102/2000 [08:52<2:42:51,  5.15s/it]  5%|▌         | 103/2000 [08:58<2:46:38,  5.27s/it]  5%|▌         | 104/2000 [09:03<2:44:46,  5.21s/it]  5%|▌         | 105/2000 [09:08<2:46:19,  5.27s/it]  5%|▌         | 106/2000 [09:14<2:46:34,  5.28s/it]  5%|▌         | 107/2000 [09:19<2:43:10,  5.17s/it]  5%|▌         | 108/2000 [09:24<2:41:35,  5.12s/it]  5%|▌         | 109/2000 [09:29<2:41:26,  5.12s/it]  6%|▌         | 110/2000 [09:34<2:42:30,  5.16s/it]                                                    {'loss': 0.302, 'grad_norm': 0.6469554305076599, 'learning_rate': 4.9997231914115064e-05, 'epoch': 0.44}
  6%|▌         | 110/2000 [09:34<2:42:30,  5.16s/it]  6%|▌         | 111/2000 [09:39<2:40:40,  5.10s/it]  6%|▌         | 112/2000 [09:44<2:39:51,  5.08s/it]  6%|▌         | 113/2000 [09:49<2:36:23,  4.97s/it]  6%|▌         | 114/2000 [09:54<2:40:18,  5.10s/it]  6%|▌         | 115/2000 [09:59<2:40:52,  5.12s/it]  6%|▌         | 116/2000 [10:04<2:38:33,  5.05s/it]  6%|▌         | 117/2000 [10:09<2:40:07,  5.10s/it]  6%|▌         | 118/2000 [10:14<2:38:39,  5.06s/it]  6%|▌         | 119/2000 [10:19<2:39:36,  5.09s/it]  6%|▌         | 120/2000 [10:25<2:39:54,  5.10s/it]                                                    {'loss': 0.2641, 'grad_norm': 0.643488347530365, 'learning_rate': 4.998766400914329e-05, 'epoch': 0.48}
  6%|▌         | 120/2000 [10:25<2:39:54,  5.10s/it]  6%|▌         | 121/2000 [10:30<2:46:37,  5.32s/it]  6%|▌         | 122/2000 [10:36<2:45:59,  5.30s/it]  6%|▌         | 123/2000 [10:41<2:42:56,  5.21s/it]  6%|▌         | 124/2000 [10:46<2:40:38,  5.14s/it]  6%|▋         | 125/2000 [10:51<2:42:55,  5.21s/it]  6%|▋         | 126/2000 [10:56<2:42:36,  5.21s/it]  6%|▋         | 127/2000 [11:01<2:41:48,  5.18s/it]  6%|▋         | 128/2000 [11:06<2:39:09,  5.10s/it]  6%|▋         | 129/2000 [11:12<2:41:34,  5.18s/it]  6%|▋         | 130/2000 [11:17<2:39:47,  5.13s/it]                                                    {'loss': 0.2472, 'grad_norm': 0.7432137727737427, 'learning_rate': 4.99712647263317e-05, 'epoch': 0.52}
  6%|▋         | 130/2000 [11:17<2:39:47,  5.13s/it]  7%|▋         | 131/2000 [11:22<2:43:29,  5.25s/it]  7%|▋         | 132/2000 [11:28<2:47:38,  5.38s/it]  7%|▋         | 133/2000 [11:33<2:43:32,  5.26s/it]  7%|▋         | 134/2000 [11:38<2:40:20,  5.16s/it]  7%|▋         | 135/2000 [11:43<2:38:35,  5.10s/it]  7%|▋         | 136/2000 [11:48<2:36:30,  5.04s/it]  7%|▋         | 137/2000 [11:53<2:37:49,  5.08s/it]  7%|▋         | 138/2000 [11:58<2:39:46,  5.15s/it]  7%|▋         | 139/2000 [12:04<2:44:44,  5.31s/it]  7%|▋         | 140/2000 [12:09<2:44:32,  5.31s/it]                                                    {'loss': 0.282, 'grad_norm': 0.662975549697876, 'learning_rate': 4.9948038549080456e-05, 'epoch': 0.56}
  7%|▋         | 140/2000 [12:10<2:44:32,  5.31s/it]  7%|▋         | 141/2000 [12:15<2:46:11,  5.36s/it]  7%|▋         | 142/2000 [12:20<2:42:25,  5.25s/it]  7%|▋         | 143/2000 [12:24<2:38:00,  5.11s/it]  7%|▋         | 144/2000 [12:30<2:40:20,  5.18s/it]  7%|▋         | 145/2000 [12:35<2:37:06,  5.08s/it]  7%|▋         | 146/2000 [12:40<2:38:07,  5.12s/it]  7%|▋         | 147/2000 [12:45<2:42:26,  5.26s/it]  7%|▋         | 148/2000 [12:50<2:39:29,  5.17s/it]  7%|▋         | 149/2000 [12:56<2:39:51,  5.18s/it]  8%|▊         | 150/2000 [13:01<2:40:37,  5.21s/it]                                                    {'loss': 0.287, 'grad_norm': 0.8524181246757507, 'learning_rate': 4.991799182719451e-05, 'epoch': 0.6}
  8%|▊         | 150/2000 [13:01<2:40:37,  5.21s/it]  8%|▊         | 151/2000 [13:06<2:39:48,  5.19s/it]  8%|▊         | 152/2000 [13:12<2:44:08,  5.33s/it]  8%|▊         | 153/2000 [13:17<2:40:22,  5.21s/it]  8%|▊         | 154/2000 [13:21<2:37:55,  5.13s/it]  8%|▊         | 155/2000 [13:26<2:35:19,  5.05s/it]  8%|▊         | 156/2000 [13:32<2:37:27,  5.12s/it]  8%|▊         | 157/2000 [13:37<2:35:32,  5.06s/it]  8%|▊         | 158/2000 [13:42<2:37:30,  5.13s/it]  8%|▊         | 159/2000 [13:47<2:35:41,  5.07s/it]  8%|▊         | 160/2000 [13:52<2:38:09,  5.16s/it]                                                    {'loss': 0.2842, 'grad_norm': 0.5454989671707153, 'learning_rate': 4.988113277514761e-05, 'epoch': 0.64}
  8%|▊         | 160/2000 [13:52<2:38:09,  5.16s/it]  8%|▊         | 161/2000 [13:58<2:39:58,  5.22s/it]  8%|▊         | 162/2000 [14:03<2:39:14,  5.20s/it]  8%|▊         | 163/2000 [14:08<2:38:34,  5.18s/it]  8%|▊         | 164/2000 [14:13<2:39:40,  5.22s/it]  8%|▊         | 165/2000 [14:19<2:43:33,  5.35s/it]  8%|▊         | 166/2000 [14:24<2:40:25,  5.25s/it]  8%|▊         | 167/2000 [14:29<2:39:08,  5.21s/it]  8%|▊         | 168/2000 [14:34<2:39:10,  5.21s/it]  8%|▊         | 169/2000 [14:39<2:39:24,  5.22s/it]  8%|▊         | 170/2000 [14:44<2:35:16,  5.09s/it]                                                    {'loss': 0.2649, 'grad_norm': 0.7174860835075378, 'learning_rate': 4.983747146983656e-05, 'epoch': 0.68}
  8%|▊         | 170/2000 [14:46<2:35:16,  5.09s/it]  9%|▊         | 171/2000 [14:51<2:49:51,  5.57s/it]  9%|▊         | 172/2000 [14:56<2:45:23,  5.43s/it]  9%|▊         | 173/2000 [15:02<2:47:08,  5.49s/it]  9%|▊         | 174/2000 [15:07<2:44:20,  5.40s/it]  9%|▉         | 175/2000 [15:12<2:42:41,  5.35s/it]  9%|▉         | 176/2000 [15:17<2:39:20,  5.24s/it]  9%|▉         | 177/2000 [15:22<2:39:40,  5.26s/it]  9%|▉         | 178/2000 [15:27<2:37:18,  5.18s/it]  9%|▉         | 179/2000 [15:32<2:37:24,  5.19s/it]  9%|▉         | 180/2000 [15:38<2:37:14,  5.18s/it]                                                    {'loss': 0.2879, 'grad_norm': 0.5177512764930725, 'learning_rate': 4.978701984782625e-05, 'epoch': 0.72}
  9%|▉         | 180/2000 [15:38<2:37:14,  5.18s/it]  9%|▉         | 181/2000 [15:43<2:40:22,  5.29s/it]  9%|▉         | 182/2000 [15:48<2:38:28,  5.23s/it]  9%|▉         | 183/2000 [15:54<2:40:35,  5.30s/it]  9%|▉         | 184/2000 [15:59<2:37:45,  5.21s/it]  9%|▉         | 185/2000 [16:04<2:38:05,  5.23s/it]  9%|▉         | 186/2000 [16:09<2:35:23,  5.14s/it]  9%|▉         | 187/2000 [16:14<2:34:56,  5.13s/it]  9%|▉         | 188/2000 [16:19<2:36:48,  5.19s/it]  9%|▉         | 189/2000 [16:25<2:37:38,  5.22s/it] 10%|▉         | 190/2000 [16:30<2:35:47,  5.16s/it]                                                    {'loss': 0.2574, 'grad_norm': 0.70014888048172, 'learning_rate': 4.9729791702086414e-05, 'epoch': 0.76}
 10%|▉         | 190/2000 [16:30<2:35:47,  5.16s/it] 10%|▉         | 191/2000 [16:35<2:34:54,  5.14s/it] 10%|▉         | 192/2000 [16:40<2:33:26,  5.09s/it] 10%|▉         | 193/2000 [16:45<2:33:54,  5.11s/it] 10%|▉         | 194/2000 [16:50<2:30:06,  4.99s/it] 10%|▉         | 195/2000 [16:55<2:30:05,  4.99s/it] 10%|▉         | 196/2000 [17:00<2:34:06,  5.13s/it] 10%|▉         | 197/2000 [17:06<2:39:01,  5.29s/it] 10%|▉         | 198/2000 [17:11<2:35:11,  5.17s/it] 10%|▉         | 199/2000 [17:15<2:30:53,  5.03s/it] 10%|█         | 200/2000 [17:21<2:32:37,  5.09s/it]                                                    {'loss': 0.2424, 'grad_norm': 0.7058272361755371, 'learning_rate': 4.966580267822065e-05, 'epoch': 0.8}
 10%|█         | 200/2000 [17:21<2:32:37,  5.09s/it] 10%|█         | 201/2000 [17:26<2:32:59,  5.10s/it] 10%|█         | 202/2000 [17:31<2:37:39,  5.26s/it] 10%|█         | 203/2000 [17:36<2:36:44,  5.23s/it] 10%|█         | 204/2000 [17:42<2:34:47,  5.17s/it] 10%|█         | 205/2000 [17:47<2:33:55,  5.14s/it] 10%|█         | 206/2000 [17:52<2:33:36,  5.14s/it] 10%|█         | 207/2000 [17:57<2:34:06,  5.16s/it] 10%|█         | 208/2000 [18:02<2:37:33,  5.28s/it] 10%|█         | 209/2000 [18:07<2:32:43,  5.12s/it] 10%|█         | 210/2000 [18:12<2:30:37,  5.05s/it]                                                    {'loss': 0.2494, 'grad_norm': 0.8716056942939758, 'learning_rate': 4.959507027018918e-05, 'epoch': 0.84}
 10%|█         | 210/2000 [18:12<2:30:37,  5.05s/it] 11%|█         | 211/2000 [18:17<2:31:08,  5.07s/it] 11%|█         | 212/2000 [18:22<2:30:00,  5.03s/it] 11%|█         | 213/2000 [18:28<2:34:13,  5.18s/it] 11%|█         | 214/2000 [18:33<2:34:35,  5.19s/it] 11%|█         | 215/2000 [18:38<2:34:20,  5.19s/it] 11%|█         | 216/2000 [18:43<2:33:01,  5.15s/it] 11%|█         | 217/2000 [18:48<2:34:26,  5.20s/it] 11%|█         | 218/2000 [18:54<2:34:20,  5.20s/it] 11%|█         | 219/2000 [18:59<2:33:54,  5.18s/it] 11%|█         | 220/2000 [19:04<2:35:42,  5.25s/it]                                                    {'loss': 0.2299, 'grad_norm': 0.5932115316390991, 'learning_rate': 4.951761381552609e-05, 'epoch': 0.88}
 11%|█         | 220/2000 [19:04<2:35:42,  5.25s/it] 11%|█         | 221/2000 [19:10<2:37:00,  5.30s/it] 11%|█         | 222/2000 [19:15<2:37:10,  5.30s/it] 11%|█         | 223/2000 [19:20<2:37:05,  5.30s/it] 11%|█         | 224/2000 [19:25<2:34:59,  5.24s/it] 11%|█▏        | 225/2000 [19:31<2:35:01,  5.24s/it] 11%|█▏        | 226/2000 [19:36<2:38:28,  5.36s/it] 11%|█▏        | 227/2000 [19:41<2:35:54,  5.28s/it] 11%|█▏        | 228/2000 [19:46<2:33:44,  5.21s/it] 11%|█▏        | 229/2000 [19:51<2:32:13,  5.16s/it] 12%|█▏        | 230/2000 [19:57<2:37:17,  5.33s/it]                                                    {'loss': 0.2287, 'grad_norm': 0.574221134185791, 'learning_rate': 4.9433454490052675e-05, 'epoch': 0.92}
 12%|█▏        | 230/2000 [19:57<2:37:17,  5.33s/it] 12%|█▏        | 231/2000 [20:02<2:36:00,  5.29s/it] 12%|█▏        | 232/2000 [20:07<2:32:53,  5.19s/it] 12%|█▏        | 233/2000 [20:12<2:31:25,  5.14s/it] 12%|█▏        | 234/2000 [20:18<2:34:12,  5.24s/it] 12%|█▏        | 235/2000 [20:23<2:35:08,  5.27s/it] 12%|█▏        | 236/2000 [20:28<2:32:27,  5.19s/it] 12%|█▏        | 237/2000 [20:33<2:31:03,  5.14s/it] 12%|█▏        | 238/2000 [20:38<2:29:31,  5.09s/it] 12%|█▏        | 239/2000 [20:44<2:33:04,  5.22s/it] 12%|█▏        | 240/2000 [20:49<2:32:20,  5.19s/it]                                                    {'loss': 0.2252, 'grad_norm': 0.7053077220916748, 'learning_rate': 4.934261530208823e-05, 'epoch': 0.96}
 12%|█▏        | 240/2000 [20:49<2:32:20,  5.19s/it] 12%|█▏        | 241/2000 [20:54<2:34:26,  5.27s/it] 12%|█▏        | 242/2000 [20:59<2:31:31,  5.17s/it] 12%|█▏        | 243/2000 [21:04<2:30:48,  5.15s/it] 12%|█▏        | 244/2000 [21:09<2:30:05,  5.13s/it] 12%|█▏        | 245/2000 [21:14<2:27:22,  5.04s/it] 12%|█▏        | 246/2000 [21:19<2:27:34,  5.05s/it] 12%|█▏        | 247/2000 [21:24<2:29:17,  5.11s/it] 12%|█▏        | 248/2000 [21:30<2:32:01,  5.21s/it] 12%|█▏        | 249/2000 [21:35<2:30:25,  5.15s/it] 12%|█▎        | 250/2000 [21:40<2:32:21,  5.22s/it]                                                    {'loss': 0.2226, 'grad_norm': 0.6561416387557983, 'learning_rate': 4.9245121086159674e-05, 'epoch': 1.0}
 12%|█▎        | 250/2000 [21:40<2:32:21,  5.22s/it][INFO|trainer.py:3993] 2025-06-12 21:46:03,658 >> Saving model checkpoint to ./models/Qwen3-8B-llama-epoch8/checkpoint-250
[INFO|configuration_utils.py:696] 2025-06-12 21:46:03,856 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-12 21:46:03,858 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2356] 2025-06-12 21:46:05,039 >> chat template saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-250/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-12 21:46:05,054 >> tokenizer config file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-12 21:46:05,063 >> Special tokens file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-250/special_tokens_map.json
 13%|█▎        | 251/2000 [21:50<3:10:04,  6.52s/it] 13%|█▎        | 252/2000 [21:55<2:56:59,  6.08s/it] 13%|█▎        | 253/2000 [22:00<2:52:34,  5.93s/it] 13%|█▎        | 254/2000 [22:06<2:45:52,  5.70s/it] 13%|█▎        | 255/2000 [22:11<2:41:04,  5.54s/it] 13%|█▎        | 256/2000 [22:16<2:36:02,  5.37s/it] 13%|█▎        | 257/2000 [22:22<2:39:40,  5.50s/it] 13%|█▎        | 258/2000 [22:27<2:36:07,  5.38s/it] 13%|█▎        | 259/2000 [22:32<2:34:00,  5.31s/it] 13%|█▎        | 260/2000 [22:37<2:34:55,  5.34s/it]                                                    {'loss': 0.2072, 'grad_norm': 0.6441370248794556, 'learning_rate': 4.91409984962122e-05, 'epoch': 1.04}
 13%|█▎        | 260/2000 [22:37<2:34:55,  5.34s/it] 13%|█▎        | 261/2000 [22:42<2:33:22,  5.29s/it] 13%|█▎        | 262/2000 [22:48<2:32:42,  5.27s/it] 13%|█▎        | 263/2000 [22:53<2:31:29,  5.23s/it] 13%|█▎        | 264/2000 [22:58<2:32:05,  5.26s/it] 13%|█▎        | 265/2000 [23:03<2:30:28,  5.20s/it] 13%|█▎        | 266/2000 [23:08<2:30:25,  5.20s/it] 13%|█▎        | 267/2000 [23:13<2:27:11,  5.10s/it] 13%|█▎        | 268/2000 [23:18<2:27:22,  5.11s/it] 13%|█▎        | 269/2000 [23:24<2:30:24,  5.21s/it] 14%|█▎        | 270/2000 [23:29<2:32:57,  5.30s/it]                                                    {'loss': 0.2076, 'grad_norm': 0.5972092747688293, 'learning_rate': 4.903027599832223e-05, 'epoch': 1.08}
 14%|█▎        | 270/2000 [23:29<2:32:57,  5.30s/it] 14%|█▎        | 271/2000 [23:35<2:34:05,  5.35s/it] 14%|█▎        | 272/2000 [23:40<2:31:17,  5.25s/it] 14%|█▎        | 273/2000 [23:45<2:32:16,  5.29s/it] 14%|█▎        | 274/2000 [23:50<2:30:57,  5.25s/it] 14%|█▍        | 275/2000 [23:56<2:34:49,  5.39s/it] 14%|█▍        | 276/2000 [24:02<2:35:59,  5.43s/it] 14%|█▍        | 277/2000 [24:07<2:34:20,  5.37s/it] 14%|█▍        | 278/2000 [24:12<2:34:15,  5.37s/it] 14%|█▍        | 279/2000 [24:17<2:31:07,  5.27s/it] 14%|█▍        | 280/2000 [24:22<2:31:01,  5.27s/it]                                                    {'loss': 0.2362, 'grad_norm': 0.6190429329872131, 'learning_rate': 4.891298386291513e-05, 'epoch': 1.12}
 14%|█▍        | 280/2000 [24:24<2:31:01,  5.27s/it] 14%|█▍        | 281/2000 [24:29<2:41:46,  5.65s/it] 14%|█▍        | 282/2000 [24:34<2:38:25,  5.53s/it] 14%|█▍        | 283/2000 [24:39<2:34:45,  5.41s/it] 14%|█▍        | 284/2000 [24:45<2:33:58,  5.38s/it] 14%|█▍        | 285/2000 [24:50<2:33:28,  5.37s/it] 14%|█▍        | 286/2000 [24:55<2:29:56,  5.25s/it] 14%|█▍        | 287/2000 [25:00<2:28:12,  5.19s/it] 14%|█▍        | 288/2000 [25:05<2:27:52,  5.18s/it] 14%|█▍        | 289/2000 [25:10<2:26:02,  5.12s/it] 14%|█▍        | 290/2000 [25:15<2:25:04,  5.09s/it]                                                    {'loss': 0.2461, 'grad_norm': 0.7586877942085266, 'learning_rate': 4.878915415648957e-05, 'epoch': 1.16}
 14%|█▍        | 290/2000 [25:16<2:25:04,  5.09s/it] 15%|█▍        | 291/2000 [25:20<2:24:47,  5.08s/it] 15%|█▍        | 292/2000 [25:25<2:24:27,  5.07s/it] 15%|█▍        | 293/2000 [25:30<2:23:49,  5.06s/it] 15%|█▍        | 294/2000 [25:36<2:27:07,  5.17s/it] 15%|█▍        | 295/2000 [25:41<2:30:44,  5.30s/it] 15%|█▍        | 296/2000 [25:47<2:30:26,  5.30s/it] 15%|█▍        | 297/2000 [25:51<2:25:37,  5.13s/it] 15%|█▍        | 298/2000 [25:56<2:23:02,  5.04s/it] 15%|█▍        | 299/2000 [26:01<2:20:54,  4.97s/it] 15%|█▌        | 300/2000 [26:06<2:21:39,  5.00s/it]                                                    {'loss': 0.1998, 'grad_norm': 0.7107453346252441, 'learning_rate': 4.865882073285086e-05, 'epoch': 1.2}
 15%|█▌        | 300/2000 [26:06<2:21:39,  5.00s/it] 15%|█▌        | 301/2000 [26:11<2:23:13,  5.06s/it] 15%|█▌        | 302/2000 [26:17<2:23:48,  5.08s/it] 15%|█▌        | 303/2000 [26:22<2:24:12,  5.10s/it] 15%|█▌        | 304/2000 [26:27<2:24:45,  5.12s/it] 15%|█▌        | 305/2000 [26:32<2:22:28,  5.04s/it] 15%|█▌        | 306/2000 [26:37<2:23:59,  5.10s/it] 15%|█▌        | 307/2000 [26:42<2:25:51,  5.17s/it] 15%|█▌        | 308/2000 [26:47<2:26:02,  5.18s/it] 15%|█▌        | 309/2000 [26:52<2:24:53,  5.14s/it] 16%|█▌        | 310/2000 [26:57<2:23:14,  5.09s/it]                                                    {'loss': 0.2099, 'grad_norm': 0.6511898040771484, 'learning_rate': 4.852201922385564e-05, 'epoch': 1.24}
 16%|█▌        | 310/2000 [26:58<2:23:14,  5.09s/it] 16%|█▌        | 311/2000 [27:04<2:32:42,  5.42s/it] 16%|█▌        | 312/2000 [27:09<2:29:51,  5.33s/it] 16%|█▌        | 313/2000 [27:14<2:27:51,  5.26s/it] 16%|█▌        | 314/2000 [27:19<2:29:06,  5.31s/it] 16%|█▌        | 315/2000 [27:25<2:28:58,  5.30s/it] 16%|█▌        | 316/2000 [27:29<2:25:21,  5.18s/it] 16%|█▌        | 317/2000 [27:34<2:23:07,  5.10s/it] 16%|█▌        | 318/2000 [27:40<2:23:27,  5.12s/it] 16%|█▌        | 319/2000 [27:45<2:24:14,  5.15s/it] 16%|█▌        | 320/2000 [27:50<2:22:25,  5.09s/it]                                                    {'loss': 0.2037, 'grad_norm': 0.6390143632888794, 'learning_rate': 4.837878702967052e-05, 'epoch': 1.28}
 16%|█▌        | 320/2000 [27:50<2:22:25,  5.09s/it] 16%|█▌        | 321/2000 [27:55<2:24:48,  5.17s/it] 16%|█▌        | 322/2000 [28:00<2:24:50,  5.18s/it] 16%|█▌        | 323/2000 [28:05<2:22:07,  5.08s/it] 16%|█▌        | 324/2000 [28:11<2:25:09,  5.20s/it] 16%|█▋        | 325/2000 [28:16<2:24:49,  5.19s/it] 16%|█▋        | 326/2000 [28:21<2:26:08,  5.24s/it] 16%|█▋        | 327/2000 [28:26<2:23:07,  5.13s/it] 16%|█▋        | 328/2000 [28:31<2:18:40,  4.98s/it] 16%|█▋        | 329/2000 [28:36<2:18:12,  4.96s/it] 16%|█▋        | 330/2000 [28:41<2:19:28,  5.01s/it]                                                    {'loss': 0.2103, 'grad_norm': 0.8550907373428345, 'learning_rate': 4.822916330854722e-05, 'epoch': 1.32}
 16%|█▋        | 330/2000 [28:41<2:19:28,  5.01s/it] 17%|█▋        | 331/2000 [28:46<2:21:27,  5.09s/it] 17%|█▋        | 332/2000 [28:51<2:23:00,  5.14s/it] 17%|█▋        | 333/2000 [28:56<2:22:31,  5.13s/it] 17%|█▋        | 334/2000 [29:01<2:20:13,  5.05s/it] 17%|█▋        | 335/2000 [29:06<2:20:26,  5.06s/it] 17%|█▋        | 336/2000 [29:11<2:18:56,  5.01s/it] 17%|█▋        | 337/2000 [29:16<2:21:13,  5.10s/it] 17%|█▋        | 338/2000 [29:22<2:22:37,  5.15s/it] 17%|█▋        | 339/2000 [29:27<2:24:01,  5.20s/it] 17%|█▋        | 340/2000 [29:32<2:22:39,  5.16s/it]                                                    {'loss': 0.209, 'grad_norm': 0.7868055105209351, 'learning_rate': 4.8073188966117126e-05, 'epoch': 1.36}
 17%|█▋        | 340/2000 [29:32<2:22:39,  5.16s/it] 17%|█▋        | 341/2000 [29:37<2:24:00,  5.21s/it] 17%|█▋        | 342/2000 [29:42<2:22:43,  5.16s/it] 17%|█▋        | 343/2000 [29:47<2:20:22,  5.08s/it] 17%|█▋        | 344/2000 [29:53<2:21:10,  5.12s/it] 17%|█▋        | 345/2000 [29:57<2:19:22,  5.05s/it] 17%|█▋        | 346/2000 [30:03<2:22:59,  5.19s/it] 17%|█▋        | 347/2000 [30:08<2:19:54,  5.08s/it] 17%|█▋        | 348/2000 [30:13<2:21:39,  5.15s/it] 17%|█▋        | 349/2000 [30:18<2:21:23,  5.14s/it] 18%|█▊        | 350/2000 [30:24<2:22:23,  5.18s/it]                                                    {'loss': 0.2395, 'grad_norm': 0.7472444772720337, 'learning_rate': 4.7910906644208054e-05, 'epoch': 1.4}
 18%|█▊        | 350/2000 [30:24<2:22:23,  5.18s/it] 18%|█▊        | 351/2000 [30:29<2:25:31,  5.29s/it] 18%|█▊        | 352/2000 [30:34<2:25:06,  5.28s/it] 18%|█▊        | 353/2000 [30:40<2:24:16,  5.26s/it] 18%|█▊        | 354/2000 [30:45<2:22:13,  5.18s/it] 18%|█▊        | 355/2000 [30:50<2:26:11,  5.33s/it] 18%|█▊        | 356/2000 [30:56<2:26:59,  5.36s/it] 18%|█▊        | 357/2000 [31:01<2:22:40,  5.21s/it] 18%|█▊        | 358/2000 [31:06<2:21:09,  5.16s/it] 18%|█▊        | 359/2000 [31:10<2:18:23,  5.06s/it] 18%|█▊        | 360/2000 [31:15<2:18:28,  5.07s/it]                                                    {'loss': 0.2004, 'grad_norm': 0.9519395232200623, 'learning_rate': 4.774236070918643e-05, 'epoch': 1.44}
 18%|█▊        | 360/2000 [31:15<2:18:28,  5.07s/it] 18%|█▊        | 361/2000 [31:21<2:20:18,  5.14s/it] 18%|█▊        | 362/2000 [31:26<2:20:53,  5.16s/it] 18%|█▊        | 363/2000 [31:31<2:19:17,  5.11s/it] 18%|█▊        | 364/2000 [31:36<2:19:11,  5.10s/it] 18%|█▊        | 365/2000 [31:41<2:16:18,  5.00s/it] 18%|█▊        | 366/2000 [31:46<2:17:25,  5.05s/it] 18%|█▊        | 367/2000 [31:51<2:20:31,  5.16s/it] 18%|█▊        | 368/2000 [31:56<2:18:31,  5.09s/it] 18%|█▊        | 369/2000 [32:02<2:19:56,  5.15s/it] 18%|█▊        | 370/2000 [32:07<2:20:44,  5.18s/it]                                                    {'loss': 0.194, 'grad_norm': 0.7228215336799622, 'learning_rate': 4.7567597239827974e-05, 'epoch': 1.48}
 18%|█▊        | 370/2000 [32:07<2:20:44,  5.18s/it] 19%|█▊        | 371/2000 [32:12<2:24:15,  5.31s/it] 19%|█▊        | 372/2000 [32:17<2:20:04,  5.16s/it] 19%|█▊        | 373/2000 [32:22<2:20:16,  5.17s/it] 19%|█▊        | 374/2000 [32:27<2:17:35,  5.08s/it] 19%|█▉        | 375/2000 [32:33<2:20:06,  5.17s/it] 19%|█▉        | 376/2000 [32:38<2:21:18,  5.22s/it] 19%|█▉        | 377/2000 [32:43<2:20:11,  5.18s/it] 19%|█▉        | 378/2000 [32:49<2:25:33,  5.38s/it] 19%|█▉        | 379/2000 [32:54<2:24:24,  5.34s/it] 19%|█▉        | 380/2000 [32:59<2:20:02,  5.19s/it]                                                    {'loss': 0.2248, 'grad_norm': 0.7804825901985168, 'learning_rate': 4.738666401472022e-05, 'epoch': 1.52}
 19%|█▉        | 380/2000 [33:00<2:20:02,  5.19s/it] 19%|█▉        | 381/2000 [33:05<2:24:54,  5.37s/it] 19%|█▉        | 382/2000 [33:10<2:24:34,  5.36s/it] 19%|█▉        | 383/2000 [33:15<2:23:02,  5.31s/it] 19%|█▉        | 384/2000 [33:21<2:28:05,  5.50s/it] 19%|█▉        | 385/2000 [33:27<2:25:37,  5.41s/it] 19%|█▉        | 386/2000 [33:33<2:30:02,  5.58s/it] 19%|█▉        | 387/2000 [33:38<2:27:45,  5.50s/it] 19%|█▉        | 388/2000 [33:43<2:26:14,  5.44s/it] 19%|█▉        | 389/2000 [33:48<2:24:43,  5.39s/it] 20%|█▉        | 390/2000 [33:54<2:24:16,  5.38s/it]                                                    {'loss': 0.2145, 'grad_norm': 0.6190008521080017, 'learning_rate': 4.719961049920027e-05, 'epoch': 1.56}
 20%|█▉        | 390/2000 [33:54<2:24:16,  5.38s/it] 20%|█▉        | 391/2000 [34:00<2:27:40,  5.51s/it] 20%|█▉        | 392/2000 [34:05<2:25:51,  5.44s/it] 20%|█▉        | 393/2000 [34:10<2:24:11,  5.38s/it] 20%|█▉        | 394/2000 [34:15<2:23:22,  5.36s/it] 20%|█▉        | 395/2000 [34:20<2:19:52,  5.23s/it] 20%|█▉        | 396/2000 [34:25<2:17:57,  5.16s/it] 20%|█▉        | 397/2000 [34:31<2:18:05,  5.17s/it] 20%|█▉        | 398/2000 [34:36<2:18:42,  5.20s/it] 20%|█▉        | 399/2000 [34:41<2:20:29,  5.27s/it] 20%|██        | 400/2000 [34:47<2:21:01,  5.29s/it]                                                    {'loss': 0.2048, 'grad_norm': 0.7045968174934387, 'learning_rate': 4.700648783183159e-05, 'epoch': 1.6}
 20%|██        | 400/2000 [34:47<2:21:01,  5.29s/it] 20%|██        | 401/2000 [34:53<2:27:35,  5.54s/it] 20%|██        | 402/2000 [34:57<2:21:03,  5.30s/it] 20%|██        | 403/2000 [35:02<2:18:08,  5.19s/it] 20%|██        | 404/2000 [35:08<2:20:20,  5.28s/it] 20%|██        | 405/2000 [35:13<2:15:56,  5.11s/it] 20%|██        | 406/2000 [35:17<2:14:19,  5.06s/it] 20%|██        | 407/2000 [35:23<2:16:52,  5.16s/it] 20%|██        | 408/2000 [35:28<2:16:44,  5.15s/it] 20%|██        | 409/2000 [35:33<2:15:13,  5.10s/it] 20%|██        | 410/2000 [35:38<2:16:45,  5.16s/it]                                                    {'loss': 0.22, 'grad_norm': 0.9870352745056152, 'learning_rate': 4.68073488104231e-05, 'epoch': 1.64}
 20%|██        | 410/2000 [35:38<2:16:45,  5.16s/it] 21%|██        | 411/2000 [35:45<2:29:09,  5.63s/it] 21%|██        | 412/2000 [35:50<2:24:22,  5.46s/it] 21%|██        | 413/2000 [35:55<2:20:39,  5.32s/it] 21%|██        | 414/2000 [36:00<2:16:09,  5.15s/it] 21%|██        | 415/2000 [36:05<2:16:48,  5.18s/it] 21%|██        | 416/2000 [36:11<2:19:00,  5.27s/it] 21%|██        | 417/2000 [36:16<2:17:51,  5.23s/it] 21%|██        | 418/2000 [36:21<2:15:32,  5.14s/it] 21%|██        | 419/2000 [36:26<2:18:01,  5.24s/it] 21%|██        | 420/2000 [36:32<2:20:41,  5.34s/it]                                                    {'loss': 0.1957, 'grad_norm': 0.6861119270324707, 'learning_rate': 4.660224787759486e-05, 'epoch': 1.68}
 21%|██        | 420/2000 [36:32<2:20:41,  5.34s/it] 21%|██        | 421/2000 [36:37<2:19:04,  5.28s/it] 21%|██        | 422/2000 [36:42<2:20:38,  5.35s/it] 21%|██        | 423/2000 [36:47<2:17:39,  5.24s/it] 21%|██        | 424/2000 [36:52<2:16:14,  5.19s/it] 21%|██▏       | 425/2000 [36:58<2:17:38,  5.24s/it] 21%|██▏       | 426/2000 [37:03<2:17:25,  5.24s/it] 21%|██▏       | 427/2000 [37:08<2:19:24,  5.32s/it] 21%|██▏       | 428/2000 [37:14<2:18:54,  5.30s/it] 21%|██▏       | 429/2000 [37:19<2:19:19,  5.32s/it] 22%|██▏       | 430/2000 [37:24<2:15:24,  5.17s/it]                                                    {'loss': 0.2224, 'grad_norm': 0.8248949646949768, 'learning_rate': 4.639124110589399e-05, 'epoch': 1.72}
 22%|██▏       | 430/2000 [37:24<2:15:24,  5.17s/it] 22%|██▏       | 431/2000 [37:29<2:13:27,  5.10s/it] 22%|██▏       | 432/2000 [37:34<2:12:59,  5.09s/it] 22%|██▏       | 433/2000 [37:39<2:15:35,  5.19s/it] 22%|██▏       | 434/2000 [37:44<2:13:30,  5.12s/it] 22%|██▏       | 435/2000 [37:49<2:13:49,  5.13s/it] 22%|██▏       | 436/2000 [37:55<2:15:34,  5.20s/it] 22%|██▏       | 437/2000 [38:00<2:12:57,  5.10s/it] 22%|██▏       | 438/2000 [38:05<2:14:03,  5.15s/it] 22%|██▏       | 439/2000 [38:10<2:15:31,  5.21s/it] 22%|██▏       | 440/2000 [38:16<2:15:57,  5.23s/it]                                                    {'loss': 0.2122, 'grad_norm': 0.7278608083724976, 'learning_rate': 4.617438618246498e-05, 'epoch': 1.76}
 22%|██▏       | 440/2000 [38:16<2:15:57,  5.23s/it] 22%|██▏       | 441/2000 [38:21<2:17:59,  5.31s/it] 22%|██▏       | 442/2000 [38:26<2:16:44,  5.27s/it] 22%|██▏       | 443/2000 [38:31<2:16:33,  5.26s/it] 22%|██▏       | 444/2000 [38:37<2:15:30,  5.23s/it] 22%|██▏       | 445/2000 [38:42<2:13:06,  5.14s/it] 22%|██▏       | 446/2000 [38:47<2:11:51,  5.09s/it] 22%|██▏       | 447/2000 [38:52<2:11:37,  5.09s/it] 22%|██▏       | 448/2000 [38:57<2:13:11,  5.15s/it] 22%|██▏       | 449/2000 [39:02<2:14:32,  5.20s/it] 22%|██▎       | 450/2000 [39:07<2:14:11,  5.19s/it]                                                    {'loss': 0.2017, 'grad_norm': 0.7908347845077515, 'learning_rate': 4.595174239327862e-05, 'epoch': 1.8}
 22%|██▎       | 450/2000 [39:07<2:14:11,  5.19s/it] 23%|██▎       | 451/2000 [39:13<2:14:54,  5.23s/it] 23%|██▎       | 452/2000 [39:18<2:11:58,  5.12s/it] 23%|██▎       | 453/2000 [39:23<2:12:28,  5.14s/it] 23%|██▎       | 454/2000 [39:28<2:12:22,  5.14s/it] 23%|██▎       | 455/2000 [39:33<2:10:06,  5.05s/it] 23%|██▎       | 456/2000 [39:38<2:12:01,  5.13s/it] 23%|██▎       | 457/2000 [39:43<2:11:16,  5.10s/it] 23%|██▎       | 458/2000 [39:48<2:08:31,  5.00s/it] 23%|██▎       | 459/2000 [39:53<2:12:43,  5.17s/it] 23%|██▎       | 460/2000 [39:59<2:12:27,  5.16s/it]                                                    {'loss': 0.1982, 'grad_norm': 0.8273302912712097, 'learning_rate': 4.572337060692379e-05, 'epoch': 1.84}
 23%|██▎       | 460/2000 [39:59<2:12:27,  5.16s/it] 23%|██▎       | 461/2000 [40:04<2:13:17,  5.20s/it] 23%|██▎       | 462/2000 [40:09<2:11:12,  5.12s/it] 23%|██▎       | 463/2000 [40:14<2:12:46,  5.18s/it] 23%|██▎       | 464/2000 [40:19<2:13:24,  5.21s/it] 23%|██▎       | 465/2000 [40:25<2:13:31,  5.22s/it] 23%|██▎       | 466/2000 [40:30<2:12:15,  5.17s/it] 23%|██▎       | 467/2000 [40:35<2:15:48,  5.32s/it] 23%|██▎       | 468/2000 [40:40<2:13:57,  5.25s/it] 23%|██▎       | 469/2000 [40:45<2:11:11,  5.14s/it] 24%|██▎       | 470/2000 [40:50<2:10:46,  5.13s/it]                                                    {'loss': 0.1695, 'grad_norm': 0.9659898281097412, 'learning_rate': 4.548933325796658e-05, 'epoch': 1.88}
 24%|██▎       | 470/2000 [40:50<2:10:46,  5.13s/it] 24%|██▎       | 471/2000 [40:56<2:12:17,  5.19s/it] 24%|██▎       | 472/2000 [41:01<2:15:49,  5.33s/it] 24%|██▎       | 473/2000 [41:07<2:15:07,  5.31s/it] 24%|██▎       | 474/2000 [41:12<2:15:43,  5.34s/it] 24%|██▍       | 475/2000 [41:17<2:16:06,  5.35s/it] 24%|██▍       | 476/2000 [41:23<2:14:36,  5.30s/it] 24%|██▍       | 477/2000 [41:27<2:09:33,  5.10s/it] 24%|██▍       | 478/2000 [41:32<2:09:05,  5.09s/it] 24%|██▍       | 479/2000 [41:38<2:10:09,  5.13s/it] 24%|██▍       | 480/2000 [41:43<2:10:33,  5.15s/it]                                                    {'loss': 0.2158, 'grad_norm': 0.6768594980239868, 'learning_rate': 4.524969432988138e-05, 'epoch': 1.92}
 24%|██▍       | 480/2000 [41:43<2:10:33,  5.15s/it] 24%|██▍       | 481/2000 [41:48<2:13:42,  5.28s/it] 24%|██▍       | 482/2000 [41:53<2:10:35,  5.16s/it] 24%|██▍       | 483/2000 [41:59<2:15:54,  5.38s/it] 24%|██▍       | 484/2000 [42:04<2:13:03,  5.27s/it] 24%|██▍       | 485/2000 [42:09<2:11:54,  5.22s/it] 24%|██▍       | 486/2000 [42:15<2:13:48,  5.30s/it] 24%|██▍       | 487/2000 [42:20<2:12:06,  5.24s/it] 24%|██▍       | 488/2000 [42:26<2:15:35,  5.38s/it] 24%|██▍       | 489/2000 [42:31<2:13:34,  5.30s/it] 24%|██▍       | 490/2000 [42:36<2:14:29,  5.34s/it]                                                    {'loss': 0.2303, 'grad_norm': 0.49688443541526794, 'learning_rate': 4.500451933755833e-05, 'epoch': 1.96}
 24%|██▍       | 490/2000 [42:37<2:14:29,  5.34s/it] 25%|██▍       | 491/2000 [42:42<2:20:49,  5.60s/it] 25%|██▍       | 492/2000 [42:47<2:15:04,  5.37s/it] 25%|██▍       | 493/2000 [42:52<2:14:05,  5.34s/it] 25%|██▍       | 494/2000 [42:57<2:11:16,  5.23s/it] 25%|██▍       | 495/2000 [43:03<2:11:58,  5.26s/it] 25%|██▍       | 496/2000 [43:08<2:10:02,  5.19s/it] 25%|██▍       | 497/2000 [43:13<2:09:32,  5.17s/it] 25%|██▍       | 498/2000 [43:18<2:10:10,  5.20s/it] 25%|██▍       | 499/2000 [43:23<2:11:07,  5.24s/it] 25%|██▌       | 500/2000 [43:28<2:09:14,  5.17s/it]                                                    {'loss': 0.2098, 'grad_norm': 0.7891145348548889, 'learning_rate': 4.4753875309392266e-05, 'epoch': 2.0}
 25%|██▌       | 500/2000 [43:29<2:09:14,  5.17s/it][INFO|trainer.py:3993] 2025-06-12 22:07:51,835 >> Saving model checkpoint to ./models/Qwen3-8B-llama-epoch8/checkpoint-500
[INFO|configuration_utils.py:696] 2025-06-12 22:07:51,899 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-12 22:07:51,901 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2356] 2025-06-12 22:07:53,381 >> chat template saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-12 22:07:53,390 >> tokenizer config file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-12 22:07:53,398 >> Special tokens file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-500/special_tokens_map.json
 25%|██▌       | 501/2000 [43:38<2:45:06,  6.61s/it] 25%|██▌       | 502/2000 [43:44<2:36:30,  6.27s/it] 25%|██▌       | 503/2000 [43:49<2:28:05,  5.94s/it] 25%|██▌       | 504/2000 [43:54<2:22:58,  5.73s/it] 25%|██▌       | 505/2000 [44:00<2:19:24,  5.60s/it] 25%|██▌       | 506/2000 [44:05<2:17:41,  5.53s/it] 25%|██▌       | 507/2000 [44:10<2:15:26,  5.44s/it] 25%|██▌       | 508/2000 [44:15<2:12:48,  5.34s/it] 25%|██▌       | 509/2000 [44:20<2:10:34,  5.25s/it] 26%|██▌       | 510/2000 [44:25<2:07:46,  5.15s/it]                                                    {'loss': 0.1571, 'grad_norm': 0.6379923820495605, 'learning_rate': 4.449783076895783e-05, 'epoch': 2.04}
 26%|██▌       | 510/2000 [44:25<2:07:46,  5.15s/it] 26%|██▌       | 511/2000 [44:30<2:07:06,  5.12s/it] 26%|██▌       | 512/2000 [44:35<2:05:25,  5.06s/it] 26%|██▌       | 513/2000 [44:41<2:07:44,  5.15s/it] 26%|██▌       | 514/2000 [44:46<2:09:27,  5.23s/it] 26%|██▌       | 515/2000 [44:51<2:06:14,  5.10s/it] 26%|██▌       | 516/2000 [44:56<2:07:16,  5.15s/it] 26%|██▌       | 517/2000 [45:01<2:05:50,  5.09s/it] 26%|██▌       | 518/2000 [45:06<2:02:37,  4.96s/it] 26%|██▌       | 519/2000 [45:11<2:06:07,  5.11s/it] 26%|██▌       | 520/2000 [45:17<2:07:40,  5.18s/it]                                                    {'loss': 0.1413, 'grad_norm': 0.7233333587646484, 'learning_rate': 4.42364557162758e-05, 'epoch': 2.08}
 26%|██▌       | 520/2000 [45:17<2:07:40,  5.18s/it] 26%|██▌       | 521/2000 [45:22<2:07:08,  5.16s/it] 26%|██▌       | 522/2000 [45:27<2:07:36,  5.18s/it] 26%|██▌       | 523/2000 [45:32<2:06:38,  5.14s/it] 26%|██▌       | 524/2000 [45:37<2:07:32,  5.18s/it] 26%|██▋       | 525/2000 [45:42<2:05:35,  5.11s/it] 26%|██▋       | 526/2000 [45:47<2:04:38,  5.07s/it] 26%|██▋       | 527/2000 [45:53<2:07:13,  5.18s/it] 26%|██▋       | 528/2000 [45:58<2:08:10,  5.22s/it] 26%|██▋       | 529/2000 [46:03<2:08:14,  5.23s/it] 26%|██▋       | 530/2000 [46:08<2:06:30,  5.16s/it]                                                    {'loss': 0.1319, 'grad_norm': 0.7255476713180542, 'learning_rate': 4.396982160867575e-05, 'epoch': 2.12}
 26%|██▋       | 530/2000 [46:08<2:06:30,  5.16s/it] 27%|██▋       | 531/2000 [46:13<2:04:52,  5.10s/it] 27%|██▋       | 532/2000 [46:19<2:07:04,  5.19s/it] 27%|██▋       | 533/2000 [46:23<2:04:50,  5.11s/it] 27%|██▋       | 534/2000 [46:29<2:05:16,  5.13s/it] 27%|██▋       | 535/2000 [46:34<2:09:57,  5.32s/it] 27%|██▋       | 536/2000 [46:39<2:07:16,  5.22s/it] 27%|██▋       | 537/2000 [46:45<2:11:35,  5.40s/it] 27%|██▋       | 538/2000 [46:51<2:12:14,  5.43s/it] 27%|██▋       | 539/2000 [46:56<2:10:03,  5.34s/it] 27%|██▋       | 540/2000 [47:01<2:11:08,  5.39s/it]                                                    {'loss': 0.1575, 'grad_norm': 0.7741148471832275, 'learning_rate': 4.369800134126039e-05, 'epoch': 2.16}
 27%|██▋       | 540/2000 [47:03<2:11:08,  5.39s/it] 27%|██▋       | 541/2000 [47:08<2:21:04,  5.80s/it] 27%|██▋       | 542/2000 [47:13<2:14:55,  5.55s/it] 27%|██▋       | 543/2000 [47:18<2:08:29,  5.29s/it] 27%|██▋       | 544/2000 [47:24<2:12:09,  5.45s/it] 27%|██▋       | 545/2000 [47:29<2:10:08,  5.37s/it] 27%|██▋       | 546/2000 [47:34<2:08:31,  5.30s/it] 27%|██▋       | 547/2000 [47:39<2:08:09,  5.29s/it] 27%|██▋       | 548/2000 [47:44<2:06:23,  5.22s/it] 27%|██▋       | 549/2000 [47:49<2:03:38,  5.11s/it] 28%|██▊       | 550/2000 [47:54<2:04:07,  5.14s/it]                                                    {'loss': 0.1435, 'grad_norm': 0.7695088982582092, 'learning_rate': 4.342106922697669e-05, 'epoch': 2.2}
 28%|██▊       | 550/2000 [47:55<2:04:07,  5.14s/it] 28%|██▊       | 551/2000 [48:01<2:12:25,  5.48s/it] 28%|██▊       | 552/2000 [48:06<2:11:39,  5.46s/it] 28%|██▊       | 553/2000 [48:11<2:09:58,  5.39s/it] 28%|██▊       | 554/2000 [48:16<2:05:38,  5.21s/it] 28%|██▊       | 555/2000 [48:21<2:05:24,  5.21s/it] 28%|██▊       | 556/2000 [48:27<2:06:29,  5.26s/it] 28%|██▊       | 557/2000 [48:32<2:10:59,  5.45s/it] 28%|██▊       | 558/2000 [48:37<2:06:39,  5.27s/it] 28%|██▊       | 559/2000 [48:43<2:06:27,  5.27s/it] 28%|██▊       | 560/2000 [48:48<2:07:38,  5.32s/it]                                                    {'loss': 0.1576, 'grad_norm': 0.9368693828582764, 'learning_rate': 4.313910097629959e-05, 'epoch': 2.24}
 28%|██▊       | 560/2000 [48:48<2:07:38,  5.32s/it] 28%|██▊       | 561/2000 [48:53<2:08:05,  5.34s/it] 28%|██▊       | 562/2000 [48:58<2:06:10,  5.26s/it] 28%|██▊       | 563/2000 [49:04<2:06:31,  5.28s/it] 28%|██▊       | 564/2000 [49:09<2:04:47,  5.21s/it] 28%|██▊       | 565/2000 [49:14<2:03:27,  5.16s/it] 28%|██▊       | 566/2000 [49:19<2:03:53,  5.18s/it] 28%|██▊       | 567/2000 [49:24<2:02:24,  5.13s/it] 28%|██▊       | 568/2000 [49:29<2:02:52,  5.15s/it] 28%|██▊       | 569/2000 [49:35<2:06:07,  5.29s/it] 28%|██▊       | 570/2000 [49:40<2:02:19,  5.13s/it]                                                    {'loss': 0.159, 'grad_norm': 0.7443162798881531, 'learning_rate': 4.2852173676533356e-05, 'epoch': 2.28}
 28%|██▊       | 570/2000 [49:40<2:02:19,  5.13s/it] 29%|██▊       | 571/2000 [49:45<2:02:38,  5.15s/it] 29%|██▊       | 572/2000 [49:50<2:03:14,  5.18s/it] 29%|██▊       | 573/2000 [49:55<2:03:27,  5.19s/it] 29%|██▊       | 574/2000 [50:01<2:04:50,  5.25s/it] 29%|██▉       | 575/2000 [50:06<2:02:58,  5.18s/it] 29%|██▉       | 576/2000 [50:11<2:02:25,  5.16s/it] 29%|██▉       | 577/2000 [50:16<2:03:47,  5.22s/it] 29%|██▉       | 578/2000 [50:21<2:02:16,  5.16s/it] 29%|██▉       | 579/2000 [50:27<2:04:26,  5.25s/it] 29%|██▉       | 580/2000 [50:32<2:04:40,  5.27s/it]                                                    {'loss': 0.1766, 'grad_norm': 0.8167752027511597, 'learning_rate': 4.256036577073681e-05, 'epoch': 2.32}
 29%|██▉       | 580/2000 [50:33<2:04:40,  5.27s/it] 29%|██▉       | 581/2000 [50:38<2:10:59,  5.54s/it] 29%|██▉       | 582/2000 [50:43<2:07:58,  5.41s/it] 29%|██▉       | 583/2000 [50:49<2:08:24,  5.44s/it] 29%|██▉       | 584/2000 [50:54<2:06:08,  5.34s/it] 29%|██▉       | 585/2000 [50:59<2:03:54,  5.25s/it] 29%|██▉       | 586/2000 [51:04<2:02:11,  5.19s/it] 29%|██▉       | 587/2000 [51:09<2:01:47,  5.17s/it] 29%|██▉       | 588/2000 [51:14<2:00:05,  5.10s/it] 29%|██▉       | 589/2000 [51:19<1:58:29,  5.04s/it] 30%|██▉       | 590/2000 [51:24<1:57:25,  5.00s/it]                                                    {'loss': 0.1516, 'grad_norm': 1.2854719161987305, 'learning_rate': 4.2263757036277705e-05, 'epoch': 2.36}
 30%|██▉       | 590/2000 [51:24<1:57:25,  5.00s/it] 30%|██▉       | 591/2000 [51:29<1:58:22,  5.04s/it] 30%|██▉       | 592/2000 [51:34<2:00:29,  5.13s/it] 30%|██▉       | 593/2000 [51:40<2:02:15,  5.21s/it] 30%|██▉       | 594/2000 [51:45<2:05:33,  5.36s/it] 30%|██▉       | 595/2000 [51:51<2:06:48,  5.42s/it] 30%|██▉       | 596/2000 [51:56<2:03:22,  5.27s/it] 30%|██▉       | 597/2000 [52:01<2:02:20,  5.23s/it] 30%|██▉       | 598/2000 [52:07<2:04:05,  5.31s/it] 30%|██▉       | 599/2000 [52:12<2:04:49,  5.35s/it] 30%|███       | 600/2000 [52:18<2:06:11,  5.41s/it]                                                    {'loss': 0.1526, 'grad_norm': 1.026908278465271, 'learning_rate': 4.1962428563022414e-05, 'epoch': 2.4}
 30%|███       | 600/2000 [52:18<2:06:11,  5.41s/it] 30%|███       | 601/2000 [52:23<2:05:35,  5.39s/it] 30%|███       | 602/2000 [52:28<2:00:40,  5.18s/it] 30%|███       | 603/2000 [52:33<2:01:43,  5.23s/it] 30%|███       | 604/2000 [52:38<2:01:45,  5.23s/it] 30%|███       | 605/2000 [52:43<2:00:59,  5.20s/it] 30%|███       | 606/2000 [52:48<1:58:09,  5.09s/it] 30%|███       | 607/2000 [52:54<2:00:33,  5.19s/it] 30%|███       | 608/2000 [52:58<1:57:33,  5.07s/it] 30%|███       | 609/2000 [53:03<1:56:16,  5.02s/it] 30%|███       | 610/2000 [53:09<1:58:17,  5.11s/it]                                                    {'loss': 0.1879, 'grad_norm': 1.0801889896392822, 'learning_rate': 4.165646273116681e-05, 'epoch': 2.44}
 30%|███       | 610/2000 [53:09<1:58:17,  5.11s/it] 31%|███       | 611/2000 [53:14<1:58:38,  5.12s/it] 31%|███       | 612/2000 [53:19<1:59:17,  5.16s/it] 31%|███       | 613/2000 [53:24<1:59:09,  5.15s/it] 31%|███       | 614/2000 [53:29<1:59:33,  5.18s/it] 31%|███       | 615/2000 [53:35<2:01:27,  5.26s/it] 31%|███       | 616/2000 [53:40<2:01:14,  5.26s/it] 31%|███       | 617/2000 [53:45<1:57:35,  5.10s/it] 31%|███       | 618/2000 [53:50<1:56:49,  5.07s/it] 31%|███       | 619/2000 [53:55<1:57:27,  5.10s/it] 31%|███       | 620/2000 [54:00<1:57:32,  5.11s/it]                                                    {'loss': 0.1642, 'grad_norm': 1.149841070175171, 'learning_rate': 4.134594318871423e-05, 'epoch': 2.48}
 31%|███       | 620/2000 [54:00<1:57:32,  5.11s/it] 31%|███       | 621/2000 [54:06<2:04:44,  5.43s/it] 31%|███       | 622/2000 [54:11<2:03:32,  5.38s/it] 31%|███       | 623/2000 [54:17<2:03:26,  5.38s/it] 31%|███       | 624/2000 [54:22<2:01:26,  5.30s/it] 31%|███▏      | 625/2000 [54:27<2:00:25,  5.25s/it] 31%|███▏      | 626/2000 [54:32<1:59:29,  5.22s/it] 31%|███▏      | 627/2000 [54:37<1:58:24,  5.17s/it] 31%|███▏      | 628/2000 [54:43<2:00:59,  5.29s/it] 31%|███▏      | 629/2000 [54:48<1:59:20,  5.22s/it] 32%|███▏      | 630/2000 [54:53<1:59:21,  5.23s/it]                                                    {'loss': 0.1578, 'grad_norm': 1.2051423788070679, 'learning_rate': 4.1030954828607095e-05, 'epoch': 2.52}
 32%|███▏      | 630/2000 [54:55<1:59:21,  5.23s/it] 32%|███▏      | 631/2000 [55:00<2:11:29,  5.76s/it] 32%|███▏      | 632/2000 [55:05<2:07:48,  5.61s/it] 32%|███▏      | 633/2000 [55:10<2:03:12,  5.41s/it] 32%|███▏      | 634/2000 [55:16<2:01:46,  5.35s/it] 32%|███▏      | 635/2000 [55:21<1:59:13,  5.24s/it] 32%|███▏      | 636/2000 [55:26<1:58:31,  5.21s/it] 32%|███▏      | 637/2000 [55:31<1:59:31,  5.26s/it] 32%|███▏      | 638/2000 [55:37<2:01:29,  5.35s/it] 32%|███▏      | 639/2000 [55:42<1:59:43,  5.28s/it] 32%|███▏      | 640/2000 [55:47<1:58:40,  5.24s/it]                                                    {'loss': 0.142, 'grad_norm': 0.8225200772285461, 'learning_rate': 4.07115837655179e-05, 'epoch': 2.56}
 32%|███▏      | 640/2000 [55:47<1:58:40,  5.24s/it] 32%|███▏      | 641/2000 [55:53<2:01:41,  5.37s/it] 32%|███▏      | 642/2000 [55:58<1:59:59,  5.30s/it] 32%|███▏      | 643/2000 [56:03<1:58:02,  5.22s/it] 32%|███▏      | 644/2000 [56:08<1:57:55,  5.22s/it] 32%|███▏      | 645/2000 [56:13<1:54:58,  5.09s/it] 32%|███▏      | 646/2000 [56:18<1:58:04,  5.23s/it] 32%|███▏      | 647/2000 [56:23<1:54:21,  5.07s/it] 32%|███▏      | 648/2000 [56:28<1:55:25,  5.12s/it] 32%|███▏      | 649/2000 [56:34<1:59:48,  5.32s/it] 32%|███▎      | 650/2000 [56:39<1:56:23,  5.17s/it]                                                    {'loss': 0.1636, 'grad_norm': 0.7248509526252747, 'learning_rate': 4.0387917312306414e-05, 'epoch': 2.6}
 32%|███▎      | 650/2000 [56:39<1:56:23,  5.17s/it] 33%|███▎      | 651/2000 [56:44<1:58:26,  5.27s/it] 33%|███▎      | 652/2000 [56:49<1:56:37,  5.19s/it] 33%|███▎      | 653/2000 [56:54<1:54:39,  5.11s/it] 33%|███▎      | 654/2000 [57:00<1:56:11,  5.18s/it] 33%|███▎      | 655/2000 [57:05<1:55:46,  5.16s/it] 33%|███▎      | 656/2000 [57:10<1:55:33,  5.16s/it] 33%|███▎      | 657/2000 [57:15<1:54:18,  5.11s/it] 33%|███▎      | 658/2000 [57:21<1:58:06,  5.28s/it] 33%|███▎      | 659/2000 [57:26<1:59:27,  5.35s/it] 33%|███▎      | 660/2000 [57:32<2:01:07,  5.42s/it]                                                    {'loss': 0.1607, 'grad_norm': 1.0849543809890747, 'learning_rate': 4.006004395614913e-05, 'epoch': 2.64}
 33%|███▎      | 660/2000 [57:32<2:01:07,  5.42s/it] 33%|███▎      | 661/2000 [57:37<1:59:33,  5.36s/it] 33%|███▎      | 662/2000 [57:42<1:57:08,  5.25s/it] 33%|███▎      | 663/2000 [57:47<1:59:10,  5.35s/it] 33%|███▎      | 664/2000 [57:53<1:56:58,  5.25s/it] 33%|███▎      | 665/2000 [57:58<1:55:38,  5.20s/it] 33%|███▎      | 666/2000 [58:03<1:55:52,  5.21s/it] 33%|███▎      | 667/2000 [58:08<1:56:11,  5.23s/it] 33%|███▎      | 668/2000 [58:13<1:54:24,  5.15s/it] 33%|███▎      | 669/2000 [58:18<1:51:26,  5.02s/it] 34%|███▎      | 670/2000 [58:23<1:51:37,  5.04s/it]                                                    {'loss': 0.1565, 'grad_norm': 1.0299516916275024, 'learning_rate': 3.972805333434784e-05, 'epoch': 2.68}
 34%|███▎      | 670/2000 [58:25<1:51:37,  5.04s/it] 34%|███▎      | 671/2000 [58:30<2:03:40,  5.58s/it] 34%|███▎      | 672/2000 [58:35<1:58:51,  5.37s/it] 34%|███▎      | 673/2000 [58:40<1:57:00,  5.29s/it] 34%|███▎      | 674/2000 [58:45<1:57:14,  5.30s/it] 34%|███▍      | 675/2000 [58:50<1:56:14,  5.26s/it] 34%|███▍      | 676/2000 [58:55<1:55:42,  5.24s/it] 34%|███▍      | 677/2000 [59:01<1:55:02,  5.22s/it] 34%|███▍      | 678/2000 [59:06<1:53:53,  5.17s/it] 34%|███▍      | 679/2000 [59:11<1:53:00,  5.13s/it] 34%|███▍      | 680/2000 [59:16<1:53:58,  5.18s/it]                                                    {'loss': 0.1652, 'grad_norm': 1.0227798223495483, 'learning_rate': 3.9392036209823644e-05, 'epoch': 2.72}
 34%|███▍      | 680/2000 [59:18<1:53:58,  5.18s/it] 34%|███▍      | 681/2000 [59:23<2:03:43,  5.63s/it] 34%|███▍      | 682/2000 [59:27<1:57:33,  5.35s/it] 34%|███▍      | 683/2000 [59:33<1:56:21,  5.30s/it] 34%|███▍      | 684/2000 [59:38<1:58:59,  5.42s/it] 34%|███▍      | 685/2000 [59:44<1:58:14,  5.40s/it] 34%|███▍      | 686/2000 [59:48<1:54:17,  5.22s/it] 34%|███▍      | 687/2000 [59:53<1:52:46,  5.15s/it] 34%|███▍      | 688/2000 [59:58<1:51:17,  5.09s/it] 34%|███▍      | 689/2000 [1:00:04<1:52:57,  5.17s/it] 34%|███▍      | 690/2000 [1:00:09<1:54:39,  5.25s/it]                                                      {'loss': 0.1615, 'grad_norm': 0.8402481079101562, 'learning_rate': 3.905208444630327e-05, 'epoch': 2.76}
 34%|███▍      | 690/2000 [1:00:09<1:54:39,  5.25s/it] 35%|███▍      | 691/2000 [1:00:14<1:52:31,  5.16s/it] 35%|███▍      | 692/2000 [1:00:20<1:55:00,  5.28s/it] 35%|███▍      | 693/2000 [1:00:25<1:56:10,  5.33s/it] 35%|███▍      | 694/2000 [1:00:30<1:53:16,  5.20s/it] 35%|███▍      | 695/2000 [1:00:35<1:51:32,  5.13s/it] 35%|███▍      | 696/2000 [1:00:40<1:52:17,  5.17s/it] 35%|███▍      | 697/2000 [1:00:45<1:52:27,  5.18s/it] 35%|███▍      | 698/2000 [1:00:50<1:51:21,  5.13s/it] 35%|███▍      | 699/2000 [1:00:55<1:50:18,  5.09s/it] 35%|███▌      | 700/2000 [1:01:00<1:50:21,  5.09s/it]                                                      {'loss': 0.1382, 'grad_norm': 1.055949330329895, 'learning_rate': 3.870829098320446e-05, 'epoch': 2.8}
 35%|███▌      | 700/2000 [1:01:01<1:50:21,  5.09s/it] 35%|███▌      | 701/2000 [1:01:06<1:51:40,  5.16s/it] 35%|███▌      | 702/2000 [1:01:11<1:52:08,  5.18s/it] 35%|███▌      | 703/2000 [1:01:16<1:53:29,  5.25s/it] 35%|███▌      | 704/2000 [1:01:21<1:51:46,  5.17s/it] 35%|███▌      | 705/2000 [1:01:27<1:51:09,  5.15s/it] 35%|███▌      | 706/2000 [1:01:32<1:50:05,  5.10s/it] 35%|███▌      | 707/2000 [1:01:37<1:49:08,  5.06s/it] 35%|███▌      | 708/2000 [1:01:41<1:48:14,  5.03s/it] 35%|███▌      | 709/2000 [1:01:46<1:48:03,  5.02s/it] 36%|███▌      | 710/2000 [1:01:52<1:49:37,  5.10s/it]                                                      {'loss': 0.1485, 'grad_norm': 1.1515161991119385, 'learning_rate': 3.8360749810227286e-05, 'epoch': 2.84}
 36%|███▌      | 710/2000 [1:01:53<1:49:37,  5.10s/it] 36%|███▌      | 711/2000 [1:01:58<1:54:41,  5.34s/it] 36%|███▌      | 712/2000 [1:02:03<1:52:50,  5.26s/it] 36%|███▌      | 713/2000 [1:02:08<1:53:12,  5.28s/it] 36%|███▌      | 714/2000 [1:02:13<1:51:39,  5.21s/it] 36%|███▌      | 715/2000 [1:02:18<1:50:55,  5.18s/it] 36%|███▌      | 716/2000 [1:02:23<1:50:09,  5.15s/it] 36%|███▌      | 717/2000 [1:02:28<1:49:24,  5.12s/it] 36%|███▌      | 718/2000 [1:02:34<1:53:33,  5.31s/it] 36%|███▌      | 719/2000 [1:02:40<1:55:21,  5.40s/it] 36%|███▌      | 720/2000 [1:02:45<1:52:48,  5.29s/it]                                                      {'loss': 0.1589, 'grad_norm': 0.9038056135177612, 'learning_rate': 3.800955594165825e-05, 'epoch': 2.88}
 36%|███▌      | 720/2000 [1:02:47<1:52:48,  5.29s/it] 36%|███▌      | 721/2000 [1:02:52<2:06:06,  5.92s/it] 36%|███▌      | 722/2000 [1:02:57<2:00:43,  5.67s/it] 36%|███▌      | 723/2000 [1:03:02<1:57:39,  5.53s/it] 36%|███▌      | 724/2000 [1:03:07<1:54:30,  5.38s/it] 36%|███▋      | 725/2000 [1:03:12<1:50:54,  5.22s/it] 36%|███▋      | 726/2000 [1:03:17<1:47:31,  5.06s/it] 36%|███▋      | 727/2000 [1:03:22<1:46:17,  5.01s/it] 36%|███▋      | 728/2000 [1:03:27<1:47:54,  5.09s/it] 36%|███▋      | 729/2000 [1:03:32<1:47:00,  5.05s/it] 36%|███▋      | 730/2000 [1:03:37<1:47:42,  5.09s/it]                                                      {'loss': 0.1494, 'grad_norm': 1.1564466953277588, 'learning_rate': 3.7654805390394366e-05, 'epoch': 2.92}
 36%|███▋      | 730/2000 [1:03:37<1:47:42,  5.09s/it] 37%|███▋      | 731/2000 [1:03:42<1:47:34,  5.09s/it] 37%|███▋      | 732/2000 [1:03:47<1:47:46,  5.10s/it] 37%|███▋      | 733/2000 [1:03:53<1:47:32,  5.09s/it] 37%|███▋      | 734/2000 [1:03:57<1:45:33,  5.00s/it] 37%|███▋      | 735/2000 [1:04:03<1:46:45,  5.06s/it] 37%|███▋      | 736/2000 [1:04:07<1:45:36,  5.01s/it] 37%|███▋      | 737/2000 [1:04:13<1:47:27,  5.10s/it] 37%|███▋      | 738/2000 [1:04:18<1:47:31,  5.11s/it] 37%|███▋      | 739/2000 [1:04:23<1:49:05,  5.19s/it] 37%|███▋      | 740/2000 [1:04:28<1:48:28,  5.17s/it]                                                      {'loss': 0.1554, 'grad_norm': 0.9284684062004089, 'learning_rate': 3.72965951416942e-05, 'epoch': 2.96}
 37%|███▋      | 740/2000 [1:04:29<1:48:28,  5.17s/it] 37%|███▋      | 741/2000 [1:04:34<1:51:15,  5.30s/it] 37%|███▋      | 742/2000 [1:04:39<1:48:41,  5.18s/it] 37%|███▋      | 743/2000 [1:04:44<1:49:12,  5.21s/it] 37%|███▋      | 744/2000 [1:04:50<1:50:22,  5.27s/it] 37%|███▋      | 745/2000 [1:04:55<1:50:14,  5.27s/it] 37%|███▋      | 746/2000 [1:05:00<1:49:42,  5.25s/it] 37%|███▋      | 747/2000 [1:05:05<1:49:31,  5.24s/it] 37%|███▋      | 748/2000 [1:05:11<1:49:12,  5.23s/it] 37%|███▋      | 749/2000 [1:05:15<1:47:22,  5.15s/it] 38%|███▊      | 750/2000 [1:05:20<1:44:59,  5.04s/it]                                                      {'loss': 0.1629, 'grad_norm': 0.9886398315429688, 'learning_rate': 3.693502312666304e-05, 'epoch': 3.0}
 38%|███▊      | 750/2000 [1:05:20<1:44:59,  5.04s/it][INFO|trainer.py:3993] 2025-06-12 22:29:43,640 >> Saving model checkpoint to ./models/Qwen3-8B-llama-epoch8/checkpoint-750
[INFO|configuration_utils.py:696] 2025-06-12 22:29:43,712 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-12 22:29:43,714 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2356] 2025-06-12 22:29:44,781 >> chat template saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-750/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-12 22:29:44,798 >> tokenizer config file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-12 22:29:44,811 >> Special tokens file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-750/special_tokens_map.json
 38%|███▊      | 751/2000 [1:05:30<2:13:55,  6.43s/it] 38%|███▊      | 752/2000 [1:05:35<2:03:16,  5.93s/it] 38%|███▊      | 753/2000 [1:05:40<1:58:07,  5.68s/it] 38%|███▊      | 754/2000 [1:05:45<1:52:31,  5.42s/it] 38%|███▊      | 755/2000 [1:05:50<1:49:45,  5.29s/it] 38%|███▊      | 756/2000 [1:05:55<1:50:38,  5.34s/it] 38%|███▊      | 757/2000 [1:06:00<1:48:48,  5.25s/it] 38%|███▊      | 758/2000 [1:06:05<1:46:58,  5.17s/it] 38%|███▊      | 759/2000 [1:06:10<1:45:19,  5.09s/it] 38%|███▊      | 760/2000 [1:06:16<1:48:33,  5.25s/it]                                                      {'loss': 0.091, 'grad_norm': 0.6769514083862305, 'learning_rate': 3.65701881954795e-05, 'epoch': 3.04}
 38%|███▊      | 760/2000 [1:06:16<1:48:33,  5.25s/it] 38%|███▊      | 761/2000 [1:06:20<1:45:47,  5.12s/it] 38%|███▊      | 762/2000 [1:06:26<1:46:22,  5.16s/it] 38%|███▊      | 763/2000 [1:06:31<1:48:19,  5.25s/it] 38%|███▊      | 764/2000 [1:06:36<1:46:55,  5.19s/it] 38%|███▊      | 765/2000 [1:06:42<1:48:20,  5.26s/it] 38%|███▊      | 766/2000 [1:06:47<1:47:34,  5.23s/it] 38%|███▊      | 767/2000 [1:06:52<1:46:14,  5.17s/it] 38%|███▊      | 768/2000 [1:06:57<1:48:48,  5.30s/it] 38%|███▊      | 769/2000 [1:07:03<1:47:59,  5.26s/it] 38%|███▊      | 770/2000 [1:07:08<1:47:46,  5.26s/it]                                                      {'loss': 0.0904, 'grad_norm': 1.1413426399230957, 'learning_rate': 3.6202190090370944e-05, 'epoch': 3.08}
 38%|███▊      | 770/2000 [1:07:08<1:47:46,  5.26s/it] 39%|███▊      | 771/2000 [1:07:14<1:50:24,  5.39s/it] 39%|███▊      | 772/2000 [1:07:19<1:52:56,  5.52s/it] 39%|███▊      | 773/2000 [1:07:24<1:48:53,  5.32s/it] 39%|███▊      | 774/2000 [1:07:29<1:47:49,  5.28s/it] 39%|███▉      | 775/2000 [1:07:34<1:45:34,  5.17s/it] 39%|███▉      | 776/2000 [1:07:39<1:43:53,  5.09s/it] 39%|███▉      | 777/2000 [1:07:44<1:44:55,  5.15s/it] 39%|███▉      | 778/2000 [1:07:49<1:43:15,  5.07s/it] 39%|███▉      | 779/2000 [1:07:55<1:46:26,  5.23s/it] 39%|███▉      | 780/2000 [1:08:01<1:48:29,  5.34s/it]                                                      {'loss': 0.1105, 'grad_norm': 1.5558580160140991, 'learning_rate': 3.5831129418344845e-05, 'epoch': 3.12}
 39%|███▉      | 780/2000 [1:08:01<1:48:29,  5.34s/it] 39%|███▉      | 781/2000 [1:08:06<1:50:18,  5.43s/it] 39%|███▉      | 782/2000 [1:08:11<1:48:56,  5.37s/it] 39%|███▉      | 783/2000 [1:08:17<1:48:15,  5.34s/it] 39%|███▉      | 784/2000 [1:08:22<1:46:08,  5.24s/it] 39%|███▉      | 785/2000 [1:08:27<1:46:31,  5.26s/it] 39%|███▉      | 786/2000 [1:08:32<1:43:46,  5.13s/it] 39%|███▉      | 787/2000 [1:08:37<1:44:21,  5.16s/it] 39%|███▉      | 788/2000 [1:08:42<1:45:09,  5.21s/it] 39%|███▉      | 789/2000 [1:08:47<1:42:49,  5.09s/it] 40%|███▉      | 790/2000 [1:08:52<1:43:48,  5.15s/it]                                                      {'loss': 0.1093, 'grad_norm': 0.9596624970436096, 'learning_rate': 3.545710762368392e-05, 'epoch': 3.16}
 40%|███▉      | 790/2000 [1:08:53<1:43:48,  5.15s/it] 40%|███▉      | 791/2000 [1:08:58<1:48:12,  5.37s/it] 40%|███▉      | 792/2000 [1:09:04<1:47:13,  5.33s/it] 40%|███▉      | 793/2000 [1:09:09<1:46:17,  5.28s/it] 40%|███▉      | 794/2000 [1:09:14<1:44:54,  5.22s/it] 40%|███▉      | 795/2000 [1:09:19<1:46:07,  5.28s/it] 40%|███▉      | 796/2000 [1:09:24<1:44:09,  5.19s/it] 40%|███▉      | 797/2000 [1:09:30<1:45:34,  5.27s/it] 40%|███▉      | 798/2000 [1:09:35<1:43:05,  5.15s/it] 40%|███▉      | 799/2000 [1:09:40<1:42:58,  5.14s/it] 40%|████      | 800/2000 [1:09:45<1:42:53,  5.14s/it]                                                      {'loss': 0.0761, 'grad_norm': 1.3483314514160156, 'learning_rate': 3.508022696021226e-05, 'epoch': 3.2}
 40%|████      | 800/2000 [1:09:45<1:42:53,  5.14s/it] 40%|████      | 801/2000 [1:09:50<1:45:09,  5.26s/it] 40%|████      | 802/2000 [1:09:56<1:46:23,  5.33s/it] 40%|████      | 803/2000 [1:10:01<1:45:42,  5.30s/it] 40%|████      | 804/2000 [1:10:06<1:46:08,  5.32s/it] 40%|████      | 805/2000 [1:10:11<1:43:10,  5.18s/it] 40%|████      | 806/2000 [1:10:17<1:43:09,  5.18s/it] 40%|████      | 807/2000 [1:10:22<1:45:15,  5.29s/it] 40%|████      | 808/2000 [1:10:27<1:44:28,  5.26s/it] 40%|████      | 809/2000 [1:10:32<1:42:56,  5.19s/it] 40%|████      | 810/2000 [1:10:37<1:42:28,  5.17s/it]                                                      {'loss': 0.1154, 'grad_norm': 1.9245853424072266, 'learning_rate': 3.470059046334011e-05, 'epoch': 3.24}
 40%|████      | 810/2000 [1:10:37<1:42:28,  5.17s/it] 41%|████      | 811/2000 [1:10:42<1:41:57,  5.14s/it] 41%|████      | 812/2000 [1:10:48<1:42:05,  5.16s/it] 41%|████      | 813/2000 [1:10:53<1:41:27,  5.13s/it] 41%|████      | 814/2000 [1:10:58<1:42:53,  5.21s/it] 41%|████      | 815/2000 [1:11:03<1:42:42,  5.20s/it] 41%|████      | 816/2000 [1:11:09<1:45:06,  5.33s/it] 41%|████      | 817/2000 [1:11:14<1:43:00,  5.22s/it] 41%|████      | 818/2000 [1:11:19<1:41:20,  5.14s/it] 41%|████      | 819/2000 [1:11:24<1:42:02,  5.18s/it] 41%|████      | 820/2000 [1:11:29<1:40:36,  5.12s/it]                                                      {'loss': 0.0842, 'grad_norm': 0.9619826674461365, 'learning_rate': 3.4318301921895084e-05, 'epoch': 3.28}
 41%|████      | 820/2000 [1:11:29<1:40:36,  5.12s/it] 41%|████      | 821/2000 [1:11:34<1:40:30,  5.11s/it] 41%|████      | 822/2000 [1:11:39<1:40:52,  5.14s/it] 41%|████      | 823/2000 [1:11:45<1:42:10,  5.21s/it] 41%|████      | 824/2000 [1:11:50<1:41:12,  5.16s/it] 41%|████▏     | 825/2000 [1:11:55<1:39:45,  5.09s/it] 41%|████▏     | 826/2000 [1:12:00<1:39:40,  5.09s/it] 41%|████▏     | 827/2000 [1:12:05<1:38:16,  5.03s/it] 41%|████▏     | 828/2000 [1:12:10<1:40:06,  5.12s/it] 41%|████▏     | 829/2000 [1:12:16<1:42:03,  5.23s/it] 42%|████▏     | 830/2000 [1:12:21<1:40:38,  5.16s/it]                                                      {'loss': 0.0934, 'grad_norm': 1.1189240217208862, 'learning_rate': 3.3933465849747275e-05, 'epoch': 3.32}
 42%|████▏     | 830/2000 [1:12:21<1:40:38,  5.16s/it] 42%|████▏     | 831/2000 [1:12:26<1:40:46,  5.17s/it] 42%|████▏     | 832/2000 [1:12:31<1:38:46,  5.07s/it] 42%|████▏     | 833/2000 [1:12:36<1:38:39,  5.07s/it] 42%|████▏     | 834/2000 [1:12:41<1:38:17,  5.06s/it] 42%|████▏     | 835/2000 [1:12:46<1:40:14,  5.16s/it] 42%|████▏     | 836/2000 [1:12:51<1:39:25,  5.12s/it] 42%|████▏     | 837/2000 [1:12:56<1:38:30,  5.08s/it] 42%|████▏     | 838/2000 [1:13:01<1:38:03,  5.06s/it] 42%|████▏     | 839/2000 [1:13:07<1:42:45,  5.31s/it] 42%|████▏     | 840/2000 [1:13:12<1:40:24,  5.19s/it]                                                      {'loss': 0.0814, 'grad_norm': 1.503852367401123, 'learning_rate': 3.3546187457236244e-05, 'epoch': 3.36}
 42%|████▏     | 840/2000 [1:13:13<1:40:24,  5.19s/it] 42%|████▏     | 841/2000 [1:13:18<1:43:08,  5.34s/it] 42%|████▏     | 842/2000 [1:13:23<1:41:46,  5.27s/it] 42%|████▏     | 843/2000 [1:13:28<1:42:38,  5.32s/it] 42%|████▏     | 844/2000 [1:13:34<1:43:00,  5.35s/it] 42%|████▏     | 845/2000 [1:13:39<1:41:33,  5.28s/it] 42%|████▏     | 846/2000 [1:13:44<1:41:27,  5.28s/it] 42%|████▏     | 847/2000 [1:13:49<1:40:04,  5.21s/it] 42%|████▏     | 848/2000 [1:13:54<1:39:12,  5.17s/it] 42%|████▏     | 849/2000 [1:13:59<1:37:19,  5.07s/it] 42%|████▎     | 850/2000 [1:14:04<1:36:11,  5.02s/it]                                                      {'loss': 0.0948, 'grad_norm': 1.1348211765289307, 'learning_rate': 3.3156572622407565e-05, 'epoch': 3.4}
 42%|████▎     | 850/2000 [1:14:05<1:36:11,  5.02s/it] 43%|████▎     | 851/2000 [1:14:10<1:41:46,  5.31s/it] 43%|████▎     | 852/2000 [1:14:15<1:40:00,  5.23s/it] 43%|████▎     | 853/2000 [1:14:19<1:36:01,  5.02s/it] 43%|████▎     | 854/2000 [1:14:24<1:35:40,  5.01s/it] 43%|████▎     | 855/2000 [1:14:30<1:37:26,  5.11s/it] 43%|████▎     | 856/2000 [1:14:35<1:38:21,  5.16s/it] 43%|████▎     | 857/2000 [1:14:40<1:39:40,  5.23s/it] 43%|████▎     | 858/2000 [1:14:46<1:39:56,  5.25s/it] 43%|████▎     | 859/2000 [1:14:51<1:38:59,  5.21s/it] 43%|████▎     | 860/2000 [1:14:56<1:37:45,  5.15s/it]                                                      {'loss': 0.1224, 'grad_norm': 1.4533876180648804, 'learning_rate': 3.276472786206679e-05, 'epoch': 3.44}
 43%|████▎     | 860/2000 [1:14:56<1:37:45,  5.15s/it] 43%|████▎     | 861/2000 [1:15:01<1:36:52,  5.10s/it] 43%|████▎     | 862/2000 [1:15:06<1:37:45,  5.15s/it] 43%|████▎     | 863/2000 [1:15:12<1:39:18,  5.24s/it] 43%|████▎     | 864/2000 [1:15:17<1:38:51,  5.22s/it] 43%|████▎     | 865/2000 [1:15:22<1:38:38,  5.21s/it] 43%|████▎     | 866/2000 [1:15:27<1:40:20,  5.31s/it] 43%|████▎     | 867/2000 [1:15:33<1:40:08,  5.30s/it] 43%|████▎     | 868/2000 [1:15:38<1:42:16,  5.42s/it] 43%|████▎     | 869/2000 [1:15:44<1:40:32,  5.33s/it] 44%|████▎     | 870/2000 [1:15:49<1:40:26,  5.33s/it]                                                      {'loss': 0.0834, 'grad_norm': 1.134530782699585, 'learning_rate': 3.237076030265884e-05, 'epoch': 3.48}
 44%|████▎     | 870/2000 [1:15:49<1:40:26,  5.33s/it] 44%|████▎     | 871/2000 [1:15:55<1:42:49,  5.46s/it] 44%|████▎     | 872/2000 [1:16:00<1:43:41,  5.52s/it] 44%|████▎     | 873/2000 [1:16:05<1:41:41,  5.41s/it] 44%|████▎     | 874/2000 [1:16:11<1:39:51,  5.32s/it] 44%|████▍     | 875/2000 [1:16:16<1:41:08,  5.39s/it] 44%|████▍     | 876/2000 [1:16:22<1:41:37,  5.42s/it] 44%|████▍     | 877/2000 [1:16:28<1:44:08,  5.56s/it] 44%|████▍     | 878/2000 [1:16:33<1:44:25,  5.58s/it] 44%|████▍     | 879/2000 [1:16:38<1:40:54,  5.40s/it] 44%|████▍     | 880/2000 [1:16:44<1:41:38,  5.44s/it]                                                      {'loss': 0.0851, 'grad_norm': 1.2395575046539307, 'learning_rate': 3.1974777650980735e-05, 'epoch': 3.52}
 44%|████▍     | 880/2000 [1:16:44<1:41:38,  5.44s/it] 44%|████▍     | 881/2000 [1:16:49<1:39:55,  5.36s/it] 44%|████▍     | 882/2000 [1:16:54<1:38:06,  5.27s/it] 44%|████▍     | 883/2000 [1:16:59<1:39:20,  5.34s/it] 44%|████▍     | 884/2000 [1:17:05<1:40:11,  5.39s/it] 44%|████▍     | 885/2000 [1:17:10<1:38:54,  5.32s/it] 44%|████▍     | 886/2000 [1:17:15<1:37:26,  5.25s/it] 44%|████▍     | 887/2000 [1:17:20<1:37:59,  5.28s/it] 44%|████▍     | 888/2000 [1:17:26<1:37:37,  5.27s/it] 44%|████▍     | 889/2000 [1:17:31<1:37:08,  5.25s/it] 44%|████▍     | 890/2000 [1:17:36<1:35:56,  5.19s/it]                                                      {'loss': 0.098, 'grad_norm': 1.1561075448989868, 'learning_rate': 3.1576888164735575e-05, 'epoch': 3.56}
 44%|████▍     | 890/2000 [1:17:36<1:35:56,  5.19s/it] 45%|████▍     | 891/2000 [1:17:41<1:35:01,  5.14s/it] 45%|████▍     | 892/2000 [1:17:46<1:34:29,  5.12s/it] 45%|████▍     | 893/2000 [1:17:52<1:39:56,  5.42s/it] 45%|████▍     | 894/2000 [1:17:57<1:39:16,  5.39s/it] 45%|████▍     | 895/2000 [1:18:02<1:37:00,  5.27s/it] 45%|████▍     | 896/2000 [1:18:08<1:38:22,  5.35s/it] 45%|████▍     | 897/2000 [1:18:13<1:38:39,  5.37s/it] 45%|████▍     | 898/2000 [1:18:19<1:37:59,  5.34s/it] 45%|████▍     | 899/2000 [1:18:23<1:34:47,  5.17s/it] 45%|████▌     | 900/2000 [1:18:28<1:32:25,  5.04s/it]                                                      {'loss': 0.0841, 'grad_norm': 1.1285754442214966, 'learning_rate': 3.117720062293599e-05, 'epoch': 3.6}
 45%|████▌     | 900/2000 [1:18:28<1:32:25,  5.04s/it] 45%|████▌     | 901/2000 [1:18:35<1:40:53,  5.51s/it] 45%|████▌     | 902/2000 [1:18:40<1:39:38,  5.45s/it] 45%|████▌     | 903/2000 [1:18:45<1:38:50,  5.41s/it] 45%|████▌     | 904/2000 [1:18:51<1:40:07,  5.48s/it] 45%|████▌     | 905/2000 [1:18:56<1:36:52,  5.31s/it] 45%|████▌     | 906/2000 [1:19:02<1:38:41,  5.41s/it] 45%|████▌     | 907/2000 [1:19:07<1:37:21,  5.34s/it] 45%|████▌     | 908/2000 [1:19:12<1:37:10,  5.34s/it] 45%|████▌     | 909/2000 [1:19:17<1:36:31,  5.31s/it] 46%|████▌     | 910/2000 [1:19:23<1:37:39,  5.38s/it]                                                      {'loss': 0.0911, 'grad_norm': 1.5371792316436768, 'learning_rate': 3.077582429616506e-05, 'epoch': 3.64}
 46%|████▌     | 910/2000 [1:19:23<1:37:39,  5.38s/it] 46%|████▌     | 911/2000 [1:19:28<1:36:32,  5.32s/it] 46%|████▌     | 912/2000 [1:19:33<1:35:56,  5.29s/it] 46%|████▌     | 913/2000 [1:19:39<1:35:16,  5.26s/it] 46%|████▌     | 914/2000 [1:19:44<1:35:01,  5.25s/it] 46%|████▌     | 915/2000 [1:19:49<1:34:41,  5.24s/it] 46%|████▌     | 916/2000 [1:19:54<1:33:40,  5.18s/it] 46%|████▌     | 917/2000 [1:19:59<1:32:32,  5.13s/it] 46%|████▌     | 918/2000 [1:20:04<1:32:36,  5.14s/it] 46%|████▌     | 919/2000 [1:20:10<1:33:54,  5.21s/it] 46%|████▌     | 920/2000 [1:20:15<1:34:13,  5.23s/it]                                                      {'loss': 0.1052, 'grad_norm': 1.8212970495224, 'learning_rate': 3.037286891670281e-05, 'epoch': 3.68}
 46%|████▌     | 920/2000 [1:20:15<1:34:13,  5.23s/it] 46%|████▌     | 921/2000 [1:20:20<1:34:31,  5.26s/it] 46%|████▌     | 922/2000 [1:20:25<1:33:19,  5.19s/it] 46%|████▌     | 923/2000 [1:20:30<1:33:13,  5.19s/it] 46%|████▌     | 924/2000 [1:20:36<1:33:59,  5.24s/it] 46%|████▋     | 925/2000 [1:20:41<1:34:32,  5.28s/it] 46%|████▋     | 926/2000 [1:20:46<1:33:48,  5.24s/it] 46%|████▋     | 927/2000 [1:20:52<1:33:52,  5.25s/it] 46%|████▋     | 928/2000 [1:20:57<1:33:31,  5.23s/it] 46%|████▋     | 929/2000 [1:21:02<1:32:59,  5.21s/it] 46%|████▋     | 930/2000 [1:21:07<1:33:41,  5.25s/it]                                                      {'loss': 0.0809, 'grad_norm': 1.330436110496521, 'learning_rate': 2.9968444648526493e-05, 'epoch': 3.72}
 46%|████▋     | 930/2000 [1:21:07<1:33:41,  5.25s/it] 47%|████▋     | 931/2000 [1:21:13<1:35:35,  5.37s/it] 47%|████▋     | 932/2000 [1:21:18<1:35:25,  5.36s/it] 47%|████▋     | 933/2000 [1:21:23<1:34:39,  5.32s/it] 47%|████▋     | 934/2000 [1:21:29<1:37:08,  5.47s/it] 47%|████▋     | 935/2000 [1:21:35<1:37:02,  5.47s/it] 47%|████▋     | 936/2000 [1:21:40<1:34:29,  5.33s/it] 47%|████▋     | 937/2000 [1:21:45<1:32:37,  5.23s/it] 47%|████▋     | 938/2000 [1:21:50<1:32:15,  5.21s/it] 47%|████▋     | 939/2000 [1:21:55<1:31:45,  5.19s/it] 47%|████▋     | 940/2000 [1:22:01<1:35:15,  5.39s/it]                                                      {'loss': 0.0942, 'grad_norm': 1.6518101692199707, 'learning_rate': 2.956266205719288e-05, 'epoch': 3.76}
 47%|████▋     | 940/2000 [1:22:01<1:35:15,  5.39s/it] 47%|████▋     | 941/2000 [1:22:06<1:35:08,  5.39s/it] 47%|████▋     | 942/2000 [1:22:11<1:33:30,  5.30s/it] 47%|████▋     | 943/2000 [1:22:17<1:32:32,  5.25s/it] 47%|████▋     | 944/2000 [1:22:22<1:31:23,  5.19s/it] 47%|████▋     | 945/2000 [1:22:27<1:31:17,  5.19s/it] 47%|████▋     | 946/2000 [1:22:32<1:30:36,  5.16s/it] 47%|████▋     | 947/2000 [1:22:37<1:30:18,  5.15s/it] 47%|████▋     | 948/2000 [1:22:42<1:30:19,  5.15s/it] 47%|████▋     | 949/2000 [1:22:47<1:30:28,  5.17s/it] 48%|████▊     | 950/2000 [1:22:52<1:29:04,  5.09s/it]                                                      {'loss': 0.0898, 'grad_norm': 1.4574402570724487, 'learning_rate': 2.915563207961074e-05, 'epoch': 3.8}
 48%|████▊     | 950/2000 [1:22:52<1:29:04,  5.09s/it] 48%|████▊     | 951/2000 [1:22:58<1:31:44,  5.25s/it] 48%|████▊     | 952/2000 [1:23:03<1:32:52,  5.32s/it] 48%|████▊     | 953/2000 [1:23:08<1:31:18,  5.23s/it] 48%|████▊     | 954/2000 [1:23:13<1:30:06,  5.17s/it] 48%|████▊     | 955/2000 [1:23:19<1:30:33,  5.20s/it] 48%|████▊     | 956/2000 [1:23:24<1:32:13,  5.30s/it] 48%|████▊     | 957/2000 [1:23:29<1:32:08,  5.30s/it] 48%|████▊     | 958/2000 [1:23:35<1:32:04,  5.30s/it] 48%|████▊     | 959/2000 [1:23:40<1:31:40,  5.28s/it] 48%|████▊     | 960/2000 [1:23:46<1:33:25,  5.39s/it]                                                      {'loss': 0.0962, 'grad_norm': 1.210965871810913, 'learning_rate': 2.874746599371175e-05, 'epoch': 3.84}
 48%|████▊     | 960/2000 [1:23:46<1:33:25,  5.39s/it] 48%|████▊     | 961/2000 [1:23:51<1:33:11,  5.38s/it] 48%|████▊     | 962/2000 [1:23:56<1:31:48,  5.31s/it] 48%|████▊     | 963/2000 [1:24:01<1:29:35,  5.18s/it] 48%|████▊     | 964/2000 [1:24:06<1:28:28,  5.12s/it] 48%|████▊     | 965/2000 [1:24:12<1:30:49,  5.27s/it] 48%|████▊     | 966/2000 [1:24:17<1:31:08,  5.29s/it] 48%|████▊     | 967/2000 [1:24:22<1:29:12,  5.18s/it] 48%|████▊     | 968/2000 [1:24:27<1:28:59,  5.17s/it] 48%|████▊     | 969/2000 [1:24:32<1:29:02,  5.18s/it] 48%|████▊     | 970/2000 [1:24:37<1:28:28,  5.15s/it]                                                      {'loss': 0.0917, 'grad_norm': 1.0545246601104736, 'learning_rate': 2.8338275388028295e-05, 'epoch': 3.88}
 48%|████▊     | 970/2000 [1:24:37<1:28:28,  5.15s/it] 49%|████▊     | 971/2000 [1:24:43<1:29:09,  5.20s/it] 49%|████▊     | 972/2000 [1:24:48<1:28:27,  5.16s/it] 49%|████▊     | 973/2000 [1:24:53<1:27:24,  5.11s/it] 49%|████▊     | 974/2000 [1:24:58<1:27:55,  5.14s/it] 49%|████▉     | 975/2000 [1:25:03<1:26:24,  5.06s/it] 49%|████▉     | 976/2000 [1:25:08<1:27:26,  5.12s/it] 49%|████▉     | 977/2000 [1:25:13<1:26:38,  5.08s/it] 49%|████▉     | 978/2000 [1:25:18<1:27:24,  5.13s/it] 49%|████▉     | 979/2000 [1:25:24<1:28:56,  5.23s/it] 49%|████▉     | 980/2000 [1:25:29<1:27:41,  5.16s/it]                                                      {'loss': 0.0781, 'grad_norm': 1.2779860496520996, 'learning_rate': 2.792817213118623e-05, 'epoch': 3.92}
 49%|████▉     | 980/2000 [1:25:29<1:27:41,  5.16s/it] 49%|████▉     | 981/2000 [1:25:34<1:28:00,  5.18s/it] 49%|████▉     | 982/2000 [1:25:39<1:27:43,  5.17s/it] 49%|████▉     | 983/2000 [1:25:44<1:26:18,  5.09s/it] 49%|████▉     | 984/2000 [1:25:49<1:25:47,  5.07s/it] 49%|████▉     | 985/2000 [1:25:54<1:25:10,  5.03s/it] 49%|████▉     | 986/2000 [1:25:59<1:24:49,  5.02s/it] 49%|████▉     | 987/2000 [1:26:04<1:24:16,  4.99s/it] 49%|████▉     | 988/2000 [1:26:09<1:24:52,  5.03s/it] 49%|████▉     | 989/2000 [1:26:14<1:25:33,  5.08s/it] 50%|████▉     | 990/2000 [1:26:19<1:25:13,  5.06s/it]                                                      {'loss': 0.0869, 'grad_norm': 1.143169641494751, 'learning_rate': 2.7517268341321112e-05, 'epoch': 3.96}
 50%|████▉     | 990/2000 [1:26:19<1:25:13,  5.06s/it] 50%|████▉     | 991/2000 [1:26:24<1:23:57,  4.99s/it] 50%|████▉     | 992/2000 [1:26:29<1:24:51,  5.05s/it] 50%|████▉     | 993/2000 [1:26:34<1:25:08,  5.07s/it] 50%|████▉     | 994/2000 [1:26:40<1:29:14,  5.32s/it] 50%|████▉     | 995/2000 [1:26:45<1:28:15,  5.27s/it] 50%|████▉     | 996/2000 [1:26:51<1:27:58,  5.26s/it] 50%|████▉     | 997/2000 [1:26:56<1:28:42,  5.31s/it] 50%|████▉     | 998/2000 [1:27:02<1:29:46,  5.38s/it] 50%|████▉     | 999/2000 [1:27:07<1:29:55,  5.39s/it] 50%|█████     | 1000/2000 [1:27:12<1:28:18,  5.30s/it]                                                       {'loss': 0.1057, 'grad_norm': 1.779848337173462, 'learning_rate': 2.7105676355426248e-05, 'epoch': 4.0}
 50%|█████     | 1000/2000 [1:27:12<1:28:18,  5.30s/it][INFO|trainer.py:3993] 2025-06-12 22:51:35,505 >> Saving model checkpoint to ./models/Qwen3-8B-llama-epoch8/checkpoint-1000
[INFO|configuration_utils.py:696] 2025-06-12 22:51:35,577 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-12 22:51:35,579 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2356] 2025-06-12 22:51:37,435 >> chat template saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1000/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-12 22:51:37,673 >> tokenizer config file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-12 22:51:38,548 >> Special tokens file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1000/special_tokens_map.json
 50%|█████     | 1001/2000 [1:27:24<2:00:16,  7.22s/it] 50%|█████     | 1002/2000 [1:27:29<1:50:30,  6.64s/it] 50%|█████     | 1003/2000 [1:27:34<1:42:17,  6.16s/it] 50%|█████     | 1004/2000 [1:27:40<1:38:49,  5.95s/it] 50%|█████     | 1005/2000 [1:27:45<1:34:32,  5.70s/it] 50%|█████     | 1006/2000 [1:27:50<1:34:31,  5.71s/it] 50%|█████     | 1007/2000 [1:27:56<1:32:41,  5.60s/it] 50%|█████     | 1008/2000 [1:28:02<1:34:27,  5.71s/it] 50%|█████     | 1009/2000 [1:28:07<1:32:32,  5.60s/it] 50%|█████     | 1010/2000 [1:28:12<1:30:49,  5.50s/it]                                                       {'loss': 0.0425, 'grad_norm': 0.6043973565101624, 'learning_rate': 2.6693508698640852e-05, 'epoch': 4.04}
 50%|█████     | 1010/2000 [1:28:12<1:30:49,  5.50s/it] 51%|█████     | 1011/2000 [1:28:18<1:28:46,  5.39s/it] 51%|█████     | 1012/2000 [1:28:23<1:28:12,  5.36s/it] 51%|█████     | 1013/2000 [1:28:28<1:26:19,  5.25s/it] 51%|█████     | 1014/2000 [1:28:33<1:26:37,  5.27s/it] 51%|█████     | 1015/2000 [1:28:39<1:27:29,  5.33s/it] 51%|█████     | 1016/2000 [1:28:44<1:26:55,  5.30s/it] 51%|█████     | 1017/2000 [1:28:49<1:26:16,  5.27s/it] 51%|█████     | 1018/2000 [1:28:54<1:26:16,  5.27s/it] 51%|█████     | 1019/2000 [1:29:00<1:26:12,  5.27s/it] 51%|█████     | 1020/2000 [1:29:05<1:26:05,  5.27s/it]                                                       {'loss': 0.0398, 'grad_norm': 1.0699951648712158, 'learning_rate': 2.6280878053486828e-05, 'epoch': 4.08}
 51%|█████     | 1020/2000 [1:29:05<1:26:05,  5.27s/it] 51%|█████     | 1021/2000 [1:29:10<1:26:42,  5.31s/it] 51%|█████     | 1022/2000 [1:29:15<1:25:06,  5.22s/it] 51%|█████     | 1023/2000 [1:29:21<1:25:02,  5.22s/it] 51%|█████     | 1024/2000 [1:29:26<1:26:46,  5.33s/it] 51%|█████▏    | 1025/2000 [1:29:31<1:25:34,  5.27s/it] 51%|█████▏    | 1026/2000 [1:29:36<1:24:34,  5.21s/it] 51%|█████▏    | 1027/2000 [1:29:42<1:26:27,  5.33s/it] 51%|█████▏    | 1028/2000 [1:29:47<1:25:49,  5.30s/it] 51%|█████▏    | 1029/2000 [1:29:52<1:24:52,  5.25s/it] 52%|█████▏    | 1030/2000 [1:29:58<1:27:08,  5.39s/it]                                                       {'loss': 0.0319, 'grad_norm': 1.5690643787384033, 'learning_rate': 2.5867897229062478e-05, 'epoch': 4.12}
 52%|█████▏    | 1030/2000 [1:29:58<1:27:08,  5.39s/it] 52%|█████▏    | 1031/2000 [1:30:03<1:27:11,  5.40s/it] 52%|█████▏    | 1032/2000 [1:30:09<1:27:33,  5.43s/it] 52%|█████▏    | 1033/2000 [1:30:14<1:25:20,  5.30s/it] 52%|█████▏    | 1034/2000 [1:30:19<1:24:13,  5.23s/it] 52%|█████▏    | 1035/2000 [1:30:24<1:23:22,  5.18s/it] 52%|█████▏    | 1036/2000 [1:30:29<1:22:48,  5.15s/it] 52%|█████▏    | 1037/2000 [1:30:34<1:23:02,  5.17s/it] 52%|█████▏    | 1038/2000 [1:30:39<1:22:18,  5.13s/it] 52%|█████▏    | 1039/2000 [1:30:45<1:23:33,  5.22s/it] 52%|█████▏    | 1040/2000 [1:30:50<1:22:03,  5.13s/it]                                                       {'loss': 0.0412, 'grad_norm': 2.701613664627075, 'learning_rate': 2.5454679130201593e-05, 'epoch': 4.16}
 52%|█████▏    | 1040/2000 [1:30:50<1:22:03,  5.13s/it] 52%|█████▏    | 1041/2000 [1:30:55<1:21:32,  5.10s/it] 52%|█████▏    | 1042/2000 [1:31:00<1:20:53,  5.07s/it] 52%|█████▏    | 1043/2000 [1:31:05<1:20:24,  5.04s/it] 52%|█████▏    | 1044/2000 [1:31:10<1:21:58,  5.14s/it] 52%|█████▏    | 1045/2000 [1:31:15<1:21:26,  5.12s/it] 52%|█████▏    | 1046/2000 [1:31:21<1:23:44,  5.27s/it] 52%|█████▏    | 1047/2000 [1:31:26<1:23:58,  5.29s/it] 52%|█████▏    | 1048/2000 [1:31:32<1:25:28,  5.39s/it] 52%|█████▏    | 1049/2000 [1:31:37<1:24:31,  5.33s/it] 52%|█████▎    | 1050/2000 [1:31:42<1:23:09,  5.25s/it]                                                       {'loss': 0.0377, 'grad_norm': 0.9073522090911865, 'learning_rate': 2.5041336726606457e-05, 'epoch': 4.2}
 52%|█████▎    | 1050/2000 [1:31:43<1:23:09,  5.25s/it] 53%|█████▎    | 1051/2000 [1:31:48<1:28:22,  5.59s/it] 53%|█████▎    | 1052/2000 [1:31:53<1:24:12,  5.33s/it] 53%|█████▎    | 1053/2000 [1:31:58<1:23:56,  5.32s/it] 53%|█████▎    | 1054/2000 [1:32:04<1:25:52,  5.45s/it] 53%|█████▎    | 1055/2000 [1:32:10<1:25:45,  5.44s/it] 53%|█████▎    | 1056/2000 [1:32:15<1:26:08,  5.48s/it] 53%|█████▎    | 1057/2000 [1:32:21<1:26:48,  5.52s/it] 53%|█████▎    | 1058/2000 [1:32:26<1:25:37,  5.45s/it] 53%|█████▎    | 1059/2000 [1:32:31<1:24:59,  5.42s/it] 53%|█████▎    | 1060/2000 [1:32:36<1:23:08,  5.31s/it]                                                       {'loss': 0.0483, 'grad_norm': 1.708414912223816, 'learning_rate': 2.4627983021963014e-05, 'epoch': 4.24}
 53%|█████▎    | 1060/2000 [1:32:38<1:23:08,  5.31s/it] 53%|█████▎    | 1061/2000 [1:32:43<1:30:43,  5.80s/it] 53%|█████▎    | 1062/2000 [1:32:49<1:28:11,  5.64s/it] 53%|█████▎    | 1063/2000 [1:32:54<1:24:26,  5.41s/it] 53%|█████▎    | 1064/2000 [1:32:58<1:21:40,  5.24s/it] 53%|█████▎    | 1065/2000 [1:33:03<1:19:58,  5.13s/it] 53%|█████▎    | 1066/2000 [1:33:08<1:20:03,  5.14s/it] 53%|█████▎    | 1067/2000 [1:33:14<1:20:49,  5.20s/it] 53%|█████▎    | 1068/2000 [1:33:19<1:20:36,  5.19s/it] 53%|█████▎    | 1069/2000 [1:33:24<1:22:15,  5.30s/it] 54%|█████▎    | 1070/2000 [1:33:30<1:21:34,  5.26s/it]                                                       {'loss': 0.0504, 'grad_norm': 1.0982863903045654, 'learning_rate': 2.4214731023046793e-05, 'epoch': 4.28}
 54%|█████▎    | 1070/2000 [1:33:30<1:21:34,  5.26s/it] 54%|█████▎    | 1071/2000 [1:33:35<1:21:35,  5.27s/it] 54%|█████▎    | 1072/2000 [1:33:40<1:20:50,  5.23s/it] 54%|█████▎    | 1073/2000 [1:33:45<1:19:53,  5.17s/it] 54%|█████▎    | 1074/2000 [1:33:50<1:19:51,  5.17s/it] 54%|█████▍    | 1075/2000 [1:33:55<1:18:59,  5.12s/it] 54%|█████▍    | 1076/2000 [1:34:01<1:20:06,  5.20s/it] 54%|█████▍    | 1077/2000 [1:34:06<1:20:30,  5.23s/it] 54%|█████▍    | 1078/2000 [1:34:11<1:19:33,  5.18s/it] 54%|█████▍    | 1079/2000 [1:34:16<1:20:33,  5.25s/it] 54%|█████▍    | 1080/2000 [1:34:22<1:21:18,  5.30s/it]                                                       {'loss': 0.0491, 'grad_norm': 1.9663890600204468, 'learning_rate': 2.3801693708828013e-05, 'epoch': 4.32}
 54%|█████▍    | 1080/2000 [1:34:22<1:21:18,  5.30s/it] 54%|█████▍    | 1081/2000 [1:34:27<1:19:45,  5.21s/it] 54%|█████▍    | 1082/2000 [1:34:32<1:19:49,  5.22s/it] 54%|█████▍    | 1083/2000 [1:34:37<1:20:03,  5.24s/it] 54%|█████▍    | 1084/2000 [1:34:43<1:20:15,  5.26s/it] 54%|█████▍    | 1085/2000 [1:34:48<1:19:13,  5.20s/it] 54%|█████▍    | 1086/2000 [1:34:53<1:17:40,  5.10s/it] 54%|█████▍    | 1087/2000 [1:34:58<1:20:00,  5.26s/it] 54%|█████▍    | 1088/2000 [1:35:03<1:18:47,  5.18s/it] 54%|█████▍    | 1089/2000 [1:35:09<1:21:42,  5.38s/it] 55%|█████▍    | 1090/2000 [1:35:14<1:19:33,  5.25s/it]                                                       {'loss': 0.0505, 'grad_norm': 0.797344982624054, 'learning_rate': 2.3388983999584224e-05, 'epoch': 4.36}
 55%|█████▍    | 1090/2000 [1:35:14<1:19:33,  5.25s/it] 55%|█████▍    | 1091/2000 [1:35:19<1:18:22,  5.17s/it] 55%|█████▍    | 1092/2000 [1:35:24<1:18:16,  5.17s/it] 55%|█████▍    | 1093/2000 [1:35:29<1:16:59,  5.09s/it] 55%|█████▍    | 1094/2000 [1:35:34<1:16:41,  5.08s/it] 55%|█████▍    | 1095/2000 [1:35:40<1:17:56,  5.17s/it] 55%|█████▍    | 1096/2000 [1:35:45<1:17:05,  5.12s/it] 55%|█████▍    | 1097/2000 [1:35:49<1:16:10,  5.06s/it] 55%|█████▍    | 1098/2000 [1:35:55<1:18:03,  5.19s/it] 55%|█████▍    | 1099/2000 [1:36:00<1:17:41,  5.17s/it] 55%|█████▌    | 1100/2000 [1:36:05<1:16:59,  5.13s/it]                                                       {'loss': 0.0424, 'grad_norm': 1.7030792236328125, 'learning_rate': 2.2976714726029068e-05, 'epoch': 4.4}
 55%|█████▌    | 1100/2000 [1:36:05<1:16:59,  5.13s/it] 55%|█████▌    | 1101/2000 [1:36:10<1:16:16,  5.09s/it] 55%|█████▌    | 1102/2000 [1:36:15<1:16:27,  5.11s/it] 55%|█████▌    | 1103/2000 [1:36:20<1:16:26,  5.11s/it] 55%|█████▌    | 1104/2000 [1:36:26<1:17:42,  5.20s/it] 55%|█████▌    | 1105/2000 [1:36:31<1:17:29,  5.20s/it] 55%|█████▌    | 1106/2000 [1:36:36<1:14:45,  5.02s/it] 55%|█████▌    | 1107/2000 [1:36:40<1:14:02,  4.97s/it] 55%|█████▌    | 1108/2000 [1:36:46<1:17:27,  5.21s/it] 55%|█████▌    | 1109/2000 [1:36:51<1:16:06,  5.12s/it] 56%|█████▌    | 1110/2000 [1:36:57<1:18:07,  5.27s/it]                                                       {'loss': 0.0334, 'grad_norm': 1.3334076404571533, 'learning_rate': 2.2564998598465423e-05, 'epoch': 4.44}
 56%|█████▌    | 1110/2000 [1:36:57<1:18:07,  5.27s/it] 56%|█████▌    | 1111/2000 [1:37:02<1:18:43,  5.31s/it] 56%|█████▌    | 1112/2000 [1:37:08<1:19:12,  5.35s/it] 56%|█████▌    | 1113/2000 [1:37:13<1:18:39,  5.32s/it] 56%|█████▌    | 1114/2000 [1:37:18<1:19:38,  5.39s/it] 56%|█████▌    | 1115/2000 [1:37:23<1:16:59,  5.22s/it] 56%|█████▌    | 1116/2000 [1:37:28<1:16:01,  5.16s/it] 56%|█████▌    | 1117/2000 [1:37:33<1:15:01,  5.10s/it] 56%|█████▌    | 1118/2000 [1:37:38<1:15:06,  5.11s/it] 56%|█████▌    | 1119/2000 [1:37:44<1:15:48,  5.16s/it] 56%|█████▌    | 1120/2000 [1:37:48<1:14:22,  5.07s/it]                                                       {'loss': 0.0341, 'grad_norm': 0.9138010740280151, 'learning_rate': 2.215394817597162e-05, 'epoch': 4.48}
 56%|█████▌    | 1120/2000 [1:37:50<1:14:22,  5.07s/it] 56%|█████▌    | 1121/2000 [1:37:56<1:25:37,  5.84s/it] 56%|█████▌    | 1122/2000 [1:38:02<1:24:16,  5.76s/it] 56%|█████▌    | 1123/2000 [1:38:07<1:21:45,  5.59s/it] 56%|█████▌    | 1124/2000 [1:38:12<1:19:38,  5.45s/it] 56%|█████▋    | 1125/2000 [1:38:17<1:19:16,  5.44s/it] 56%|█████▋    | 1126/2000 [1:38:22<1:16:55,  5.28s/it] 56%|█████▋    | 1127/2000 [1:38:28<1:16:26,  5.25s/it] 56%|█████▋    | 1128/2000 [1:38:32<1:15:00,  5.16s/it] 56%|█████▋    | 1129/2000 [1:38:38<1:17:25,  5.33s/it] 56%|█████▋    | 1130/2000 [1:38:44<1:17:20,  5.33s/it]                                                       {'loss': 0.0385, 'grad_norm': 1.5002373456954956, 'learning_rate': 2.1743675835628856e-05, 'epoch': 4.52}
 56%|█████▋    | 1130/2000 [1:38:44<1:17:20,  5.33s/it] 57%|█████▋    | 1131/2000 [1:38:50<1:20:15,  5.54s/it] 57%|█████▋    | 1132/2000 [1:38:55<1:17:50,  5.38s/it] 57%|█████▋    | 1133/2000 [1:39:00<1:17:52,  5.39s/it] 57%|█████▋    | 1134/2000 [1:39:05<1:16:02,  5.27s/it] 57%|█████▋    | 1135/2000 [1:39:10<1:15:26,  5.23s/it] 57%|█████▋    | 1136/2000 [1:39:15<1:13:58,  5.14s/it] 57%|█████▋    | 1137/2000 [1:39:20<1:13:23,  5.10s/it] 57%|█████▋    | 1138/2000 [1:39:25<1:12:42,  5.06s/it] 57%|█████▋    | 1139/2000 [1:39:30<1:11:38,  4.99s/it] 57%|█████▋    | 1140/2000 [1:39:35<1:11:48,  5.01s/it]                                                       {'loss': 0.0385, 'grad_norm': 1.112619400024414, 'learning_rate': 2.1334293741798432e-05, 'epoch': 4.56}
 57%|█████▋    | 1140/2000 [1:39:35<1:11:48,  5.01s/it] 57%|█████▋    | 1141/2000 [1:39:40<1:13:34,  5.14s/it] 57%|█████▋    | 1142/2000 [1:39:46<1:14:17,  5.20s/it] 57%|█████▋    | 1143/2000 [1:39:51<1:13:41,  5.16s/it] 57%|█████▋    | 1144/2000 [1:39:57<1:16:26,  5.36s/it] 57%|█████▋    | 1145/2000 [1:40:02<1:14:45,  5.25s/it] 57%|█████▋    | 1146/2000 [1:40:07<1:14:05,  5.21s/it] 57%|█████▋    | 1147/2000 [1:40:12<1:15:23,  5.30s/it] 57%|█████▋    | 1148/2000 [1:40:18<1:15:52,  5.34s/it] 57%|█████▋    | 1149/2000 [1:40:23<1:17:13,  5.45s/it] 57%|█████▊    | 1150/2000 [1:40:28<1:14:47,  5.28s/it]                                                       {'loss': 0.0521, 'grad_norm': 1.8817555904388428, 'learning_rate': 2.0925913815457153e-05, 'epoch': 4.6}
 57%|█████▊    | 1150/2000 [1:40:28<1:14:47,  5.28s/it] 58%|█████▊    | 1151/2000 [1:40:33<1:13:45,  5.21s/it] 58%|█████▊    | 1152/2000 [1:40:38<1:13:37,  5.21s/it] 58%|█████▊    | 1153/2000 [1:40:44<1:14:00,  5.24s/it] 58%|█████▊    | 1154/2000 [1:40:49<1:14:25,  5.28s/it] 58%|█████▊    | 1155/2000 [1:40:54<1:14:18,  5.28s/it] 58%|█████▊    | 1156/2000 [1:41:00<1:13:52,  5.25s/it] 58%|█████▊    | 1157/2000 [1:41:05<1:13:18,  5.22s/it] 58%|█████▊    | 1158/2000 [1:41:10<1:12:16,  5.15s/it] 58%|█████▊    | 1159/2000 [1:41:15<1:11:40,  5.11s/it] 58%|█████▊    | 1160/2000 [1:41:20<1:13:21,  5.24s/it]                                                       {'loss': 0.0405, 'grad_norm': 0.9787304401397705, 'learning_rate': 2.0518647703599236e-05, 'epoch': 4.64}
 58%|█████▊    | 1160/2000 [1:41:20<1:13:21,  5.24s/it] 58%|█████▊    | 1161/2000 [1:41:26<1:13:12,  5.23s/it] 58%|█████▊    | 1162/2000 [1:41:31<1:12:06,  5.16s/it] 58%|█████▊    | 1163/2000 [1:41:36<1:13:20,  5.26s/it] 58%|█████▊    | 1164/2000 [1:41:41<1:12:46,  5.22s/it] 58%|█████▊    | 1165/2000 [1:41:46<1:11:13,  5.12s/it] 58%|█████▊    | 1166/2000 [1:41:51<1:10:43,  5.09s/it] 58%|█████▊    | 1167/2000 [1:41:56<1:10:08,  5.05s/it] 58%|█████▊    | 1168/2000 [1:42:01<1:10:34,  5.09s/it] 58%|█████▊    | 1169/2000 [1:42:06<1:09:12,  5.00s/it] 58%|█████▊    | 1170/2000 [1:42:11<1:09:02,  4.99s/it]                                                       {'loss': 0.0395, 'grad_norm': 1.2245625257492065, 'learning_rate': 2.0112606748713138e-05, 'epoch': 4.68}
 58%|█████▊    | 1170/2000 [1:42:11<1:09:02,  4.99s/it] 59%|█████▊    | 1171/2000 [1:42:16<1:09:20,  5.02s/it] 59%|█████▊    | 1172/2000 [1:42:21<1:08:58,  5.00s/it] 59%|█████▊    | 1173/2000 [1:42:26<1:10:14,  5.10s/it] 59%|█████▊    | 1174/2000 [1:42:32<1:12:12,  5.24s/it] 59%|█████▉    | 1175/2000 [1:42:37<1:11:41,  5.21s/it] 59%|█████▉    | 1176/2000 [1:42:42<1:12:30,  5.28s/it] 59%|█████▉    | 1177/2000 [1:42:48<1:11:34,  5.22s/it] 59%|█████▉    | 1178/2000 [1:42:53<1:10:58,  5.18s/it] 59%|█████▉    | 1179/2000 [1:42:58<1:10:10,  5.13s/it] 59%|█████▉    | 1180/2000 [1:43:02<1:08:42,  5.03s/it]                                                       {'loss': 0.0631, 'grad_norm': 1.316472053527832, 'learning_rate': 1.9707901958341612e-05, 'epoch': 4.72}
 59%|█████▉    | 1180/2000 [1:43:03<1:08:42,  5.03s/it] 59%|█████▉    | 1181/2000 [1:43:08<1:11:16,  5.22s/it] 59%|█████▉    | 1182/2000 [1:43:13<1:09:47,  5.12s/it] 59%|█████▉    | 1183/2000 [1:43:18<1:07:47,  4.98s/it] 59%|█████▉    | 1184/2000 [1:43:23<1:07:19,  4.95s/it] 59%|█████▉    | 1185/2000 [1:43:28<1:07:31,  4.97s/it] 59%|█████▉    | 1186/2000 [1:43:33<1:09:02,  5.09s/it] 59%|█████▉    | 1187/2000 [1:43:38<1:09:18,  5.11s/it] 59%|█████▉    | 1188/2000 [1:43:43<1:09:34,  5.14s/it] 59%|█████▉    | 1189/2000 [1:43:48<1:08:13,  5.05s/it] 60%|█████▉    | 1190/2000 [1:43:53<1:08:25,  5.07s/it]                                                       {'loss': 0.0315, 'grad_norm': 1.2185531854629517, 'learning_rate': 1.930464397473341e-05, 'epoch': 4.76}
 60%|█████▉    | 1190/2000 [1:43:54<1:08:25,  5.07s/it] 60%|█████▉    | 1191/2000 [1:43:59<1:10:00,  5.19s/it] 60%|█████▉    | 1192/2000 [1:44:04<1:10:06,  5.21s/it] 60%|█████▉    | 1193/2000 [1:44:09<1:10:33,  5.25s/it] 60%|█████▉    | 1194/2000 [1:44:14<1:09:13,  5.15s/it] 60%|█████▉    | 1195/2000 [1:44:19<1:08:50,  5.13s/it] 60%|█████▉    | 1196/2000 [1:44:24<1:08:24,  5.11s/it] 60%|█████▉    | 1197/2000 [1:44:30<1:09:26,  5.19s/it] 60%|█████▉    | 1198/2000 [1:44:35<1:09:09,  5.17s/it] 60%|█████▉    | 1199/2000 [1:44:40<1:10:25,  5.28s/it] 60%|██████    | 1200/2000 [1:44:45<1:08:57,  5.17s/it]                                                       {'loss': 0.0412, 'grad_norm': 1.749247431755066, 'learning_rate': 1.8902943044594735e-05, 'epoch': 4.8}
 60%|██████    | 1200/2000 [1:44:45<1:08:57,  5.17s/it] 60%|██████    | 1201/2000 [1:44:51<1:09:42,  5.23s/it] 60%|██████    | 1202/2000 [1:44:56<1:11:09,  5.35s/it] 60%|██████    | 1203/2000 [1:45:01<1:09:22,  5.22s/it] 60%|██████    | 1204/2000 [1:45:07<1:10:24,  5.31s/it] 60%|██████    | 1205/2000 [1:45:12<1:09:19,  5.23s/it] 60%|██████    | 1206/2000 [1:45:17<1:09:06,  5.22s/it] 60%|██████    | 1207/2000 [1:45:22<1:08:21,  5.17s/it] 60%|██████    | 1208/2000 [1:45:27<1:08:45,  5.21s/it] 60%|██████    | 1209/2000 [1:45:33<1:08:38,  5.21s/it] 60%|██████    | 1210/2000 [1:45:38<1:08:15,  5.18s/it]                                                       {'loss': 0.0526, 'grad_norm': 1.6715943813323975, 'learning_rate': 1.850290898894893e-05, 'epoch': 4.84}
 60%|██████    | 1210/2000 [1:45:38<1:08:15,  5.18s/it] 61%|██████    | 1211/2000 [1:45:43<1:08:44,  5.23s/it] 61%|██████    | 1212/2000 [1:45:48<1:09:12,  5.27s/it] 61%|██████    | 1213/2000 [1:45:53<1:08:25,  5.22s/it] 61%|██████    | 1214/2000 [1:45:58<1:07:30,  5.15s/it] 61%|██████    | 1215/2000 [1:46:04<1:08:12,  5.21s/it] 61%|██████    | 1216/2000 [1:46:09<1:07:46,  5.19s/it] 61%|██████    | 1217/2000 [1:46:14<1:08:14,  5.23s/it] 61%|██████    | 1218/2000 [1:46:20<1:08:07,  5.23s/it] 61%|██████    | 1219/2000 [1:46:25<1:08:14,  5.24s/it] 61%|██████    | 1220/2000 [1:46:30<1:08:26,  5.26s/it]                                                       {'loss': 0.0417, 'grad_norm': 1.7296545505523682, 'learning_rate': 1.8104651173112503e-05, 'epoch': 4.88}
 61%|██████    | 1220/2000 [1:46:30<1:08:26,  5.26s/it] 61%|██████    | 1221/2000 [1:46:35<1:08:25,  5.27s/it] 61%|██████    | 1222/2000 [1:46:41<1:07:53,  5.24s/it] 61%|██████    | 1223/2000 [1:46:45<1:06:27,  5.13s/it] 61%|██████    | 1224/2000 [1:46:51<1:06:40,  5.15s/it] 61%|██████▏   | 1225/2000 [1:46:56<1:08:57,  5.34s/it] 61%|██████▏   | 1226/2000 [1:47:01<1:07:19,  5.22s/it] 61%|██████▏   | 1227/2000 [1:47:07<1:07:59,  5.28s/it] 61%|██████▏   | 1228/2000 [1:47:12<1:08:26,  5.32s/it] 61%|██████▏   | 1229/2000 [1:47:18<1:09:04,  5.38s/it] 62%|██████▏   | 1230/2000 [1:47:23<1:07:08,  5.23s/it]                                                       {'loss': 0.0396, 'grad_norm': 1.761790156364441, 'learning_rate': 1.770827847679571e-05, 'epoch': 4.92}
 62%|██████▏   | 1230/2000 [1:47:23<1:07:08,  5.23s/it] 62%|██████▏   | 1231/2000 [1:47:28<1:06:33,  5.19s/it] 62%|██████▏   | 1232/2000 [1:47:33<1:06:47,  5.22s/it] 62%|██████▏   | 1233/2000 [1:47:38<1:06:06,  5.17s/it] 62%|██████▏   | 1234/2000 [1:47:43<1:07:03,  5.25s/it] 62%|██████▏   | 1235/2000 [1:47:49<1:06:47,  5.24s/it] 62%|██████▏   | 1236/2000 [1:47:54<1:07:05,  5.27s/it] 62%|██████▏   | 1237/2000 [1:47:59<1:07:01,  5.27s/it] 62%|██████▏   | 1238/2000 [1:48:05<1:07:44,  5.33s/it] 62%|██████▏   | 1239/2000 [1:48:10<1:05:59,  5.20s/it] 62%|██████▏   | 1240/2000 [1:48:15<1:06:04,  5.22s/it]                                                       {'loss': 0.0362, 'grad_norm': 1.1496320962905884, 'learning_rate': 1.7313899264335985e-05, 'epoch': 4.96}
 62%|██████▏   | 1240/2000 [1:48:15<1:06:04,  5.22s/it] 62%|██████▏   | 1241/2000 [1:48:20<1:04:42,  5.12s/it] 62%|██████▏   | 1242/2000 [1:48:25<1:03:35,  5.03s/it] 62%|██████▏   | 1243/2000 [1:48:30<1:03:33,  5.04s/it] 62%|██████▏   | 1244/2000 [1:48:35<1:05:08,  5.17s/it] 62%|██████▏   | 1245/2000 [1:48:40<1:05:29,  5.20s/it] 62%|██████▏   | 1246/2000 [1:48:46<1:05:08,  5.18s/it] 62%|██████▏   | 1247/2000 [1:48:51<1:05:25,  5.21s/it] 62%|██████▏   | 1248/2000 [1:48:56<1:04:56,  5.18s/it] 62%|██████▏   | 1249/2000 [1:49:01<1:04:21,  5.14s/it] 62%|██████▎   | 1250/2000 [1:49:06<1:03:35,  5.09s/it]                                                       {'loss': 0.0497, 'grad_norm': 0.9963491559028625, 'learning_rate': 1.6921621355072144e-05, 'epoch': 5.0}
 62%|██████▎   | 1250/2000 [1:49:08<1:03:35,  5.09s/it][INFO|trainer.py:3993] 2025-06-12 23:13:30,872 >> Saving model checkpoint to ./models/Qwen3-8B-llama-epoch8/checkpoint-1250
[INFO|configuration_utils.py:696] 2025-06-12 23:13:31,157 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-12 23:13:31,160 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2356] 2025-06-12 23:13:32,487 >> chat template saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1250/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-12 23:13:32,496 >> tokenizer config file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-12 23:13:32,506 >> Special tokens file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1250/special_tokens_map.json
 63%|██████▎   | 1251/2000 [1:49:17<1:26:28,  6.93s/it] 63%|██████▎   | 1252/2000 [1:49:23<1:20:35,  6.46s/it] 63%|██████▎   | 1253/2000 [1:49:28<1:15:47,  6.09s/it] 63%|██████▎   | 1254/2000 [1:49:33<1:12:40,  5.84s/it] 63%|██████▎   | 1255/2000 [1:49:38<1:09:43,  5.62s/it] 63%|██████▎   | 1256/2000 [1:49:44<1:09:00,  5.56s/it] 63%|██████▎   | 1257/2000 [1:49:49<1:07:09,  5.42s/it] 63%|██████▎   | 1258/2000 [1:49:54<1:07:02,  5.42s/it] 63%|██████▎   | 1259/2000 [1:50:00<1:07:47,  5.49s/it] 63%|██████▎   | 1260/2000 [1:50:05<1:08:19,  5.54s/it]                                                       {'loss': 0.023, 'grad_norm': 0.898017168045044, 'learning_rate': 1.6531551993867717e-05, 'epoch': 5.04}
 63%|██████▎   | 1260/2000 [1:50:05<1:08:19,  5.54s/it] 63%|██████▎   | 1261/2000 [1:50:11<1:07:12,  5.46s/it] 63%|██████▎   | 1262/2000 [1:50:16<1:06:12,  5.38s/it] 63%|██████▎   | 1263/2000 [1:50:21<1:04:42,  5.27s/it] 63%|██████▎   | 1264/2000 [1:50:26<1:04:03,  5.22s/it] 63%|██████▎   | 1265/2000 [1:50:31<1:03:02,  5.15s/it] 63%|██████▎   | 1266/2000 [1:50:36<1:03:26,  5.19s/it] 63%|██████▎   | 1267/2000 [1:50:42<1:04:23,  5.27s/it] 63%|██████▎   | 1268/2000 [1:50:47<1:03:56,  5.24s/it] 63%|██████▎   | 1269/2000 [1:50:52<1:02:55,  5.16s/it] 64%|██████▎   | 1270/2000 [1:50:57<1:03:19,  5.21s/it]                                                       {'loss': 0.0132, 'grad_norm': 0.8273861408233643, 'learning_rate': 1.614379782179124e-05, 'epoch': 5.08}
 64%|██████▎   | 1270/2000 [1:50:59<1:03:19,  5.21s/it] 64%|██████▎   | 1271/2000 [1:51:05<1:11:33,  5.89s/it] 64%|██████▎   | 1272/2000 [1:51:10<1:09:23,  5.72s/it] 64%|██████▎   | 1273/2000 [1:51:15<1:06:27,  5.48s/it] 64%|██████▎   | 1274/2000 [1:51:20<1:04:35,  5.34s/it] 64%|██████▍   | 1275/2000 [1:51:26<1:05:24,  5.41s/it] 64%|██████▍   | 1276/2000 [1:51:31<1:04:25,  5.34s/it] 64%|██████▍   | 1277/2000 [1:51:35<1:02:17,  5.17s/it] 64%|██████▍   | 1278/2000 [1:51:41<1:02:57,  5.23s/it] 64%|██████▍   | 1279/2000 [1:51:46<1:03:18,  5.27s/it] 64%|██████▍   | 1280/2000 [1:51:51<1:01:38,  5.14s/it]                                                       {'loss': 0.0151, 'grad_norm': 0.7883839011192322, 'learning_rate': 1.5758464846961657e-05, 'epoch': 5.12}
 64%|██████▍   | 1280/2000 [1:51:51<1:01:38,  5.14s/it] 64%|██████▍   | 1281/2000 [1:51:56<1:02:03,  5.18s/it] 64%|██████▍   | 1282/2000 [1:52:02<1:02:21,  5.21s/it] 64%|██████▍   | 1283/2000 [1:52:07<1:02:47,  5.26s/it] 64%|██████▍   | 1284/2000 [1:52:12<1:03:05,  5.29s/it] 64%|██████▍   | 1285/2000 [1:52:17<1:02:08,  5.22s/it] 64%|██████▍   | 1286/2000 [1:52:22<1:01:08,  5.14s/it] 64%|██████▍   | 1287/2000 [1:52:27<1:00:08,  5.06s/it] 64%|██████▍   | 1288/2000 [1:52:32<58:37,  4.94s/it]   64%|██████▍   | 1289/2000 [1:52:37<58:46,  4.96s/it] 64%|██████▍   | 1290/2000 [1:52:42<58:46,  4.97s/it]                                                     {'loss': 0.0121, 'grad_norm': 1.1033058166503906, 'learning_rate': 1.537565841556676e-05, 'epoch': 5.16}
 64%|██████▍   | 1290/2000 [1:52:42<58:46,  4.97s/it] 65%|██████▍   | 1291/2000 [1:52:47<59:08,  5.01s/it] 65%|██████▍   | 1292/2000 [1:52:52<58:22,  4.95s/it] 65%|██████▍   | 1293/2000 [1:52:57<58:50,  4.99s/it] 65%|██████▍   | 1294/2000 [1:53:02<59:07,  5.02s/it] 65%|██████▍   | 1295/2000 [1:53:08<1:01:17,  5.22s/it] 65%|██████▍   | 1296/2000 [1:53:13<1:02:07,  5.30s/it] 65%|██████▍   | 1297/2000 [1:53:18<1:01:30,  5.25s/it] 65%|██████▍   | 1298/2000 [1:53:23<1:00:48,  5.20s/it] 65%|██████▍   | 1299/2000 [1:53:29<1:01:11,  5.24s/it] 65%|██████▌   | 1300/2000 [1:53:34<1:00:02,  5.15s/it]                                                       {'loss': 0.0094, 'grad_norm': 0.25501754879951477, 'learning_rate': 1.499548318306259e-05, 'epoch': 5.2}
 65%|██████▌   | 1300/2000 [1:53:34<1:00:02,  5.15s/it] 65%|██████▌   | 1301/2000 [1:53:39<1:01:11,  5.25s/it] 65%|██████▌   | 1302/2000 [1:53:45<1:02:04,  5.34s/it] 65%|██████▌   | 1303/2000 [1:53:50<1:00:58,  5.25s/it] 65%|██████▌   | 1304/2000 [1:53:55<1:00:24,  5.21s/it] 65%|██████▌   | 1305/2000 [1:54:00<1:00:51,  5.25s/it] 65%|██████▌   | 1306/2000 [1:54:05<1:00:25,  5.22s/it] 65%|██████▌   | 1307/2000 [1:54:10<59:04,  5.12s/it]   65%|██████▌   | 1308/2000 [1:54:16<59:58,  5.20s/it] 65%|██████▌   | 1309/2000 [1:54:21<1:00:24,  5.25s/it] 66%|██████▌   | 1310/2000 [1:54:26<59:40,  5.19s/it]                                                       {'loss': 0.0156, 'grad_norm': 0.8562408089637756, 'learning_rate': 1.4618043085561702e-05, 'epoch': 5.24}
 66%|██████▌   | 1310/2000 [1:54:26<59:40,  5.19s/it] 66%|██████▌   | 1311/2000 [1:54:31<59:17,  5.16s/it] 66%|██████▌   | 1312/2000 [1:54:37<1:02:17,  5.43s/it] 66%|██████▌   | 1313/2000 [1:54:42<1:01:38,  5.38s/it] 66%|██████▌   | 1314/2000 [1:54:48<1:02:12,  5.44s/it] 66%|██████▌   | 1315/2000 [1:54:53<1:02:06,  5.44s/it] 66%|██████▌   | 1316/2000 [1:54:59<1:01:23,  5.39s/it] 66%|██████▌   | 1317/2000 [1:55:04<1:02:24,  5.48s/it] 66%|██████▌   | 1318/2000 [1:55:10<1:01:41,  5.43s/it] 66%|██████▌   | 1319/2000 [1:55:15<1:00:04,  5.29s/it] 66%|██████▌   | 1320/2000 [1:55:20<59:45,  5.27s/it]                                                       {'loss': 0.0094, 'grad_norm': 0.9318285584449768, 'learning_rate': 1.4243441311418013e-05, 'epoch': 5.28}
 66%|██████▌   | 1320/2000 [1:55:21<59:45,  5.27s/it] 66%|██████▌   | 1321/2000 [1:55:26<1:01:54,  5.47s/it] 66%|██████▌   | 1322/2000 [1:55:31<1:02:09,  5.50s/it] 66%|██████▌   | 1323/2000 [1:55:37<1:01:55,  5.49s/it] 66%|██████▌   | 1324/2000 [1:55:42<1:01:33,  5.46s/it] 66%|██████▋   | 1325/2000 [1:55:47<1:00:31,  5.38s/it] 66%|██████▋   | 1326/2000 [1:55:53<59:31,  5.30s/it]   66%|██████▋   | 1327/2000 [1:55:58<1:00:04,  5.36s/it] 66%|██████▋   | 1328/2000 [1:56:03<58:34,  5.23s/it]   66%|██████▋   | 1329/2000 [1:56:08<59:22,  5.31s/it] 66%|██████▋   | 1330/2000 [1:56:13<57:42,  5.17s/it]                                                     {'loss': 0.0111, 'grad_norm': 1.2270172834396362, 'learning_rate': 1.3871780273016215e-05, 'epoch': 5.32}
 66%|██████▋   | 1330/2000 [1:56:13<57:42,  5.17s/it] 67%|██████▋   | 1331/2000 [1:56:19<59:16,  5.32s/it] 67%|██████▋   | 1332/2000 [1:56:24<57:40,  5.18s/it] 67%|██████▋   | 1333/2000 [1:56:29<56:55,  5.12s/it] 67%|██████▋   | 1334/2000 [1:56:34<57:38,  5.19s/it] 67%|██████▋   | 1335/2000 [1:56:39<57:00,  5.14s/it] 67%|██████▋   | 1336/2000 [1:56:45<57:33,  5.20s/it] 67%|██████▋   | 1337/2000 [1:56:49<56:05,  5.08s/it] 67%|██████▋   | 1338/2000 [1:56:54<56:12,  5.09s/it] 67%|██████▋   | 1339/2000 [1:57:00<56:23,  5.12s/it] 67%|██████▋   | 1340/2000 [1:57:05<58:37,  5.33s/it]                                                     {'loss': 0.011, 'grad_norm': 1.362053632736206, 'learning_rate': 1.3503161578773193e-05, 'epoch': 5.36}
 67%|██████▋   | 1340/2000 [1:57:06<58:37,  5.33s/it] 67%|██████▋   | 1341/2000 [1:57:11<1:00:57,  5.55s/it] 67%|██████▋   | 1342/2000 [1:57:17<59:34,  5.43s/it]   67%|██████▋   | 1343/2000 [1:57:22<59:35,  5.44s/it] 67%|██████▋   | 1344/2000 [1:57:27<58:58,  5.39s/it] 67%|██████▋   | 1345/2000 [1:57:32<57:41,  5.28s/it] 67%|██████▋   | 1346/2000 [1:57:37<56:28,  5.18s/it] 67%|██████▋   | 1347/2000 [1:57:43<56:48,  5.22s/it] 67%|██████▋   | 1348/2000 [1:57:48<57:23,  5.28s/it] 67%|██████▋   | 1349/2000 [1:57:53<56:01,  5.16s/it] 68%|██████▊   | 1350/2000 [1:57:58<55:21,  5.11s/it]                                                     {'loss': 0.01, 'grad_norm': 1.435691237449646, 'learning_rate': 1.3137686005359318e-05, 'epoch': 5.4}
 68%|██████▊   | 1350/2000 [1:57:58<55:21,  5.11s/it] 68%|██████▊   | 1351/2000 [1:58:03<54:24,  5.03s/it] 68%|██████▊   | 1352/2000 [1:58:08<54:11,  5.02s/it] 68%|██████▊   | 1353/2000 [1:58:13<54:57,  5.10s/it] 68%|██████▊   | 1354/2000 [1:58:19<56:28,  5.25s/it] 68%|██████▊   | 1355/2000 [1:58:24<56:50,  5.29s/it] 68%|██████▊   | 1356/2000 [1:58:29<55:37,  5.18s/it] 68%|██████▊   | 1357/2000 [1:58:34<56:16,  5.25s/it] 68%|██████▊   | 1358/2000 [1:58:40<56:26,  5.27s/it] 68%|██████▊   | 1359/2000 [1:58:45<56:33,  5.29s/it] 68%|██████▊   | 1360/2000 [1:58:50<55:34,  5.21s/it]                                                     {'loss': 0.0123, 'grad_norm': 0.8125078082084656, 'learning_rate': 1.2775453470147106e-05, 'epoch': 5.44}
 68%|██████▊   | 1360/2000 [1:58:50<55:34,  5.21s/it] 68%|██████▊   | 1361/2000 [1:58:55<55:56,  5.25s/it] 68%|██████▊   | 1362/2000 [1:59:01<55:21,  5.21s/it] 68%|██████▊   | 1363/2000 [1:59:06<54:42,  5.15s/it] 68%|██████▊   | 1364/2000 [1:59:11<54:02,  5.10s/it] 68%|██████▊   | 1365/2000 [1:59:16<54:13,  5.12s/it] 68%|██████▊   | 1366/2000 [1:59:21<53:13,  5.04s/it] 68%|██████▊   | 1367/2000 [1:59:26<54:18,  5.15s/it] 68%|██████▊   | 1368/2000 [1:59:31<54:06,  5.14s/it] 68%|██████▊   | 1369/2000 [1:59:36<53:34,  5.09s/it] 68%|██████▊   | 1370/2000 [1:59:41<53:08,  5.06s/it]                                                     {'loss': 0.0108, 'grad_norm': 1.0587670803070068, 'learning_rate': 1.2416563003894826e-05, 'epoch': 5.48}
 68%|██████▊   | 1370/2000 [1:59:41<53:08,  5.06s/it] 69%|██████▊   | 1371/2000 [1:59:46<53:00,  5.06s/it] 69%|██████▊   | 1372/2000 [1:59:51<52:46,  5.04s/it] 69%|██████▊   | 1373/2000 [1:59:56<52:45,  5.05s/it] 69%|██████▊   | 1374/2000 [2:00:01<52:16,  5.01s/it] 69%|██████▉   | 1375/2000 [2:00:07<55:17,  5.31s/it] 69%|██████▉   | 1376/2000 [2:00:12<55:16,  5.32s/it] 69%|██████▉   | 1377/2000 [2:00:18<54:29,  5.25s/it] 69%|██████▉   | 1378/2000 [2:00:22<53:10,  5.13s/it] 69%|██████▉   | 1379/2000 [2:00:28<53:43,  5.19s/it] 69%|██████▉   | 1380/2000 [2:00:33<54:51,  5.31s/it]                                                     {'loss': 0.0193, 'grad_norm': 1.9051893949508667, 'learning_rate': 1.2061112723672438e-05, 'epoch': 5.52}
 69%|██████▉   | 1380/2000 [2:00:35<54:51,  5.31s/it] 69%|██████▉   | 1381/2000 [2:00:40<58:22,  5.66s/it] 69%|██████▉   | 1382/2000 [2:00:46<59:06,  5.74s/it] 69%|██████▉   | 1383/2000 [2:00:51<56:44,  5.52s/it] 69%|██████▉   | 1384/2000 [2:00:56<55:35,  5.41s/it] 69%|██████▉   | 1385/2000 [2:01:01<54:39,  5.33s/it] 69%|██████▉   | 1386/2000 [2:01:06<53:51,  5.26s/it] 69%|██████▉   | 1387/2000 [2:01:11<53:37,  5.25s/it] 69%|██████▉   | 1388/2000 [2:01:16<52:43,  5.17s/it] 69%|██████▉   | 1389/2000 [2:01:22<53:03,  5.21s/it] 70%|██████▉   | 1390/2000 [2:01:27<52:56,  5.21s/it]                                                     {'loss': 0.0178, 'grad_norm': 0.87193763256073, 'learning_rate': 1.170919980603741e-05, 'epoch': 5.56}
 70%|██████▉   | 1390/2000 [2:01:27<52:56,  5.21s/it] 70%|██████▉   | 1391/2000 [2:01:32<53:56,  5.31s/it] 70%|██████▉   | 1392/2000 [2:01:38<53:50,  5.31s/it] 70%|██████▉   | 1393/2000 [2:01:42<52:09,  5.15s/it] 70%|██████▉   | 1394/2000 [2:01:48<52:09,  5.16s/it] 70%|██████▉   | 1395/2000 [2:01:54<54:36,  5.42s/it] 70%|██████▉   | 1396/2000 [2:01:59<53:03,  5.27s/it] 70%|██████▉   | 1397/2000 [2:02:04<52:43,  5.25s/it] 70%|██████▉   | 1398/2000 [2:02:09<52:00,  5.18s/it] 70%|██████▉   | 1399/2000 [2:02:14<50:54,  5.08s/it] 70%|███████   | 1400/2000 [2:02:19<51:10,  5.12s/it]                                                     {'loss': 0.013, 'grad_norm': 0.9877201318740845, 'learning_rate': 1.1360920460467598e-05, 'epoch': 5.6}
 70%|███████   | 1400/2000 [2:02:19<51:10,  5.12s/it] 70%|███████   | 1401/2000 [2:02:25<52:43,  5.28s/it] 70%|███████   | 1402/2000 [2:02:29<51:38,  5.18s/it] 70%|███████   | 1403/2000 [2:02:34<50:29,  5.07s/it] 70%|███████   | 1404/2000 [2:02:39<50:12,  5.06s/it] 70%|███████   | 1405/2000 [2:02:45<51:41,  5.21s/it] 70%|███████   | 1406/2000 [2:02:50<51:01,  5.15s/it] 70%|███████   | 1407/2000 [2:02:56<52:44,  5.34s/it] 70%|███████   | 1408/2000 [2:03:01<51:44,  5.24s/it] 70%|███████   | 1409/2000 [2:03:06<51:30,  5.23s/it] 70%|███████   | 1410/2000 [2:03:12<53:04,  5.40s/it]                                                     {'loss': 0.0135, 'grad_norm': 0.5690433979034424, 'learning_rate': 1.1016369903058529e-05, 'epoch': 5.64}
 70%|███████   | 1410/2000 [2:03:12<53:04,  5.40s/it] 71%|███████   | 1411/2000 [2:03:17<52:54,  5.39s/it] 71%|███████   | 1412/2000 [2:03:22<52:05,  5.31s/it] 71%|███████   | 1413/2000 [2:03:28<52:13,  5.34s/it] 71%|███████   | 1414/2000 [2:03:33<51:09,  5.24s/it] 71%|███████   | 1415/2000 [2:03:37<49:51,  5.11s/it] 71%|███████   | 1416/2000 [2:03:42<49:38,  5.10s/it] 71%|███████   | 1417/2000 [2:03:48<49:58,  5.14s/it] 71%|███████   | 1418/2000 [2:03:53<50:00,  5.16s/it] 71%|███████   | 1419/2000 [2:03:59<52:08,  5.38s/it] 71%|███████   | 1420/2000 [2:04:04<50:30,  5.22s/it]                                                     {'loss': 0.0191, 'grad_norm': 0.6493420600891113, 'learning_rate': 1.0675642330492286e-05, 'epoch': 5.68}
 71%|███████   | 1420/2000 [2:04:04<50:30,  5.22s/it] 71%|███████   | 1421/2000 [2:04:09<51:20,  5.32s/it] 71%|███████   | 1422/2000 [2:04:14<50:35,  5.25s/it] 71%|███████   | 1423/2000 [2:04:19<50:02,  5.20s/it] 71%|███████   | 1424/2000 [2:04:24<48:44,  5.08s/it] 71%|███████▏  | 1425/2000 [2:04:29<48:52,  5.10s/it] 71%|███████▏  | 1426/2000 [2:04:35<49:12,  5.14s/it] 71%|███████▏  | 1427/2000 [2:04:40<49:03,  5.14s/it] 71%|███████▏  | 1428/2000 [2:04:45<48:40,  5.11s/it] 71%|███████▏  | 1429/2000 [2:04:50<48:32,  5.10s/it] 72%|███████▏  | 1430/2000 [2:04:55<48:22,  5.09s/it]                                                     {'loss': 0.0248, 'grad_norm': 2.069669008255005, 'learning_rate': 1.0338830894285065e-05, 'epoch': 5.72}
 72%|███████▏  | 1430/2000 [2:04:55<48:22,  5.09s/it] 72%|███████▏  | 1431/2000 [2:05:00<48:09,  5.08s/it] 72%|███████▏  | 1432/2000 [2:05:05<48:31,  5.13s/it] 72%|███████▏  | 1433/2000 [2:05:10<48:14,  5.11s/it] 72%|███████▏  | 1434/2000 [2:05:16<48:48,  5.17s/it] 72%|███████▏  | 1435/2000 [2:05:20<47:50,  5.08s/it] 72%|███████▏  | 1436/2000 [2:05:26<47:59,  5.11s/it] 72%|███████▏  | 1437/2000 [2:05:31<47:31,  5.06s/it] 72%|███████▏  | 1438/2000 [2:05:36<47:49,  5.11s/it] 72%|███████▏  | 1439/2000 [2:05:40<46:26,  4.97s/it] 72%|███████▏  | 1440/2000 [2:05:46<47:49,  5.12s/it]                                                     {'loss': 0.0081, 'grad_norm': 0.47190406918525696, 'learning_rate': 1.0006027675320493e-05, 'epoch': 5.76}
 72%|███████▏  | 1440/2000 [2:05:46<47:49,  5.12s/it] 72%|███████▏  | 1441/2000 [2:05:51<47:51,  5.14s/it] 72%|███████▏  | 1442/2000 [2:05:56<47:40,  5.13s/it] 72%|███████▏  | 1443/2000 [2:06:01<47:40,  5.14s/it] 72%|███████▏  | 1444/2000 [2:06:06<46:41,  5.04s/it] 72%|███████▏  | 1445/2000 [2:06:11<46:45,  5.05s/it] 72%|███████▏  | 1446/2000 [2:06:17<48:52,  5.29s/it] 72%|███████▏  | 1447/2000 [2:06:22<48:29,  5.26s/it] 72%|███████▏  | 1448/2000 [2:06:28<49:35,  5.39s/it] 72%|███████▏  | 1449/2000 [2:06:33<48:45,  5.31s/it] 72%|███████▎  | 1450/2000 [2:06:38<47:27,  5.18s/it]                                                     {'loss': 0.0123, 'grad_norm': 2.3991918563842773, 'learning_rate': 9.677323658675594e-06, 'epoch': 5.8}
 72%|███████▎  | 1450/2000 [2:06:38<47:27,  5.18s/it] 73%|███████▎  | 1451/2000 [2:06:44<48:58,  5.35s/it] 73%|███████▎  | 1452/2000 [2:06:49<47:30,  5.20s/it] 73%|███████▎  | 1453/2000 [2:06:54<46:54,  5.14s/it] 73%|███████▎  | 1454/2000 [2:07:00<49:12,  5.41s/it] 73%|███████▎  | 1455/2000 [2:07:05<49:33,  5.46s/it] 73%|███████▎  | 1456/2000 [2:07:10<47:36,  5.25s/it] 73%|███████▎  | 1457/2000 [2:07:15<46:45,  5.17s/it] 73%|███████▎  | 1458/2000 [2:07:20<47:03,  5.21s/it] 73%|███████▎  | 1459/2000 [2:07:25<46:25,  5.15s/it] 73%|███████▎  | 1460/2000 [2:07:31<46:49,  5.20s/it]                                                     {'loss': 0.0126, 'grad_norm': 1.9974632263183594, 'learning_rate': 9.352808708746441e-06, 'epoch': 5.84}
 73%|███████▎  | 1460/2000 [2:07:31<46:49,  5.20s/it] 73%|███████▎  | 1461/2000 [2:07:36<47:48,  5.32s/it] 73%|███████▎  | 1462/2000 [2:07:42<47:55,  5.35s/it] 73%|███████▎  | 1463/2000 [2:07:47<47:10,  5.27s/it] 73%|███████▎  | 1464/2000 [2:07:51<45:53,  5.14s/it] 73%|███████▎  | 1465/2000 [2:07:57<45:47,  5.13s/it] 73%|███████▎  | 1466/2000 [2:08:02<47:02,  5.29s/it] 73%|███████▎  | 1467/2000 [2:08:07<46:17,  5.21s/it] 73%|███████▎  | 1468/2000 [2:08:12<45:23,  5.12s/it] 73%|███████▎  | 1469/2000 [2:08:17<45:41,  5.16s/it] 74%|███████▎  | 1470/2000 [2:08:23<45:51,  5.19s/it]                                                     {'loss': 0.0187, 'grad_norm': 1.9253125190734863, 'learning_rate': 9.032571544680086e-06, 'epoch': 5.88}
 74%|███████▎  | 1470/2000 [2:08:23<45:51,  5.19s/it] 74%|███████▎  | 1471/2000 [2:08:28<46:13,  5.24s/it] 74%|███████▎  | 1472/2000 [2:08:33<45:53,  5.22s/it] 74%|███████▎  | 1473/2000 [2:08:39<46:03,  5.24s/it] 74%|███████▎  | 1474/2000 [2:08:44<46:31,  5.31s/it] 74%|███████▍  | 1475/2000 [2:08:49<46:10,  5.28s/it] 74%|███████▍  | 1476/2000 [2:08:54<45:45,  5.24s/it] 74%|███████▍  | 1477/2000 [2:09:00<46:20,  5.32s/it] 74%|███████▍  | 1478/2000 [2:09:05<46:28,  5.34s/it] 74%|███████▍  | 1479/2000 [2:09:10<45:42,  5.26s/it] 74%|███████▍  | 1480/2000 [2:09:15<45:00,  5.19s/it]                                                     {'loss': 0.0148, 'grad_norm': 1.5963048934936523, 'learning_rate': 8.71669971611963e-06, 'epoch': 5.92}
 74%|███████▍  | 1480/2000 [2:09:15<45:00,  5.19s/it] 74%|███████▍  | 1481/2000 [2:09:21<45:18,  5.24s/it] 74%|███████▍  | 1482/2000 [2:09:26<44:45,  5.18s/it] 74%|███████▍  | 1483/2000 [2:09:31<44:27,  5.16s/it] 74%|███████▍  | 1484/2000 [2:09:36<45:31,  5.29s/it] 74%|███████▍  | 1485/2000 [2:09:42<44:59,  5.24s/it] 74%|███████▍  | 1486/2000 [2:09:47<45:06,  5.27s/it] 74%|███████▍  | 1487/2000 [2:09:52<44:30,  5.21s/it] 74%|███████▍  | 1488/2000 [2:09:57<43:42,  5.12s/it] 74%|███████▍  | 1489/2000 [2:10:02<44:14,  5.19s/it] 74%|███████▍  | 1490/2000 [2:10:08<45:47,  5.39s/it]                                                     {'loss': 0.0242, 'grad_norm': 0.36920061707496643, 'learning_rate': 8.405279579269046e-06, 'epoch': 5.96}
 74%|███████▍  | 1490/2000 [2:10:08<45:47,  5.39s/it] 75%|███████▍  | 1491/2000 [2:10:13<44:43,  5.27s/it] 75%|███████▍  | 1492/2000 [2:10:18<44:11,  5.22s/it] 75%|███████▍  | 1493/2000 [2:10:23<43:34,  5.16s/it] 75%|███████▍  | 1494/2000 [2:10:28<43:46,  5.19s/it] 75%|███████▍  | 1495/2000 [2:10:34<43:17,  5.14s/it] 75%|███████▍  | 1496/2000 [2:10:39<42:56,  5.11s/it] 75%|███████▍  | 1497/2000 [2:10:44<43:14,  5.16s/it] 75%|███████▍  | 1498/2000 [2:10:50<44:45,  5.35s/it] 75%|███████▍  | 1499/2000 [2:10:54<43:21,  5.19s/it] 75%|███████▌  | 1500/2000 [2:11:00<43:18,  5.20s/it]                                                     {'loss': 0.0127, 'grad_norm': 1.769115686416626, 'learning_rate': 8.098396273284236e-06, 'epoch': 6.0}
 75%|███████▌  | 1500/2000 [2:11:00<43:18,  5.20s/it][INFO|trainer.py:3993] 2025-06-12 23:35:23,066 >> Saving model checkpoint to ./models/Qwen3-8B-llama-epoch8/checkpoint-1500
[INFO|configuration_utils.py:696] 2025-06-12 23:35:23,155 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-12 23:35:23,157 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2356] 2025-06-12 23:35:24,172 >> chat template saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1500/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-12 23:35:24,197 >> tokenizer config file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-12 23:35:24,211 >> Special tokens file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1500/special_tokens_map.json
[INFO|trainer.py:4102] 2025-06-12 23:35:29,630 >> Deleting older checkpoint [models/Qwen3-8B-llama-epoch8/checkpoint-250] due to args.save_total_limit
 75%|███████▌  | 1501/2000 [2:11:12<1:00:26,  7.27s/it] 75%|███████▌  | 1502/2000 [2:11:17<55:08,  6.64s/it]   75%|███████▌  | 1503/2000 [2:11:22<51:23,  6.20s/it] 75%|███████▌  | 1504/2000 [2:11:27<47:59,  5.81s/it] 75%|███████▌  | 1505/2000 [2:11:32<46:49,  5.68s/it] 75%|███████▌  | 1506/2000 [2:11:37<44:51,  5.45s/it] 75%|███████▌  | 1507/2000 [2:11:43<44:39,  5.43s/it] 75%|███████▌  | 1508/2000 [2:11:48<43:14,  5.27s/it] 75%|███████▌  | 1509/2000 [2:11:53<43:40,  5.34s/it] 76%|███████▌  | 1510/2000 [2:11:58<42:26,  5.20s/it]                                                     {'loss': 0.0074, 'grad_norm': 3.611658811569214, 'learning_rate': 7.796133696996858e-06, 'epoch': 6.04}
 76%|███████▌  | 1510/2000 [2:11:58<42:26,  5.20s/it] 76%|███████▌  | 1511/2000 [2:12:03<43:04,  5.29s/it] 76%|███████▌  | 1512/2000 [2:12:09<44:18,  5.45s/it] 76%|███████▌  | 1513/2000 [2:12:14<43:29,  5.36s/it] 76%|███████▌  | 1514/2000 [2:12:19<42:01,  5.19s/it] 76%|███████▌  | 1515/2000 [2:12:24<41:55,  5.19s/it] 76%|███████▌  | 1516/2000 [2:12:29<41:26,  5.14s/it] 76%|███████▌  | 1517/2000 [2:12:34<41:08,  5.11s/it] 76%|███████▌  | 1518/2000 [2:12:40<41:08,  5.12s/it] 76%|███████▌  | 1519/2000 [2:12:45<42:34,  5.31s/it] 76%|███████▌  | 1520/2000 [2:12:50<41:51,  5.23s/it]                                                     {'loss': 0.0035, 'grad_norm': 0.1974148452281952, 'learning_rate': 7.498574485977172e-06, 'epoch': 6.08}
 76%|███████▌  | 1520/2000 [2:12:50<41:51,  5.23s/it] 76%|███████▌  | 1521/2000 [2:12:56<42:36,  5.34s/it] 76%|███████▌  | 1522/2000 [2:13:01<42:03,  5.28s/it] 76%|███████▌  | 1523/2000 [2:13:06<41:50,  5.26s/it] 76%|███████▌  | 1524/2000 [2:13:11<41:20,  5.21s/it] 76%|███████▋  | 1525/2000 [2:13:16<40:33,  5.12s/it] 76%|███████▋  | 1526/2000 [2:13:21<40:25,  5.12s/it] 76%|███████▋  | 1527/2000 [2:13:26<39:39,  5.03s/it] 76%|███████▋  | 1528/2000 [2:13:31<39:24,  5.01s/it] 76%|███████▋  | 1529/2000 [2:13:36<39:22,  5.02s/it] 76%|███████▋  | 1530/2000 [2:13:42<40:35,  5.18s/it]                                                     {'loss': 0.0025, 'grad_norm': 0.17174473404884338, 'learning_rate': 7.205799989942372e-06, 'epoch': 6.12}
 76%|███████▋  | 1530/2000 [2:13:42<40:35,  5.18s/it] 77%|███████▋  | 1531/2000 [2:13:47<39:54,  5.11s/it] 77%|███████▋  | 1532/2000 [2:13:52<40:45,  5.23s/it] 77%|███████▋  | 1533/2000 [2:13:57<39:59,  5.14s/it] 77%|███████▋  | 1534/2000 [2:14:03<40:43,  5.24s/it] 77%|███████▋  | 1535/2000 [2:14:08<39:38,  5.11s/it] 77%|███████▋  | 1536/2000 [2:14:13<40:59,  5.30s/it] 77%|███████▋  | 1537/2000 [2:14:19<40:45,  5.28s/it] 77%|███████▋  | 1538/2000 [2:14:24<40:55,  5.32s/it] 77%|███████▋  | 1539/2000 [2:14:29<40:42,  5.30s/it] 77%|███████▋  | 1540/2000 [2:14:35<40:57,  5.34s/it]                                                     {'loss': 0.0055, 'grad_norm': 1.0977089405059814, 'learning_rate': 6.91789025051634e-06, 'epoch': 6.16}
 77%|███████▋  | 1540/2000 [2:14:36<40:57,  5.34s/it] 77%|███████▋  | 1541/2000 [2:14:41<43:23,  5.67s/it] 77%|███████▋  | 1542/2000 [2:14:47<44:22,  5.81s/it] 77%|███████▋  | 1543/2000 [2:14:52<42:21,  5.56s/it] 77%|███████▋  | 1544/2000 [2:14:57<41:33,  5.47s/it] 77%|███████▋  | 1545/2000 [2:15:03<40:54,  5.39s/it] 77%|███████▋  | 1546/2000 [2:15:08<40:17,  5.33s/it] 77%|███████▋  | 1547/2000 [2:15:13<39:49,  5.28s/it] 77%|███████▋  | 1548/2000 [2:15:19<40:42,  5.40s/it] 77%|███████▋  | 1549/2000 [2:15:24<39:33,  5.26s/it] 78%|███████▊  | 1550/2000 [2:15:29<39:17,  5.24s/it]                                                     {'loss': 0.0063, 'grad_norm': 0.3600948750972748, 'learning_rate': 6.634923979347074e-06, 'epoch': 6.2}
 78%|███████▊  | 1550/2000 [2:15:29<39:17,  5.24s/it] 78%|███████▊  | 1551/2000 [2:15:34<40:07,  5.36s/it] 78%|███████▊  | 1552/2000 [2:15:39<39:14,  5.26s/it] 78%|███████▊  | 1553/2000 [2:15:45<38:48,  5.21s/it] 78%|███████▊  | 1554/2000 [2:15:50<38:12,  5.14s/it] 78%|███████▊  | 1555/2000 [2:15:55<38:08,  5.14s/it] 78%|███████▊  | 1556/2000 [2:16:00<37:23,  5.05s/it] 78%|███████▊  | 1557/2000 [2:16:05<38:01,  5.15s/it] 78%|███████▊  | 1558/2000 [2:16:10<37:38,  5.11s/it] 78%|███████▊  | 1559/2000 [2:16:15<36:45,  5.00s/it] 78%|███████▊  | 1560/2000 [2:16:20<37:26,  5.10s/it]                                                     {'loss': 0.0047, 'grad_norm': 0.1402607411146164, 'learning_rate': 6.3569785365877125e-06, 'epoch': 6.24}
 78%|███████▊  | 1560/2000 [2:16:20<37:26,  5.10s/it] 78%|███████▊  | 1561/2000 [2:16:25<37:28,  5.12s/it] 78%|███████▊  | 1562/2000 [2:16:30<36:52,  5.05s/it] 78%|███████▊  | 1563/2000 [2:16:35<36:51,  5.06s/it] 78%|███████▊  | 1564/2000 [2:16:40<36:36,  5.04s/it] 78%|███████▊  | 1565/2000 [2:16:45<36:28,  5.03s/it] 78%|███████▊  | 1566/2000 [2:16:50<36:29,  5.05s/it] 78%|███████▊  | 1567/2000 [2:16:56<37:47,  5.24s/it] 78%|███████▊  | 1568/2000 [2:17:01<37:39,  5.23s/it] 78%|███████▊  | 1569/2000 [2:17:06<37:30,  5.22s/it] 78%|███████▊  | 1570/2000 [2:17:12<38:04,  5.31s/it]                                                     {'loss': 0.0028, 'grad_norm': 0.5384505391120911, 'learning_rate': 6.084129909747033e-06, 'epoch': 6.28}
 78%|███████▊  | 1570/2000 [2:17:12<38:04,  5.31s/it] 79%|███████▊  | 1571/2000 [2:17:17<37:22,  5.23s/it] 79%|███████▊  | 1572/2000 [2:17:22<37:11,  5.21s/it] 79%|███████▊  | 1573/2000 [2:17:27<37:07,  5.22s/it] 79%|███████▊  | 1574/2000 [2:17:33<37:30,  5.28s/it] 79%|███████▉  | 1575/2000 [2:17:38<38:08,  5.39s/it] 79%|███████▉  | 1576/2000 [2:17:43<36:34,  5.18s/it] 79%|███████▉  | 1577/2000 [2:17:48<36:28,  5.17s/it] 79%|███████▉  | 1578/2000 [2:17:54<37:06,  5.28s/it] 79%|███████▉  | 1579/2000 [2:17:59<36:10,  5.15s/it] 79%|███████▉  | 1580/2000 [2:18:04<36:52,  5.27s/it]                                                     {'loss': 0.0078, 'grad_norm': 0.3181851804256439, 'learning_rate': 5.816452692915242e-06, 'epoch': 6.32}
 79%|███████▉  | 1580/2000 [2:18:04<36:52,  5.27s/it] 79%|███████▉  | 1581/2000 [2:18:10<37:49,  5.42s/it] 79%|███████▉  | 1582/2000 [2:18:15<37:01,  5.31s/it] 79%|███████▉  | 1583/2000 [2:18:20<36:28,  5.25s/it] 79%|███████▉  | 1584/2000 [2:18:25<35:56,  5.18s/it] 79%|███████▉  | 1585/2000 [2:18:31<37:35,  5.44s/it] 79%|███████▉  | 1586/2000 [2:18:36<37:17,  5.40s/it] 79%|███████▉  | 1587/2000 [2:18:42<37:05,  5.39s/it] 79%|███████▉  | 1588/2000 [2:18:47<36:35,  5.33s/it] 79%|███████▉  | 1589/2000 [2:18:52<36:32,  5.33s/it] 80%|███████▉  | 1590/2000 [2:18:57<36:02,  5.27s/it]                                                     {'loss': 0.0029, 'grad_norm': 0.6008574962615967, 'learning_rate': 5.554020066370677e-06, 'epoch': 6.36}
 80%|███████▉  | 1590/2000 [2:18:57<36:02,  5.27s/it] 80%|███████▉  | 1591/2000 [2:19:02<35:29,  5.21s/it] 80%|███████▉  | 1592/2000 [2:19:08<35:47,  5.26s/it] 80%|███████▉  | 1593/2000 [2:19:13<35:27,  5.23s/it] 80%|███████▉  | 1594/2000 [2:19:18<35:35,  5.26s/it] 80%|███████▉  | 1595/2000 [2:19:24<35:32,  5.27s/it] 80%|███████▉  | 1596/2000 [2:19:29<34:53,  5.18s/it] 80%|███████▉  | 1597/2000 [2:19:34<34:47,  5.18s/it] 80%|███████▉  | 1598/2000 [2:19:39<34:32,  5.15s/it] 80%|███████▉  | 1599/2000 [2:19:44<34:09,  5.11s/it] 80%|████████  | 1600/2000 [2:19:49<34:59,  5.25s/it]                                                     {'loss': 0.0034, 'grad_norm': 0.11751804500818253, 'learning_rate': 5.296903776573065e-06, 'epoch': 6.4}
 80%|████████  | 1600/2000 [2:19:50<34:59,  5.25s/it] 80%|████████  | 1601/2000 [2:19:55<34:32,  5.19s/it] 80%|████████  | 1602/2000 [2:20:00<34:52,  5.26s/it] 80%|████████  | 1603/2000 [2:20:05<34:35,  5.23s/it] 80%|████████  | 1604/2000 [2:20:11<35:17,  5.35s/it] 80%|████████  | 1605/2000 [2:20:16<34:54,  5.30s/it] 80%|████████  | 1606/2000 [2:20:22<35:24,  5.39s/it] 80%|████████  | 1607/2000 [2:20:27<34:34,  5.28s/it] 80%|████████  | 1608/2000 [2:20:32<33:54,  5.19s/it] 80%|████████  | 1609/2000 [2:20:37<33:38,  5.16s/it] 80%|████████  | 1610/2000 [2:20:42<34:08,  5.25s/it]                                                     {'loss': 0.003, 'grad_norm': 0.5047596096992493, 'learning_rate': 5.045174116548745e-06, 'epoch': 6.44}
 80%|████████  | 1610/2000 [2:20:43<34:08,  5.25s/it] 81%|████████  | 1611/2000 [2:20:48<35:31,  5.48s/it] 81%|████████  | 1612/2000 [2:20:54<36:04,  5.58s/it] 81%|████████  | 1613/2000 [2:20:59<34:52,  5.41s/it] 81%|████████  | 1614/2000 [2:21:04<34:06,  5.30s/it] 81%|████████  | 1615/2000 [2:21:10<35:00,  5.46s/it] 81%|████████  | 1616/2000 [2:21:15<34:10,  5.34s/it] 81%|████████  | 1617/2000 [2:21:21<35:04,  5.49s/it] 81%|████████  | 1618/2000 [2:21:26<33:47,  5.31s/it] 81%|████████  | 1619/2000 [2:21:31<33:29,  5.27s/it] 81%|████████  | 1620/2000 [2:21:36<32:44,  5.17s/it]                                                     {'loss': 0.0051, 'grad_norm': 0.1651899516582489, 'learning_rate': 4.798899906673263e-06, 'epoch': 6.48}
 81%|████████  | 1620/2000 [2:21:36<32:44,  5.17s/it] 81%|████████  | 1621/2000 [2:21:41<32:55,  5.21s/it] 81%|████████  | 1622/2000 [2:21:46<32:18,  5.13s/it] 81%|████████  | 1623/2000 [2:21:51<31:36,  5.03s/it] 81%|████████  | 1624/2000 [2:21:56<32:02,  5.11s/it] 81%|████████▏ | 1625/2000 [2:22:01<31:55,  5.11s/it] 81%|████████▏ | 1626/2000 [2:22:07<32:40,  5.24s/it] 81%|████████▏ | 1627/2000 [2:22:12<32:22,  5.21s/it] 81%|████████▏ | 1628/2000 [2:22:17<32:05,  5.18s/it] 81%|████████▏ | 1629/2000 [2:22:22<32:36,  5.27s/it] 82%|████████▏ | 1630/2000 [2:22:28<32:16,  5.23s/it]                                                     {'loss': 0.0069, 'grad_norm': 0.24463394284248352, 'learning_rate': 4.5581484758565665e-06, 'epoch': 6.52}
 82%|████████▏ | 1630/2000 [2:22:28<32:16,  5.23s/it] 82%|████████▏ | 1631/2000 [2:22:33<32:04,  5.21s/it] 82%|████████▏ | 1632/2000 [2:22:38<31:32,  5.14s/it] 82%|████████▏ | 1633/2000 [2:22:43<32:12,  5.27s/it] 82%|████████▏ | 1634/2000 [2:22:48<32:01,  5.25s/it] 82%|████████▏ | 1635/2000 [2:22:53<31:28,  5.18s/it] 82%|████████▏ | 1636/2000 [2:22:59<32:48,  5.41s/it] 82%|████████▏ | 1637/2000 [2:23:05<32:30,  5.37s/it] 82%|████████▏ | 1638/2000 [2:23:10<31:55,  5.29s/it] 82%|████████▏ | 1639/2000 [2:23:15<31:31,  5.24s/it] 82%|████████▏ | 1640/2000 [2:23:20<31:06,  5.18s/it]                                                     {'loss': 0.0061, 'grad_norm': 0.1520947366952896, 'learning_rate': 4.322985643135952e-06, 'epoch': 6.56}
 82%|████████▏ | 1640/2000 [2:23:20<31:06,  5.18s/it] 82%|████████▏ | 1641/2000 [2:23:25<30:43,  5.13s/it] 82%|████████▏ | 1642/2000 [2:23:30<30:23,  5.09s/it] 82%|████████▏ | 1643/2000 [2:23:35<30:12,  5.08s/it] 82%|████████▏ | 1644/2000 [2:23:40<30:06,  5.07s/it] 82%|████████▏ | 1645/2000 [2:23:46<30:36,  5.17s/it] 82%|████████▏ | 1646/2000 [2:23:51<31:19,  5.31s/it] 82%|████████▏ | 1647/2000 [2:23:56<30:39,  5.21s/it] 82%|████████▏ | 1648/2000 [2:24:01<30:15,  5.16s/it] 82%|████████▏ | 1649/2000 [2:24:07<30:29,  5.21s/it] 82%|████████▎ | 1650/2000 [2:24:12<30:23,  5.21s/it]                                                     {'loss': 0.0093, 'grad_norm': 1.6292918920516968, 'learning_rate': 4.093475699681806e-06, 'epoch': 6.6}
 82%|████████▎ | 1650/2000 [2:24:12<30:23,  5.21s/it] 83%|████████▎ | 1651/2000 [2:24:17<30:16,  5.20s/it] 83%|████████▎ | 1652/2000 [2:24:22<30:19,  5.23s/it] 83%|████████▎ | 1653/2000 [2:24:27<29:32,  5.11s/it] 83%|████████▎ | 1654/2000 [2:24:32<29:21,  5.09s/it] 83%|████████▎ | 1655/2000 [2:24:37<29:10,  5.07s/it] 83%|████████▎ | 1656/2000 [2:24:42<28:44,  5.01s/it] 83%|████████▎ | 1657/2000 [2:24:47<29:11,  5.11s/it] 83%|████████▎ | 1658/2000 [2:24:53<29:46,  5.22s/it] 83%|████████▎ | 1659/2000 [2:24:58<29:17,  5.15s/it] 83%|████████▎ | 1660/2000 [2:25:03<28:53,  5.10s/it]                                                     {'loss': 0.0025, 'grad_norm': 1.5129895210266113, 'learning_rate': 3.869681391221011e-06, 'epoch': 6.64}
 83%|████████▎ | 1660/2000 [2:25:03<28:53,  5.10s/it] 83%|████████▎ | 1661/2000 [2:25:09<30:26,  5.39s/it] 83%|████████▎ | 1662/2000 [2:25:14<30:11,  5.36s/it] 83%|████████▎ | 1663/2000 [2:25:19<30:06,  5.36s/it] 83%|████████▎ | 1664/2000 [2:25:24<29:20,  5.24s/it] 83%|████████▎ | 1665/2000 [2:25:30<29:25,  5.27s/it] 83%|████████▎ | 1666/2000 [2:25:35<29:05,  5.22s/it] 83%|████████▎ | 1667/2000 [2:25:40<28:57,  5.22s/it] 83%|████████▎ | 1668/2000 [2:25:46<29:25,  5.32s/it] 83%|████████▎ | 1669/2000 [2:25:51<29:23,  5.33s/it] 84%|████████▎ | 1670/2000 [2:25:56<29:05,  5.29s/it]                                                     {'loss': 0.0016, 'grad_norm': 0.412867933511734, 'learning_rate': 3.65166390088294e-06, 'epoch': 6.68}
 84%|████████▎ | 1670/2000 [2:25:57<29:05,  5.29s/it] 84%|████████▎ | 1671/2000 [2:26:02<29:35,  5.40s/it] 84%|████████▎ | 1672/2000 [2:26:07<28:39,  5.24s/it] 84%|████████▎ | 1673/2000 [2:26:12<28:18,  5.20s/it] 84%|████████▎ | 1674/2000 [2:26:17<28:55,  5.32s/it] 84%|████████▍ | 1675/2000 [2:26:23<29:07,  5.38s/it] 84%|████████▍ | 1676/2000 [2:26:28<28:26,  5.27s/it] 84%|████████▍ | 1677/2000 [2:26:33<28:39,  5.32s/it] 84%|████████▍ | 1678/2000 [2:26:39<28:20,  5.28s/it] 84%|████████▍ | 1679/2000 [2:26:44<28:31,  5.33s/it] 84%|████████▍ | 1680/2000 [2:26:50<28:43,  5.38s/it]                                                     {'loss': 0.0022, 'grad_norm': 0.6107599139213562, 'learning_rate': 3.439482832472554e-06, 'epoch': 6.72}
 84%|████████▍ | 1680/2000 [2:26:50<28:43,  5.38s/it] 84%|████████▍ | 1681/2000 [2:26:55<28:19,  5.33s/it] 84%|████████▍ | 1682/2000 [2:27:00<27:37,  5.21s/it] 84%|████████▍ | 1683/2000 [2:27:05<27:15,  5.16s/it] 84%|████████▍ | 1684/2000 [2:27:10<26:52,  5.10s/it] 84%|████████▍ | 1685/2000 [2:27:15<26:57,  5.14s/it] 84%|████████▍ | 1686/2000 [2:27:20<26:38,  5.09s/it] 84%|████████▍ | 1687/2000 [2:27:26<27:25,  5.26s/it] 84%|████████▍ | 1688/2000 [2:27:31<27:52,  5.36s/it] 84%|████████▍ | 1689/2000 [2:27:36<27:02,  5.22s/it] 84%|████████▍ | 1690/2000 [2:27:41<27:00,  5.23s/it]                                                     {'loss': 0.0018, 'grad_norm': 0.23546816408634186, 'learning_rate': 3.2331961941753474e-06, 'epoch': 6.76}
 84%|████████▍ | 1690/2000 [2:27:41<27:00,  5.23s/it] 85%|████████▍ | 1691/2000 [2:27:47<27:06,  5.26s/it] 85%|████████▍ | 1692/2000 [2:27:52<26:38,  5.19s/it] 85%|████████▍ | 1693/2000 [2:27:57<26:06,  5.10s/it] 85%|████████▍ | 1694/2000 [2:28:02<27:19,  5.36s/it] 85%|████████▍ | 1695/2000 [2:28:08<27:14,  5.36s/it] 85%|████████▍ | 1696/2000 [2:28:13<26:42,  5.27s/it] 85%|████████▍ | 1697/2000 [2:28:18<26:10,  5.18s/it] 85%|████████▍ | 1698/2000 [2:28:23<26:06,  5.19s/it] 85%|████████▍ | 1699/2000 [2:28:28<25:44,  5.13s/it] 85%|████████▌ | 1700/2000 [2:28:33<26:00,  5.20s/it]                                                     {'loss': 0.0021, 'grad_norm': 0.2877999246120453, 'learning_rate': 3.0328603826984658e-06, 'epoch': 6.8}
 85%|████████▌ | 1700/2000 [2:28:35<26:00,  5.20s/it] 85%|████████▌ | 1701/2000 [2:28:41<29:06,  5.84s/it] 85%|████████▌ | 1702/2000 [2:28:46<27:54,  5.62s/it] 85%|████████▌ | 1703/2000 [2:28:51<26:49,  5.42s/it] 85%|████████▌ | 1704/2000 [2:28:56<26:02,  5.28s/it] 85%|████████▌ | 1705/2000 [2:29:01<26:04,  5.30s/it] 85%|████████▌ | 1706/2000 [2:29:06<25:23,  5.18s/it] 85%|████████▌ | 1707/2000 [2:29:11<24:42,  5.06s/it] 85%|████████▌ | 1708/2000 [2:29:16<25:15,  5.19s/it] 85%|████████▌ | 1709/2000 [2:29:22<25:42,  5.30s/it] 86%|████████▌ | 1710/2000 [2:29:26<24:37,  5.10s/it]                                                     {'loss': 0.004, 'grad_norm': 0.20941466093063354, 'learning_rate': 2.838530167852435e-06, 'epoch': 6.84}
 86%|████████▌ | 1710/2000 [2:29:27<24:37,  5.10s/it] 86%|████████▌ | 1711/2000 [2:29:31<24:23,  5.07s/it] 86%|████████▌ | 1712/2000 [2:29:37<24:53,  5.19s/it] 86%|████████▌ | 1713/2000 [2:29:42<24:32,  5.13s/it] 86%|████████▌ | 1714/2000 [2:29:47<24:14,  5.09s/it] 86%|████████▌ | 1715/2000 [2:29:52<24:47,  5.22s/it] 86%|████████▌ | 1716/2000 [2:29:58<24:47,  5.24s/it] 86%|████████▌ | 1717/2000 [2:30:03<24:32,  5.20s/it] 86%|████████▌ | 1718/2000 [2:30:08<24:18,  5.17s/it] 86%|████████▌ | 1719/2000 [2:30:14<24:58,  5.33s/it] 86%|████████▌ | 1720/2000 [2:30:19<24:15,  5.20s/it]                                                     {'loss': 0.007, 'grad_norm': 0.11076100170612335, 'learning_rate': 2.6502586775776076e-06, 'epoch': 6.88}
 86%|████████▌ | 1720/2000 [2:30:19<24:15,  5.20s/it] 86%|████████▌ | 1721/2000 [2:30:24<24:15,  5.22s/it] 86%|████████▌ | 1722/2000 [2:30:29<23:45,  5.13s/it] 86%|████████▌ | 1723/2000 [2:30:34<23:35,  5.11s/it] 86%|████████▌ | 1724/2000 [2:30:39<23:38,  5.14s/it] 86%|████████▋ | 1725/2000 [2:30:44<23:26,  5.11s/it] 86%|████████▋ | 1726/2000 [2:30:49<23:14,  5.09s/it] 86%|████████▋ | 1727/2000 [2:30:54<22:48,  5.01s/it] 86%|████████▋ | 1728/2000 [2:30:59<22:49,  5.04s/it] 86%|████████▋ | 1729/2000 [2:31:04<22:26,  4.97s/it] 86%|████████▋ | 1730/2000 [2:31:09<22:30,  5.00s/it]                                                     {'loss': 0.0045, 'grad_norm': 3.0241377353668213, 'learning_rate': 2.4680973834195516e-06, 'epoch': 6.92}
 86%|████████▋ | 1730/2000 [2:31:09<22:30,  5.00s/it] 87%|████████▋ | 1731/2000 [2:31:14<22:48,  5.09s/it] 87%|████████▋ | 1732/2000 [2:31:19<22:38,  5.07s/it] 87%|████████▋ | 1733/2000 [2:31:24<22:27,  5.05s/it] 87%|████████▋ | 1734/2000 [2:31:29<22:28,  5.07s/it] 87%|████████▋ | 1735/2000 [2:31:34<22:09,  5.02s/it] 87%|████████▋ | 1736/2000 [2:31:39<21:57,  4.99s/it] 87%|████████▋ | 1737/2000 [2:31:44<22:08,  5.05s/it] 87%|████████▋ | 1738/2000 [2:31:50<22:20,  5.12s/it] 87%|████████▋ | 1739/2000 [2:31:55<22:03,  5.07s/it] 87%|████████▋ | 1740/2000 [2:31:59<21:37,  4.99s/it]                                                     {'loss': 0.0036, 'grad_norm': 0.34099361300468445, 'learning_rate': 2.2920960864572212e-06, 'epoch': 6.96}
 87%|████████▋ | 1740/2000 [2:32:00<21:37,  4.99s/it] 87%|████████▋ | 1741/2000 [2:32:05<21:51,  5.06s/it] 87%|████████▋ | 1742/2000 [2:32:10<22:00,  5.12s/it] 87%|████████▋ | 1743/2000 [2:32:15<21:55,  5.12s/it] 87%|████████▋ | 1744/2000 [2:32:20<21:52,  5.13s/it] 87%|████████▋ | 1745/2000 [2:32:26<22:26,  5.28s/it] 87%|████████▋ | 1746/2000 [2:32:31<22:10,  5.24s/it] 87%|████████▋ | 1747/2000 [2:32:36<21:45,  5.16s/it] 87%|████████▋ | 1748/2000 [2:32:41<21:53,  5.21s/it] 87%|████████▋ | 1749/2000 [2:32:46<21:36,  5.17s/it] 88%|████████▊ | 1750/2000 [2:32:51<21:23,  5.13s/it]                                                     {'loss': 0.011, 'grad_norm': 0.12447905540466309, 'learning_rate': 2.1223029036878395e-06, 'epoch': 7.0}
 88%|████████▊ | 1750/2000 [2:32:54<21:23,  5.13s/it][INFO|trainer.py:3993] 2025-06-12 23:57:16,875 >> Saving model checkpoint to ./models/Qwen3-8B-llama-epoch8/checkpoint-1750
[INFO|configuration_utils.py:696] 2025-06-12 23:57:16,947 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-12 23:57:16,949 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2356] 2025-06-12 23:57:18,030 >> chat template saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1750/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-12 23:57:18,040 >> tokenizer config file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-12 23:57:18,050 >> Special tokens file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-1750/special_tokens_map.json
[INFO|trainer.py:4102] 2025-06-12 23:57:20,624 >> Deleting older checkpoint [models/Qwen3-8B-llama-epoch8/checkpoint-500] due to args.save_total_limit
 88%|████████▊ | 1751/2000 [2:33:02<28:42,  6.92s/it] 88%|████████▊ | 1752/2000 [2:33:07<26:07,  6.32s/it] 88%|████████▊ | 1753/2000 [2:33:13<24:36,  5.98s/it] 88%|████████▊ | 1754/2000 [2:33:18<23:53,  5.83s/it] 88%|████████▊ | 1755/2000 [2:33:23<23:00,  5.63s/it] 88%|████████▊ | 1756/2000 [2:33:28<22:08,  5.44s/it] 88%|████████▊ | 1757/2000 [2:33:33<21:41,  5.35s/it] 88%|████████▊ | 1758/2000 [2:33:39<21:33,  5.34s/it] 88%|████████▊ | 1759/2000 [2:33:44<21:06,  5.25s/it] 88%|████████▊ | 1760/2000 [2:33:49<20:52,  5.22s/it]                                                     {'loss': 0.0027, 'grad_norm': 0.6241675019264221, 'learning_rate': 1.958764254872206e-06, 'epoch': 7.04}
 88%|████████▊ | 1760/2000 [2:33:49<20:52,  5.22s/it] 88%|████████▊ | 1761/2000 [2:33:54<20:23,  5.12s/it] 88%|████████▊ | 1762/2000 [2:33:59<20:30,  5.17s/it] 88%|████████▊ | 1763/2000 [2:34:04<20:41,  5.24s/it] 88%|████████▊ | 1764/2000 [2:34:10<20:34,  5.23s/it] 88%|████████▊ | 1765/2000 [2:34:15<20:19,  5.19s/it] 88%|████████▊ | 1766/2000 [2:34:20<20:03,  5.14s/it] 88%|████████▊ | 1767/2000 [2:34:25<19:41,  5.07s/it] 88%|████████▊ | 1768/2000 [2:34:30<19:31,  5.05s/it] 88%|████████▊ | 1769/2000 [2:34:35<20:15,  5.26s/it] 88%|████████▊ | 1770/2000 [2:34:41<20:13,  5.27s/it]                                                     {'loss': 0.001, 'grad_norm': 0.09819728881120682, 'learning_rate': 1.80152484984398e-06, 'epoch': 7.08}
 88%|████████▊ | 1770/2000 [2:34:41<20:13,  5.27s/it] 89%|████████▊ | 1771/2000 [2:34:46<19:48,  5.19s/it] 89%|████████▊ | 1772/2000 [2:34:51<19:54,  5.24s/it] 89%|████████▊ | 1773/2000 [2:34:56<19:26,  5.14s/it] 89%|████████▊ | 1774/2000 [2:35:01<19:08,  5.08s/it] 89%|████████▉ | 1775/2000 [2:35:06<19:17,  5.15s/it] 89%|████████▉ | 1776/2000 [2:35:11<19:05,  5.11s/it] 89%|████████▉ | 1777/2000 [2:35:16<19:04,  5.13s/it] 89%|████████▉ | 1778/2000 [2:35:21<18:39,  5.04s/it] 89%|████████▉ | 1779/2000 [2:35:26<18:17,  4.97s/it] 89%|████████▉ | 1780/2000 [2:35:32<18:54,  5.16s/it]                                                     {'loss': 0.0073, 'grad_norm': 0.6346645951271057, 'learning_rate': 1.650627676286473e-06, 'epoch': 7.12}
 89%|████████▉ | 1780/2000 [2:35:32<18:54,  5.16s/it] 89%|████████▉ | 1781/2000 [2:35:37<19:01,  5.21s/it] 89%|████████▉ | 1782/2000 [2:35:42<18:53,  5.20s/it] 89%|████████▉ | 1783/2000 [2:35:47<18:41,  5.17s/it] 89%|████████▉ | 1784/2000 [2:35:52<18:26,  5.12s/it] 89%|████████▉ | 1785/2000 [2:35:57<18:11,  5.08s/it] 89%|████████▉ | 1786/2000 [2:36:02<18:05,  5.07s/it] 89%|████████▉ | 1787/2000 [2:36:08<18:32,  5.22s/it] 89%|████████▉ | 1788/2000 [2:36:13<18:21,  5.20s/it] 89%|████████▉ | 1789/2000 [2:36:19<18:38,  5.30s/it] 90%|████████▉ | 1790/2000 [2:36:23<17:58,  5.13s/it]                                                     {'loss': 0.0013, 'grad_norm': 0.08634251356124878, 'learning_rate': 1.5061139879802372e-06, 'epoch': 7.16}
 90%|████████▉ | 1790/2000 [2:36:23<17:58,  5.13s/it] 90%|████████▉ | 1791/2000 [2:36:28<17:31,  5.03s/it] 90%|████████▉ | 1792/2000 [2:36:33<17:25,  5.03s/it] 90%|████████▉ | 1793/2000 [2:36:38<17:35,  5.10s/it] 90%|████████▉ | 1794/2000 [2:36:44<17:51,  5.20s/it] 90%|████████▉ | 1795/2000 [2:36:49<17:56,  5.25s/it] 90%|████████▉ | 1796/2000 [2:36:55<18:34,  5.46s/it] 90%|████████▉ | 1797/2000 [2:37:00<18:20,  5.42s/it] 90%|████████▉ | 1798/2000 [2:37:06<18:06,  5.38s/it] 90%|████████▉ | 1799/2000 [2:37:11<17:53,  5.34s/it] 90%|█████████ | 1800/2000 [2:37:16<17:48,  5.34s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.27660855650901794, 'learning_rate': 1.368023293524695e-06, 'epoch': 7.2}
 90%|█████████ | 1800/2000 [2:37:16<17:48,  5.34s/it] 90%|█████████ | 1801/2000 [2:37:22<17:45,  5.35s/it] 90%|█████████ | 1802/2000 [2:37:27<17:24,  5.27s/it] 90%|█████████ | 1803/2000 [2:37:32<17:18,  5.27s/it] 90%|█████████ | 1804/2000 [2:37:38<17:23,  5.32s/it] 90%|█████████ | 1805/2000 [2:37:43<16:58,  5.22s/it] 90%|█████████ | 1806/2000 [2:37:48<16:56,  5.24s/it] 90%|█████████ | 1807/2000 [2:37:53<16:25,  5.11s/it] 90%|█████████ | 1808/2000 [2:37:58<16:18,  5.10s/it] 90%|█████████ | 1809/2000 [2:38:03<16:14,  5.10s/it] 90%|█████████ | 1810/2000 [2:38:08<16:34,  5.23s/it]                                                     {'loss': 0.0017, 'grad_norm': 0.8861736059188843, 'learning_rate': 1.2363933455368792e-06, 'epoch': 7.24}
 90%|█████████ | 1810/2000 [2:38:08<16:34,  5.23s/it] 91%|█████████ | 1811/2000 [2:38:13<16:12,  5.15s/it] 91%|█████████ | 1812/2000 [2:38:18<16:09,  5.16s/it] 91%|█████████ | 1813/2000 [2:38:24<16:06,  5.17s/it] 91%|█████████ | 1814/2000 [2:38:29<16:12,  5.23s/it] 91%|█████████ | 1815/2000 [2:38:34<15:46,  5.12s/it] 91%|█████████ | 1816/2000 [2:38:39<15:48,  5.16s/it] 91%|█████████ | 1817/2000 [2:38:44<15:55,  5.22s/it] 91%|█████████ | 1818/2000 [2:38:49<15:29,  5.11s/it] 91%|█████████ | 1819/2000 [2:38:54<15:10,  5.03s/it] 91%|█████████ | 1820/2000 [2:38:59<15:02,  5.02s/it]                                                     {'loss': 0.0012, 'grad_norm': 0.133835569024086, 'learning_rate': 1.1112601303302605e-06, 'epoch': 7.28}
 91%|█████████ | 1820/2000 [2:38:59<15:02,  5.02s/it] 91%|█████████ | 1821/2000 [2:39:05<15:16,  5.12s/it] 91%|█████████ | 1822/2000 [2:39:10<15:43,  5.30s/it] 91%|█████████ | 1823/2000 [2:39:15<15:35,  5.28s/it] 91%|█████████ | 1824/2000 [2:39:21<15:43,  5.36s/it] 91%|█████████▏| 1825/2000 [2:39:27<15:47,  5.42s/it] 91%|█████████▏| 1826/2000 [2:39:32<15:32,  5.36s/it] 91%|█████████▏| 1827/2000 [2:39:37<15:33,  5.39s/it] 91%|█████████▏| 1828/2000 [2:39:43<15:27,  5.39s/it] 91%|█████████▏| 1829/2000 [2:39:48<15:04,  5.29s/it] 92%|█████████▏| 1830/2000 [2:39:53<14:57,  5.28s/it]                                                     {'loss': 0.0017, 'grad_norm': 0.167876735329628, 'learning_rate': 9.926578580764234e-07, 'epoch': 7.32}
 92%|█████████▏| 1830/2000 [2:39:53<14:57,  5.28s/it] 92%|█████████▏| 1831/2000 [2:39:58<14:37,  5.19s/it] 92%|█████████▏| 1832/2000 [2:40:03<14:08,  5.05s/it] 92%|█████████▏| 1833/2000 [2:40:08<14:34,  5.24s/it] 92%|█████████▏| 1834/2000 [2:40:14<14:30,  5.24s/it] 92%|█████████▏| 1835/2000 [2:40:20<15:05,  5.49s/it] 92%|█████████▏| 1836/2000 [2:40:25<14:30,  5.31s/it] 92%|█████████▏| 1837/2000 [2:40:30<14:07,  5.20s/it] 92%|█████████▏| 1838/2000 [2:40:35<14:07,  5.23s/it] 92%|█████████▏| 1839/2000 [2:40:40<14:02,  5.24s/it] 92%|█████████▏| 1840/2000 [2:40:45<13:56,  5.23s/it]                                                     {'loss': 0.001, 'grad_norm': 0.12601496279239655, 'learning_rate': 8.806189534523668e-07, 'epoch': 7.36}
 92%|█████████▏| 1840/2000 [2:40:45<13:56,  5.23s/it] 92%|█████████▏| 1841/2000 [2:40:51<13:55,  5.25s/it] 92%|█████████▏| 1842/2000 [2:40:55<13:26,  5.10s/it] 92%|█████████▏| 1843/2000 [2:41:01<13:25,  5.13s/it] 92%|█████████▏| 1844/2000 [2:41:06<13:31,  5.20s/it] 92%|█████████▏| 1845/2000 [2:41:11<13:13,  5.12s/it] 92%|█████████▏| 1846/2000 [2:41:16<13:26,  5.24s/it] 92%|█████████▏| 1847/2000 [2:41:22<13:19,  5.23s/it] 92%|█████████▏| 1848/2000 [2:41:27<13:09,  5.20s/it] 92%|█████████▏| 1849/2000 [2:41:31<12:44,  5.06s/it] 92%|█████████▎| 1850/2000 [2:41:37<12:42,  5.08s/it]                                                     {'loss': 0.0011, 'grad_norm': 0.12262493371963501, 'learning_rate': 7.751740467759145e-07, 'epoch': 7.4}
 92%|█████████▎| 1850/2000 [2:41:38<12:42,  5.08s/it] 93%|█████████▎| 1851/2000 [2:41:43<13:56,  5.61s/it] 93%|█████████▎| 1852/2000 [2:41:49<13:39,  5.54s/it] 93%|█████████▎| 1853/2000 [2:41:54<13:14,  5.41s/it] 93%|█████████▎| 1854/2000 [2:41:59<13:07,  5.39s/it] 93%|█████████▎| 1855/2000 [2:42:04<12:49,  5.31s/it] 93%|█████████▎| 1856/2000 [2:42:10<12:40,  5.28s/it] 93%|█████████▎| 1857/2000 [2:42:15<12:37,  5.30s/it] 93%|█████████▎| 1858/2000 [2:42:20<12:42,  5.37s/it] 93%|█████████▎| 1859/2000 [2:42:25<12:19,  5.24s/it] 93%|█████████▎| 1860/2000 [2:42:31<12:11,  5.23s/it]                                                     {'loss': 0.0014, 'grad_norm': 0.22849014401435852, 'learning_rate': 6.763519656316914e-07, 'epoch': 7.44}
 93%|█████████▎| 1860/2000 [2:42:31<12:11,  5.23s/it] 93%|█████████▎| 1861/2000 [2:42:36<12:19,  5.32s/it] 93%|█████████▎| 1862/2000 [2:42:41<12:08,  5.28s/it] 93%|█████████▎| 1863/2000 [2:42:47<12:04,  5.29s/it] 93%|█████████▎| 1864/2000 [2:42:52<11:57,  5.27s/it] 93%|█████████▎| 1865/2000 [2:42:57<11:41,  5.20s/it] 93%|█████████▎| 1866/2000 [2:43:02<11:25,  5.12s/it] 93%|█████████▎| 1867/2000 [2:43:07<11:28,  5.18s/it] 93%|█████████▎| 1868/2000 [2:43:12<11:24,  5.18s/it] 93%|█████████▎| 1869/2000 [2:43:18<11:20,  5.20s/it] 94%|█████████▎| 1870/2000 [2:43:23<11:38,  5.37s/it]                                                     {'loss': 0.0017, 'grad_norm': 0.18208113312721252, 'learning_rate': 5.841797269899447e-07, 'epoch': 7.48}
 94%|█████████▎| 1870/2000 [2:43:23<11:38,  5.37s/it] 94%|█████████▎| 1871/2000 [2:43:29<11:29,  5.34s/it] 94%|█████████▎| 1872/2000 [2:43:34<11:17,  5.29s/it] 94%|█████████▎| 1873/2000 [2:43:39<11:20,  5.36s/it] 94%|█████████▎| 1874/2000 [2:43:44<11:02,  5.26s/it] 94%|█████████▍| 1875/2000 [2:43:49<10:51,  5.21s/it] 94%|█████████▍| 1876/2000 [2:43:55<10:46,  5.21s/it] 94%|█████████▍| 1877/2000 [2:44:00<10:54,  5.32s/it] 94%|█████████▍| 1878/2000 [2:44:05<10:44,  5.28s/it] 94%|█████████▍| 1879/2000 [2:44:10<10:30,  5.21s/it] 94%|█████████▍| 1880/2000 [2:44:16<10:25,  5.21s/it]                                                     {'loss': 0.0014, 'grad_norm': 0.1316116452217102, 'learning_rate': 4.986825298203935e-07, 'epoch': 7.52}
 94%|█████████▍| 1880/2000 [2:44:16<10:25,  5.21s/it] 94%|█████████▍| 1881/2000 [2:44:21<10:26,  5.26s/it] 94%|█████████▍| 1882/2000 [2:44:26<10:15,  5.22s/it] 94%|█████████▍| 1883/2000 [2:44:31<09:54,  5.08s/it] 94%|█████████▍| 1884/2000 [2:44:36<09:49,  5.08s/it] 94%|█████████▍| 1885/2000 [2:44:41<09:47,  5.11s/it] 94%|█████████▍| 1886/2000 [2:44:46<09:37,  5.07s/it] 94%|█████████▍| 1887/2000 [2:44:52<09:49,  5.21s/it] 94%|█████████▍| 1888/2000 [2:44:57<09:48,  5.25s/it] 94%|█████████▍| 1889/2000 [2:45:02<09:41,  5.24s/it] 94%|█████████▍| 1890/2000 [2:45:08<09:39,  5.27s/it]                                                     {'loss': 0.0041, 'grad_norm': 0.20274493098258972, 'learning_rate': 4.1988374820305377e-07, 'epoch': 7.56}
 94%|█████████▍| 1890/2000 [2:45:08<09:39,  5.27s/it] 95%|█████████▍| 1891/2000 [2:45:13<09:44,  5.36s/it] 95%|█████████▍| 1892/2000 [2:45:18<09:34,  5.32s/it] 95%|█████████▍| 1893/2000 [2:45:24<09:33,  5.36s/it] 95%|█████████▍| 1894/2000 [2:45:29<09:21,  5.30s/it] 95%|█████████▍| 1895/2000 [2:45:34<09:02,  5.17s/it] 95%|█████████▍| 1896/2000 [2:45:39<08:50,  5.10s/it] 95%|█████████▍| 1897/2000 [2:45:44<08:58,  5.23s/it] 95%|█████████▍| 1898/2000 [2:45:50<08:58,  5.28s/it] 95%|█████████▍| 1899/2000 [2:45:55<08:44,  5.20s/it] 95%|█████████▌| 1900/2000 [2:46:00<08:37,  5.17s/it]                                                     {'loss': 0.0023, 'grad_norm': 2.885639190673828, 'learning_rate': 3.478049249380194e-07, 'epoch': 7.6}
 95%|█████████▌| 1900/2000 [2:46:00<08:37,  5.17s/it] 95%|█████████▌| 1901/2000 [2:46:05<08:30,  5.16s/it] 95%|█████████▌| 1902/2000 [2:46:10<08:30,  5.20s/it] 95%|█████████▌| 1903/2000 [2:46:15<08:18,  5.14s/it] 95%|█████████▌| 1904/2000 [2:46:21<08:31,  5.33s/it] 95%|█████████▌| 1905/2000 [2:46:26<08:21,  5.28s/it] 95%|█████████▌| 1906/2000 [2:46:31<08:08,  5.20s/it] 95%|█████████▌| 1907/2000 [2:46:36<08:03,  5.19s/it] 95%|█████████▌| 1908/2000 [2:46:42<08:00,  5.23s/it] 95%|█████████▌| 1909/2000 [2:46:47<08:03,  5.32s/it] 96%|█████████▌| 1910/2000 [2:46:52<07:47,  5.20s/it]                                                     {'loss': 0.0038, 'grad_norm': 0.2533726096153259, 'learning_rate': 2.824657656558705e-07, 'epoch': 7.64}
 96%|█████████▌| 1910/2000 [2:46:52<07:47,  5.20s/it] 96%|█████████▌| 1911/2000 [2:46:57<07:45,  5.24s/it] 96%|█████████▌| 1912/2000 [2:47:02<07:34,  5.17s/it] 96%|█████████▌| 1913/2000 [2:47:08<07:32,  5.20s/it] 96%|█████████▌| 1914/2000 [2:47:13<07:21,  5.14s/it] 96%|█████████▌| 1915/2000 [2:47:18<07:15,  5.12s/it] 96%|█████████▌| 1916/2000 [2:47:23<07:08,  5.10s/it] 96%|█████████▌| 1917/2000 [2:47:28<07:09,  5.17s/it] 96%|█████████▌| 1918/2000 [2:47:34<07:20,  5.38s/it] 96%|█████████▌| 1919/2000 [2:47:40<07:20,  5.43s/it] 96%|█████████▌| 1920/2000 [2:47:45<07:12,  5.41s/it]                                                     {'loss': 0.002, 'grad_norm': 0.17929710447788239, 'learning_rate': 2.2388413343034652e-07, 'epoch': 7.68}
 96%|█████████▌| 1920/2000 [2:47:46<07:12,  5.41s/it] 96%|█████████▌| 1921/2000 [2:47:51<07:16,  5.53s/it] 96%|█████████▌| 1922/2000 [2:47:56<07:02,  5.42s/it] 96%|█████████▌| 1923/2000 [2:48:01<06:45,  5.27s/it] 96%|█████████▌| 1924/2000 [2:48:06<06:33,  5.17s/it] 96%|█████████▋| 1925/2000 [2:48:12<06:56,  5.55s/it] 96%|█████████▋| 1926/2000 [2:48:18<06:46,  5.49s/it] 96%|█████████▋| 1927/2000 [2:48:22<06:26,  5.29s/it] 96%|█████████▋| 1928/2000 [2:48:28<06:20,  5.28s/it] 96%|█████████▋| 1929/2000 [2:48:33<06:18,  5.34s/it] 96%|█████████▋| 1930/2000 [2:48:38<06:07,  5.25s/it]                                                     {'loss': 0.0085, 'grad_norm': 0.22100038826465607, 'learning_rate': 1.720760438947644e-07, 'epoch': 7.72}
 96%|█████████▋| 1930/2000 [2:48:39<06:07,  5.25s/it] 97%|█████████▋| 1931/2000 [2:48:44<06:15,  5.44s/it] 97%|█████████▋| 1932/2000 [2:48:49<06:02,  5.33s/it] 97%|█████████▋| 1933/2000 [2:48:55<05:58,  5.35s/it] 97%|█████████▋| 1934/2000 [2:49:00<05:48,  5.28s/it] 97%|█████████▋| 1935/2000 [2:49:05<05:45,  5.32s/it] 97%|█████████▋| 1936/2000 [2:49:11<05:42,  5.35s/it] 97%|█████████▋| 1937/2000 [2:49:15<05:29,  5.23s/it] 97%|█████████▋| 1938/2000 [2:49:21<05:30,  5.34s/it] 97%|█████████▋| 1939/2000 [2:49:27<05:30,  5.43s/it] 97%|█████████▋| 1940/2000 [2:49:32<05:23,  5.39s/it]                                                     {'loss': 0.0016, 'grad_norm': 0.13584204018115997, 'learning_rate': 1.2705566086350097e-07, 'epoch': 7.76}
 97%|█████████▋| 1940/2000 [2:49:32<05:23,  5.39s/it] 97%|█████████▋| 1941/2000 [2:49:37<05:17,  5.38s/it] 97%|█████████▋| 1942/2000 [2:49:43<05:15,  5.44s/it] 97%|█████████▋| 1943/2000 [2:49:48<05:06,  5.38s/it] 97%|█████████▋| 1944/2000 [2:49:53<04:58,  5.33s/it] 97%|█████████▋| 1945/2000 [2:49:59<05:01,  5.48s/it] 97%|█████████▋| 1946/2000 [2:50:04<04:51,  5.40s/it] 97%|█████████▋| 1947/2000 [2:50:09<04:40,  5.30s/it] 97%|█████████▋| 1948/2000 [2:50:14<04:30,  5.20s/it] 97%|█████████▋| 1949/2000 [2:50:19<04:16,  5.03s/it] 98%|█████████▊| 1950/2000 [2:50:24<04:15,  5.11s/it]                                                     {'loss': 0.0022, 'grad_norm': 0.1641683578491211, 'learning_rate': 8.883529245975186e-08, 'epoch': 7.8}
 98%|█████████▊| 1950/2000 [2:50:24<04:15,  5.11s/it] 98%|█████████▊| 1951/2000 [2:50:30<04:17,  5.25s/it] 98%|█████████▊| 1952/2000 [2:50:35<04:09,  5.20s/it] 98%|█████████▊| 1953/2000 [2:50:40<04:01,  5.13s/it] 98%|█████████▊| 1954/2000 [2:50:45<03:54,  5.09s/it] 98%|█████████▊| 1955/2000 [2:50:50<03:43,  4.97s/it] 98%|█████████▊| 1956/2000 [2:50:55<03:43,  5.07s/it] 98%|█████████▊| 1957/2000 [2:51:00<03:36,  5.03s/it] 98%|█████████▊| 1958/2000 [2:51:05<03:29,  4.99s/it] 98%|█████████▊| 1959/2000 [2:51:10<03:24,  4.98s/it] 98%|█████████▊| 1960/2000 [2:51:15<03:22,  5.07s/it]                                                     {'loss': 0.0022, 'grad_norm': 0.5651951432228088, 'learning_rate': 5.742538775061201e-08, 'epoch': 7.84}
 98%|█████████▊| 1960/2000 [2:51:17<03:22,  5.07s/it] 98%|█████████▊| 1961/2000 [2:51:22<03:36,  5.56s/it] 98%|█████████▊| 1962/2000 [2:51:27<03:30,  5.54s/it] 98%|█████████▊| 1963/2000 [2:51:32<03:16,  5.32s/it] 98%|█████████▊| 1964/2000 [2:51:37<03:10,  5.30s/it] 98%|█████████▊| 1965/2000 [2:51:43<03:05,  5.29s/it] 98%|█████████▊| 1966/2000 [2:51:48<02:58,  5.24s/it] 98%|█████████▊| 1967/2000 [2:51:53<02:52,  5.24s/it] 98%|█████████▊| 1968/2000 [2:51:58<02:48,  5.26s/it] 98%|█████████▊| 1969/2000 [2:52:04<02:43,  5.27s/it] 98%|█████████▊| 1970/2000 [2:52:09<02:36,  5.21s/it]                                                     {'loss': 0.0009, 'grad_norm': 0.16513457894325256, 'learning_rate': 3.283453389039959e-08, 'epoch': 7.88}
 98%|█████████▊| 1970/2000 [2:52:09<02:36,  5.21s/it] 99%|█████████▊| 1971/2000 [2:52:14<02:30,  5.17s/it] 99%|█████████▊| 1972/2000 [2:52:19<02:24,  5.16s/it] 99%|█████████▊| 1973/2000 [2:52:24<02:18,  5.15s/it] 99%|█████████▊| 1974/2000 [2:52:29<02:14,  5.18s/it] 99%|█████████▉| 1975/2000 [2:52:34<02:09,  5.17s/it] 99%|█████████▉| 1976/2000 [2:52:40<02:05,  5.22s/it] 99%|█████████▉| 1977/2000 [2:52:45<01:59,  5.21s/it] 99%|█████████▉| 1978/2000 [2:52:51<01:57,  5.35s/it] 99%|█████████▉| 1979/2000 [2:52:56<01:53,  5.41s/it] 99%|█████████▉| 1980/2000 [2:53:01<01:45,  5.26s/it]                                                     {'loss': 0.0018, 'grad_norm': 0.1846633106470108, 'learning_rate': 1.50694537730256e-08, 'epoch': 7.92}
 99%|█████████▉| 1980/2000 [2:53:01<01:45,  5.26s/it] 99%|█████████▉| 1981/2000 [2:53:06<01:37,  5.13s/it] 99%|█████████▉| 1982/2000 [2:53:11<01:32,  5.12s/it] 99%|█████████▉| 1983/2000 [2:53:16<01:27,  5.13s/it] 99%|█████████▉| 1984/2000 [2:53:22<01:23,  5.23s/it] 99%|█████████▉| 1985/2000 [2:53:26<01:16,  5.07s/it] 99%|█████████▉| 1986/2000 [2:53:31<01:11,  5.08s/it] 99%|█████████▉| 1987/2000 [2:53:37<01:06,  5.10s/it] 99%|█████████▉| 1988/2000 [2:53:42<01:00,  5.08s/it] 99%|█████████▉| 1989/2000 [2:53:47<00:55,  5.08s/it]100%|█████████▉| 1990/2000 [2:53:52<00:50,  5.08s/it]                                                     {'loss': 0.0019, 'grad_norm': 0.11405257135629654, 'learning_rate': 4.1350041940113604e-09, 'epoch': 7.96}
100%|█████████▉| 1990/2000 [2:53:52<00:50,  5.08s/it]100%|█████████▉| 1991/2000 [2:53:57<00:46,  5.19s/it]100%|█████████▉| 1992/2000 [2:54:03<00:42,  5.31s/it]100%|█████████▉| 1993/2000 [2:54:08<00:37,  5.34s/it]100%|█████████▉| 1994/2000 [2:54:13<00:31,  5.31s/it]100%|█████████▉| 1995/2000 [2:54:18<00:26,  5.23s/it]100%|█████████▉| 1996/2000 [2:54:23<00:20,  5.09s/it]100%|█████████▉| 1997/2000 [2:54:28<00:15,  5.04s/it]100%|█████████▉| 1998/2000 [2:54:33<00:10,  5.02s/it]100%|█████████▉| 1999/2000 [2:54:38<00:05,  5.00s/it]100%|██████████| 2000/2000 [2:54:43<00:00,  5.02s/it]                                                     {'loss': 0.0021, 'grad_norm': 1.187200665473938, 'learning_rate': 3.417452268950072e-11, 'epoch': 8.0}
100%|██████████| 2000/2000 [2:54:43<00:00,  5.02s/it][INFO|trainer.py:3993] 2025-06-13 00:19:06,745 >> Saving model checkpoint to ./models/Qwen3-8B-llama-epoch8/checkpoint-2000
[INFO|configuration_utils.py:696] 2025-06-13 00:19:06,819 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-13 00:19:06,821 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2356] 2025-06-13 00:19:08,190 >> chat template saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-2000/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-13 00:19:08,202 >> tokenizer config file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-13 00:19:08,215 >> Special tokens file saved in ./models/Qwen3-8B-llama-epoch8/checkpoint-2000/special_tokens_map.json
[INFO|trainer.py:4102] 2025-06-13 00:19:12,179 >> Deleting older checkpoint [models/Qwen3-8B-llama-epoch8/checkpoint-750] due to args.save_total_limit
[INFO|trainer.py:2676] 2025-06-13 00:19:12,249 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 10489.5104, 'train_samples_per_second': 3.051, 'train_steps_per_second': 0.191, 'train_loss': 0.11926327563961968, 'epoch': 8.0}
100%|██████████| 2000/2000 [2:54:49<00:00,  5.02s/it]100%|██████████| 2000/2000 [2:54:49<00:00,  5.24s/it]
[INFO|trainer.py:3993] 2025-06-13 00:19:12,314 >> Saving model checkpoint to ./models/Qwen3-8B-llama-epoch8
[INFO|configuration_utils.py:696] 2025-06-13 00:19:12,384 >> loading configuration file ./models/Qwen3-8B/config.json
[INFO|configuration_utils.py:770] 2025-06-13 00:19:12,386 >> Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2356] 2025-06-13 00:19:14,264 >> chat template saved in ./models/Qwen3-8B-llama-epoch8/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-06-13 00:19:14,274 >> tokenizer config file saved in ./models/Qwen3-8B-llama-epoch8/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-06-13 00:19:14,282 >> Special tokens file saved in ./models/Qwen3-8B-llama-epoch8/special_tokens_map.json
***** train metrics *****
  epoch                    =         8.0
  total_flos               = 803694330GF
  train_loss               =      0.1193
  train_runtime            =  2:54:49.51
  train_samples_per_second =       3.051
  train_steps_per_second   =       0.191
Figure saved at: ./models/Qwen3-8B-llama-epoch8/training_loss.png
[WARNING|2025-06-13 00:19:15] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-06-13 00:19:15] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|modelcard.py:450] 2025-06-13 00:19:15,381 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
